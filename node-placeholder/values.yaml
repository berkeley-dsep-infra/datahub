replicaCount: 1

image:
  repository: us-central1-docker.pkg.dev/ucb-datahub-2018/core/node-placeholder-scaler
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.0.1-n5578.h61ec24d1"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

resources: {}

nodeSelector: {}


affinity: {}

rbac:
  create: true

priorityClass:
  create: true


tolerations:

calendarUrl: https://calendar.google.com/calendar/ical/c_s47m3m1nuj3s81187k3b2b5s5o%40group.calendar.google.com/public/basic.ics

grafana:
  tags:
    - node-placeholder-scale
  url: https://grafana.datahub.berkeley.edu


# Calculate size of node placeholder by:
# 1. Select a node in the node pool you want to configure
#    export NODE=<node-name>
# 2. Get total amount of memory allocatable to pods on this node
#    kubectl get node $NODE -o jsonpath='{.status.allocatable.memory}'
# 3. Get total amount of memory used by non-user pods on the node.
#    kubectl get -A pod -l 'component!=user-placeholder' --field-selector spec.nodeName=$NODE -o jsonpath='{range .items[*].spec.containers[*]}{.name}{"\t"}{.resources.requests.memory}{"\n"}{end}' | egrep -v 'pause|notebook' | sort
#    This will return memory units you'll need to sum yourself
#
# The node placeholder pod should be 'big' enough that it needs to be kicked out to get even a single
# user pod on the node - but not so big that it can't run on a node where other system pods are running!
#
# So we should make the memory be output of (2) - output of (3), with maybe another 256Mi wiggle room
# FIXME: LET US MAKE THIS INTO A SCRIPT!
nodePools:
  a11y:
    nodeSelector:
      hub.jupyter.org/pool-name: a11y-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  astro:
    nodeSelector:
      hub.jupyter.org/pool-name: astro-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  biology:
    nodeSelector:
      hub.jupyter.org/pool-name: biology-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  cee:
    nodeSelector:
      hub.jupyter.org/pool-name: cee-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  data100:
    nodeSelector:
      hub.jupyter.org/pool-name: data100-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 207869800448
    replicas: 1
  data101:
    nodeSelector:
      hub.jupyter.org/pool-name: data101-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  data102:
    nodeSelector:
      hub.jupyter.org/pool-name: data102-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  data8:
    nodeSelector:
      hub.jupyter.org/pool-name: data8-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  datahub:
    nodeSelector:
      hub.jupyter.org/pool-name: datahub-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  dlab:
    nodeSelector:
      hub.jupyter.org/pool-name: dlab-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  eecs:
    nodeSelector:
      hub.jupyter.org/pool-name: eecs-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  ischool:
    nodeSelector:
      hub.jupyter.org/pool-name: ischool-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  publichealth:
    nodeSelector:
      hub.jupyter.org/pool-name: publichealth-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  stat159:
    nodeSelector:
      hub.jupyter.org/pool-name: stat159-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  stat20:
    nodeSelector:
      hub.jupyter.org/pool-name: stat20-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  r:
    nodeSelector:
      hub.jupyter.org/pool-name: r-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
  small-courses:
    nodeSelector:
      hub.jupyter.org/pool-name: small-courses-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        memory: 61485400064
    replicas: 1
