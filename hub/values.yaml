etcGitConfig:
  enabled: false
nfsPVC:
  enabled: true

jupyterhub:
  prePuller:
    hook:
      pullOnlyOnChanges: true
  ingress:
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 256m
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
      acme.cert-manager.io/http01-edit-in-place: "true"
  scheduling:
    podPriority:
      enabled: true
      globalDefault: false
      defaultPriority: 0
      userPlaceholderPriority: -10
    userScheduler:
      enabled: false
      resources:
        requests:
          # FIXME: Just unset this?
          cpu: 0.01
          memory: 128Mi
        limits:
          memory: 1G
  proxy:
    service:
      type: ClusterIP
    chp:
      resources:
        requests:
          # FIXME: We want no guarantees here!!!
          # This is lowest possible value
          cpu: 0.001
          memory: 64Mi
        limits:
          memory: 1Gi
    traefik:
      resources:
        requests:
          memory: 96Mi
        limits:
          memory: 1Gi
    https:
      enabled: true
      letsencrypt:
        contactEmail: yuvipanda@berkeley.edu
  singleuser:
    cmd:
      # Explicitly define this, as it's no longer set by z2jh
      # https://github.com/jupyterhub/zero-to-jupyterhub-k8s/pull/2449
      - jupyterhub-singleuser
    cpu:
      # Make sure all users get at least 5% of CPU, so other users can't starve them
      # so much that they crash
      guarantee: 0.05
      # We use CPUs with 8 cores, and want to make sure that a single user can't take
      # over a node (more than one still can). But we also want to give users the
      # option of using as much CPU as they need if nobody else is using the node, as
      # is often the case. 4 seems a reasonable compromise here.
      limit: 4
    extraFiles:
      no-nfs-config:
        mountPath: /etc/ipython/ipython_kernel_config.json
        data:
          HistoryManager:
            # We do not need to keep around history for IPython, and especially not on NFS
            enabled: false
      culling-config:
        mountPath: /etc/jupyter/jupyter_notebook_config.json
        data:
          NotebookApp:
            # shutdown the server after no 30 mins of no activity
            shutdown_no_activity_timeout: 1800

          # if a user leaves a notebook with a running kernel,
          # the effective idle timeout will typically be CULL_TIMEOUT + CULL_KERNEL_TIMEOUT
          # as culling the kernel will register activity,
          # resetting the no_activity timer for the server as a whole
          MappingKernelManager:
            # shutdown kernels after 30 mins of no activity
            cull_idle_timeout: 1800
            # check for idle kernels this often
            cull_interval: 60
            # a kernel with open connections but no activity still counts as idle
            # this is what allows us to shutdown servers
            # when people leave a notebook open and wander off
            cull_connected: true
          NotebookNotary:
            # Use memory for notebook notary file to workaround corrupted files on nfs
            # https://www.sqlite.org/inmemorydb.html
            # https://github.com/jupyter/jupyter/issues/174
            # https://github.com/ipython/ipython/issues/9163
            db_file: ":memory:"
      popularity-contest:
        mountPath: /opt/conda/etc/ipython/startup/000-popularity-contest.py
        stringData: |
          import popularity_contest.reporter
          popularity_contest.reporter.setup_reporter()
    extraEnv:
      SHELL: /bin/bash
      # until https://github.com/jupyterhub/jupyterhub/pull/3918 or equivalent lands
      # Also, with jupyter server the 'Control Panel' is not being displayed
      JUPYTERHUB_SINGLEUSER_APP: "notebook.notebookapp.NotebookApp"
      PYTHON_POPCONTEST_STATSD_HOST: 'support-prometheus-statsd-exporter.support'
      PYTHON_POPCONTEST_STATSD_PORT: '9125'
      # Set RUNTIME_DIR to live under /tmp, as these need not persist across container restarts
      # This doesn't cause a *lot* of NFS traffic, but does cause *some*
      JUPYTER_RUNTIME_DIR: /tmp/jupyter-runtime
    startTimeout: 600 # 10 mins, because sometimes we have too many new nodes coming up together
    networkPolicy:
      enabled: true
      # We enable all outgoing access to the broader internet, but not to private IP ranges
      # of pods and services inside the cluster - except for outbound UDP and TCP to statsd,
      # for python-popularity-package.
      egress:
        # Allow code in hubs to talk to ingress provider, so they can talk to the hub via its
        # public URL
        - to:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  app.kubernetes.io/name: ingress-nginx
          ports:
            - port: 443
              protocol: TCP
            - port: 80
              protocol: TCP
        # datahub.berkeley.edu is still not using the ingress provider, so
        # we have to explicitly allow access to it from our user pods.
        - to:
            - podSelector:
                matchLabels:
                  app: jupyterhub
                  component: autohttps
              namespaceSelector:
                matchLabels:
                  name: datahub-prod
          ports:
            - port: http
              protocol: TCP
            - port: https
              protocol: TCP
        # Allow user code to send packets to statsd for python-popularity-contest
        - to:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  app.kubernetes.io/name: prometheus-statsd-exporter
          ports:
            # statsd ports
            - port: 9125
              protocol: TCP
            - port: 9125
              protocol: UDP
        - to:
            - ipBlock:
                cidr: 0.0.0.0/0
                except:
              # Don't allow network access to private IP ranges
              # Listed in https://datatracker.ietf.org/doc/html/rfc1918
                  - 10.0.0.0/8
                  - 172.16.0.0/12
                  - 192.168.0.0/16
              # Don't allow network access to the metadata IP
                  - 169.254.169.254/32
  hub:
    # The default of 64 is way too low for us
    concurrentSpawnLimit: 200
    loadRoles:
      # Should use this, not hub.config.JupyterHub.load_roles - that will
      # override any existing load_roles set by z2jh
      user-login:
        name: user
        scopes:
        # Allow all users access to 'services', which include the hubs that
        # use the main datahub for auth
          - access:services
          - self

    config:
      Pagination:
        # Disable pagination completely, since there's no search functionality yet
        default_per_page: 30000
        max_per_page: 30000
      JupyterHub:
        authenticator_class: canvasoauthenticator.CanvasOAuthenticator
        # Allow unauthenticated prometheus requests
        # Otherwise our prometheus server can't get to these
        authenticate_prometheus: false
      Authenticator:
        enable_auth_state: true
        manage_groups: true
      KubeSpawner:
        # Make sure working directory is ${HOME}
        # hubploy has a bug where it unconditionally puts workingdir to be /srv/repo
        working_dir: '/home/jovyan'
      CanvasOAuthenticator:
        canvas_url: https://bcourses.berkeley.edu/
        strip_email_domain: berkeley.edu
        login_service: bCourses
        scope:
          - url:GET|/api/v1/users/:user_id/profile
          - url:GET|/api/v1/courses
        username_key: primary_email

    # Generated by chartpress
    image:
      name: gcr.io/ucb-datahub-2018/jupyterhub-hub
      tag: '0.0.1-n5702.h3399ceff'
    networkPolicy:
      enabled: true
      ingress:
        - from:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  app: prometheus
                  component: server
          ports:
            - port: http
              protocol: TCP
    livenessProbe:
      enabled: true
      initialDelaySeconds: 180
    resources:
      requests:
        # Very small unit, since we don't want any CPU guarantees
        # FIXME: Can't seem to get this to null?
        cpu: 0.001
        memory: 256Mi
      limits:
        memory: 2Gi
    extraEnv:
      # This is unfortunately still needed by canvas auth
      OAUTH2_AUTHORIZE_URL: https://bcourses.berkeley.edu/login/oauth2/auth
    extraConfig:
      # Use 0x-<title> in `values.yaml` extraConfig values.
      # We want these to come *before* the extraConfig values
      # in `values.yaml`. Since these are ordered alphabetically,
      # 1x-<title> used in `common.yaml` will come after
      # 0x-<title> used in `values.yaml` - so config set there
      # will override config set here.
      01-custom-attr-spawner: |
        import asyncio
        import os
        import time
        from kubespawner import KubeSpawner
        from prometheus_client import Histogram
        from jupyterhub import metrics
        from tornado import gen
        from traitlets import ( Int, Unicode )
        import textwrap
        import z2jh

        hosted_domain = 'berkeley.edu'

        metrics.SERVER_SPAWN_DURATION_SECONDS = Histogram(
          'jupyterhub_server_spawn_duration_seconds',
          'time taken for server spawning operation',
          ['status'],
          # Use custom bucket sizes, since the default bucket ranges
          # are meant for quick running processes. Spawns can take a while!
          buckets=[0.5, 1, 2.5, 5, 10, 15, 30, 60, 120, 180, 300, 600, float("inf")],
        )

        def memparse(val):
          '''Parse memory for relative comparisons.'''
          if type(val) != str or len(val) == 0:
            return val
          mem = int(val.upper()[0:-1])
          unit = val[-1]
          n = {'B':0, 'K':1, 'M':2, 'G':3}[unit]
          return mem * 1024**n

        def mem_cmp(a, b):
          '''Compare memory values.'''
          x = memparse(a)
          y = memparse(b)
          return (x > y) - (x < y)

        class CustomAttrSpawner(KubeSpawner):
          spawn_launcher_delay = Int(
            0,
            config=True,
            help="""
            Time in seconds to delay a server launch, useful for debugging.  If set to zero,
            no delay will take place.
            """
          )

          spawn_launcher_timer = Int(
            60,
            config=True,
            help="""
            Time in seconds to wait before injecting a 'please be patient' message to display
            to the user.
            """
          )

          spawn_launcher_timer_frequency = Int(
            5,
            config=True,
            help="""
            Display the patience message every 5 seconds.
            """
          )

          spawn_launcher_timer_message = Unicode(
            """
            Server launch is taking longer than expected. Please be patient!
            """,
            config=True,
            help="""
            The injected 'please be patient' message to display to the user.
            """
          )

          def _build_common_labels(self, extra_labels):
            labels = super()._build_common_labels(extra_labels)
            # Until https://github.com/jupyterhub/kubespawner/issues/498
            # is fixed
            del labels['hub.jupyter.org/username']
            return labels

          def apply_override(self, override):
            '''Apply a dictionary of KubeSpawner settings.'''
            self.volumes += override.get('extraVolumes', [])
            self.volume_mounts += override.get('extraVolumeMounts', [])
            self.init_containers += override.get('initContainers', [])
            # set new mem thresholds if specified are bigger than current
            if 'mem_limit' in override and \
            mem_cmp(override['mem_limit'], self.mem_limit) == 1:
              self.mem_limit = override['mem_limit']
            if 'mem_guarantee' in override and \
            mem_cmp(override['mem_guarantee'], self.mem_guarantee) == 1:
              self.mem_guarantee = override['mem_guarantee']

          async def start(self):
            # delay if testing, see https://github.com/berkeley-dsep-infra/datahub/issues/4237
            if self.spawn_launcher_delay:
              await asyncio.sleep(self.spawn_launcher_delay)

            # custom.memory
            custom_memory = z2jh.get_config('custom.memory', {})
            for attr, users in custom_memory.items():
              if self.user.name in users:
                self.mem_limit = attr
                self.mem_guarantee = attr
                break

            # custom.profiles
            custom_profiles = z2jh.get_config('custom.profiles', {})
            for profile, override in custom_profiles.items():
              if 'users' in override and self.user.name in override['users']:
                self.log.warning(f'using profile {profile}')
                self.apply_override(override)

            # custom.group_profiles
            #self.log.info(f'self.user.groups: {self.user.groups}')

            group_profiles = z2jh.get_config('custom.group_profiles', {})

            is_student = False
            is_instructor = False
            for group in self.user.groups:
              # group format is 'canvas::{canvas_id}::{role}'
              _, group_course, group_role = group.name.split('::', 2)
              if not group_course in group_profiles:
                continue

              self.log.info(f'user {self.user.name} in group profile {group_course}')
              override = group_profiles.get(group_course, {})
              self.apply_override(override)

              is_instructor |= group_role in ['teacher', 'ta']
              is_student |= group_role in ['student', 'observer']

            # if the user is a student in any course specified by a profile,
            # they never get to be an admin
            if is_student:
              self.user.admin = False
            elif is_instructor:
              self.user.admin = True

            # custom.admin
            custom_admin = z2jh.get_config('custom.admin', {})
            if custom_admin and self.user.admin:
              self.apply_override(custom_admin)

            return (await super().start())

          async def progress(self):
            """
            This function is reporting back the progress of spawning a pod until
            self._start_future has fired.
            This is working with events parsed by the python kubernetes client,
            and here is the specification of events that is relevant to understand:
              ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#event-v1-core

            most of this code is directly copied from:
              https://github.com/jupyterhub/kubespawner/blob/main/kubespawner/spawner.py#L2326
            adding timer to display a message asking for patience:
              https://github.com/berkeley-dsep-infra/datahub/issues/3845
            """
            if not self.events_enabled:
              return

            self.log.debug('progress generator: %s', self.pod_name)
            start_future = self._start_future
            progress = 0
            next_event = 0
            timer = 0

            break_while_loop = False
            while True:
              # This logic avoids a race condition. self._start() will be invoked by
              # self.start() and almost directly set self._start_future. But,
              # progress() will be invoked via self.start(), so what happen first?
              # Due to this, the logic below is to avoid making an assumption that
              # self._start_future was set before this function was called.
              if start_future is None and self._start_future:
                start_future = self._start_future

              # Ensure we capture all events by inspecting events a final time
              # after the start_future signal has fired, we could have been in
              # .sleep() and missed something.
              if start_future and start_future.done():
                break_while_loop = True

              # check the timer, and if we're over 60sec ask the user to be
              # patient
              # https://github.com/berkeley-dsep-infra/datahub/issues/3845
              if timer >= self.spawn_launcher_timer:
                if timer % self.spawn_launcher_timer_frequency == 0:
                  # display only every X seconds
                  patience_message = textwrap.dedent(self.spawn_launcher_timer_message)
                  patience_message += " Current time spent waiting: %i seconds" % timer
                  yield { 'message': patience_message, }

              events = self.events
              len_events = len(events)
              if next_event < len_events:
                for i in range(next_event, len_events):
                  event = events[i]
                  # move the progress bar.
                  # Since we don't know how many events we will get,
                  # asymptotically approach 90% completion with each event.
                  # each event gets 33% closer to 90%:
                  # 30 50 63 72 78 82 84 86 87 88 88 89
                  progress += (90 - progress) / 3

                  yield {
                    'progress': int(progress),
                    'raw_event': event,
                    'message': "%s [%s] %s"
                    % (
                       event["lastTimestamp"] or event["eventTime"],
                       event["type"],
                       event["message"],
                      ),
                    }
                next_event = len_events

              if break_while_loop:
                break
              timer += 1
              await asyncio.sleep(1)

        c.JupyterHub.spawner_class = CustomAttrSpawner
      02-popularity-contest: |
        # Emit metrics for which python packages are being imported
        import os
        pod_namespace = os.environ['POD_NAMESPACE']
        c.KubeSpawner.environment.update({
          'PYTHON_POPCONTEST_STATSD_PREFIX': f'python_popcon.namespace.{pod_namespace}'
        })
