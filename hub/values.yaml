etcJupyter:
  jupyter_notebook_config.json:
    # if a user leaves a notebook with a running kernel,
    # the effective idle timeout will typically be CULL_TIMEOUT + CULL_KERNEL_TIMEOUT
    # as culling the kernel will register activity,
    # resetting the no_activity timer for the server as a whole
    MappingKernelManager:
      # shutdown kernels after no activity
      cull_idle_timeout: 3600
      # check for idle kernels this often
      cull_interval: 300
      # a kernel with open connections but no activity still counts as idle
      # this is what allows us to shutdown servers
      # when people leave a notebook open and wander off
      cull_connected: true
    NotebookNotary:
      # Use memory for notebook notary file to workaround corrupted files on nfs
      # https://www.sqlite.org/inmemorydb.html
      # https://github.com/jupyter/jupyter/issues/174
      # https://github.com/ipython/ipython/issues/9163
      db_file: ":memory:"
    NotebookApp:
      # Allow scraping metrics from Prometheus server
      authenticate_prometheus: false
      # Stop notebook when there's no activity
      # This adds an additional layer to the idle culler, which is sometimes screwed up
      # via connected websockets and /metrics calls. I'm not sure if this will actually help,
      # but we can try and find out! See https://github.com/jupyterhub/jupyterhub/issues/3099
      # for more example
      # 1h, same as culler.
      shutdown_no_activity_timeout: 3600
    ResourceUseDisplay:
      disable_legacy_endpoint: true
    WebPDFExporter:
      disable_sandbox: true

etcGitConfig:
  enabled: false
nfsPVC:
  enabled: true

jupyterhub-ssh:
  ssh:
    networkPolicy:
      ingress:
        - ports:
            - port: ssh
              protocol: TCP
          from:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  hub.jupyter.org/network-access-ssh-server: "true"
  sftp:
    networkPolicy:
      ingress:
        - ports:
            - port: sftp
              protocol: TCP
          from:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  hub.jupyter.org/network-access-ssh-server: "true"
    pvc:
      name: home-nfs

jupyterhub:
  ingress:
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 256m
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
  scheduling:
    podPriority:
      enabled: true
      globalDefault: false
      defaultPriority: 0
      userPlaceholderPriority: -10
    userScheduler:
      enabled: true
      resources:
        requests:
          # FIXME: Just unset this?
          cpu: 0.01
          memory: 128Mi
        limits:
          memory: 1G
  proxy:
    service:
      extraPorts:
        - port: 22
          targetPort: 8022
          protocol: TCP
          name: ssh
        - port: 2222
          targetPort: 2222
          protocol: TCP
          name: sftp
    chp:
      resources:
        requests:
          # FIXME: We want no guarantees here!!!
          # This is lowest possible value
          cpu: 0.001
          memory: 64Mi
        limits:
          memory: 1Gi
    traefik:
      networkPolicy:
        # We xan use 'ssh' instead of 8022 once https://github.com/jupyterhub/zero-to-jupyterhub-k8s/pull/1901 is merged
        # 8022 for ssh, 2222 for sftp
        allowedIngressPorts: [http, https, 8022, 2222]
      labels:
        hub.jupyter.org/network-access-ssh-server: "true"
        hub.jupyter.org/network-access-sftp-server: "true"
      extraStaticConfig:
        entryPoints:
          ssh:
            address: ':8022'
          sftp:
            address: ':2222'
      extraDynamicConfig:
        tcp:
          services:
            sftp:
              loadBalancer:
                servers:
                  - address: jupyterhub-sftp:22
            ssh:
              loadBalancer:
                servers:
                  - address: jupyterhub-ssh:22
          routers:
            sftp-router:
              entrypoints:
                - sftp
              rule: 'HostSNI(`*`)'
              service: sftp
            ssh-router:
              entrypoints:
                - ssh
              rule: 'HostSNI(`*`)'
              service: ssh
      resources:
        requests:
          memory: 96Mi
        limits:
          memory: 1Gi
    https:
      enabled: true
      letsencrypt:
        contactEmail: yuvipanda@berkeley.edu
  singleuser:
    extraEnv:
      SHELL: /bin/bash
    startTimeout: 600 # 10 mins, because sometimes we have too many new nodes coming up together
    extraAnnotations:
      prometheus.io/scrape: "false"
      prometheus.io/path: "/user/{username}/metrics"
      prometheus.io/port: "8888"
    networkPolicy:
      # In clusters with NetworkPolicy enabled, do not
      # allow outbound internet access that's not DNS, FTP, HTTP or HTTPS
      # We can override this on a case to case basis where
      # required.
      enabled: true
      # NOTE: This needs to be repeated in r/config/common.yaml, since we can't
      # add an item to a list. It needs external ssh access, something I don't
      # want to allow in general as an anti-abuse measure.
      egress:
        - ports:
            - port: 80
              protocol: TCP
        - ports:
            - port: 443
              protocol: TCP
        - ports:
            # Allow FTP access https://github.com/berkeley-dsep-infra/datahub/issues/1789
            - port: 21
              protocol: TCP
      # Allow ingress from promethetheus scraper in support namespace
      # This lets us get notebook metrics!
      ingress:
        - ports:
            - port: 8888
              protocol: TCP
          from:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  app: prometheus
                  component: server
  auth:
    # common settings when using Canvas Auth
    # Enabled by setting 'custom' as auth.type in individual hub config
    custom:
      className: canvasauthenticator.CanvasAuthenticator
      config:
        canvas_url: https://bcourses.berkeley.edu/
        strip_email_domain: berkeley.edu
        login_service: bCourses
        scope:
          - url:GET|/api/v1/users/:user_id/profile
          - url:GET|/api/v1/courses
        username_key: primary_email
  hub:
    # Disable all probes
    # They seem to just add a lot of requests without much use?
    livenessProbe:
      enabled: false
    readinessProbe:
      enabled: false
    # Generated by chartpress
    image:
      name: gcr.io/ucb-datahub-2018/jupyterhub-hub
      tag: '0.0.1-n3680.h6099a94'
    networkPolicy:
      enabled: true
      ingress:
        - from:
            - namespaceSelector:
                matchLabels:
                  name: support
              podSelector:
                matchLabels:
                  app: prometheus
                  component: server
          ports:
            - port: http
              protocol: TCP
    livenessProbe:
      enabled: true
      initialDelaySeconds: 180
    resources:
      requests:
        # Very small unit, since we don't want any CPU guarantees
        # FIXME: Can't seem to get this to null?
        cpu: 0.001
        memory: 256Mi
      limits:
        memory: 2Gi
    extraEnv:
      # This is unfortunately still needed by canvas auth
      OAUTH2_AUTHORIZE_URL: https://bcourses.berkeley.edu/login/oauth2/auth
    extraConfig:
      01-custom-attr-spawner: |
        import os
        from kubespawner import KubeSpawner
        from tornado import gen
        import z2jh

        hosted_domain = 'berkeley.edu'
        course_profile_tmpl = '/srv/jupyterhub/profiles.d/{}-{}.txt'

        def memparse(val):
          '''Parse memory for relative comparisons.'''
          if type(val) != str or len(val) == 0:
            return val
          mem = int(val.upper()[0:-1])
          unit = val[-1]
          n = {'B':0, 'K':1, 'M':2, 'G':3}[unit]
          return mem * 1024**n

        def mem_cmp(a, b):
          '''Compare memory values.'''
          x = memparse(a)
          y = memparse(b)
          return (x > y) - (x < y)

        def course_profile_filename(course, constituent):
          return course_profile_tmpl.format(course, constituent)

        def course_members(course, constituent, hd=None):
          '''Extracts usernames from files containing email addresses.'''
          members = []
          filename = course_profile_filename(course, constituent)
          if not os.path.exists(filename): return members
          with open(filename) as f:
            line = f.readline()
            while line != '':
              email = line.strip()
              if hd and email.endswith('@' + hd):
                members.append(email.split('@')[0])
              elif not hd:
                members.append(email)
              line = f.readline()
          return members

        class CustomAttrSpawner(KubeSpawner):

          @gen.coroutine
          def start(self):
            # custom.memory
            custom_memory = z2jh.get_config('custom.memory', {})
            for attr, users in custom_memory.items():
              if self.user.name in users:
                self.mem_limit = attr
                self.mem_guarantee = attr
                break

            # custom.profiles
            custom_profiles = z2jh.get_config('custom.profiles', {})
            is_student = False
            is_instructor = False
            for course, profile_data in custom_profiles.items():
              customize = False
              if 'users' in profile_data and self.user.name in profile_data['users']:
                customize = True
              else:
                students = course_members(course, 'students', hosted_domain)
                instructors = course_members(course, 'instructors', hosted_domain)
                is_student |= (self.user.name in students)
                is_instructor |= (self.user.name in instructors)
                customize = self.user.name in students or self.user.name in instructors
              if customize:
                self.log.warning(f'using profile {course}')
                self.volumes += profile_data.get('extraVolumes', [])
                self.volume_mounts += profile_data.get('extraVolumeMounts', [])
                # set new mem thresholds if specified are bigger than current
                if 'mem_limit' in profile_data and \
                mem_cmp(profile_data['mem_limit'], self.mem_limit) == 1:
                  self.mem_limit = profile_data['mem_limit']
                if 'mem_guarantee' in profile_data and \
                mem_cmp(profile_data['mem_guarantee'], self.mem_guarantee) == 1:
                  self.mem_guarantee = profile_data['mem_guarantee']

            # if the user is a student in any course specified by a profile,
            # they never get to be an admin
            if is_student:
              self.user.admin = False
            elif is_instructor:
              self.user.admin = True

            # custom.admin
            custom_admin = z2jh.get_config('custom.admin', {})
            if custom_admin and self.user.admin:
              self.init_containers += custom_admin.get('initContainers', [])
              self.volume_mounts += custom_admin.get('extraVolumeMounts', [])

            return (yield super().start())

        c.JupyterHub.spawner_class = CustomAttrSpawner
        # these are for accessing canvas api
        c.CustomAttrSpawner.env_keep.append('OAUTH2_ACCESS_TOKEN')
        c.CustomAttrSpawner.env_keep.append('OAUTH2_LOGIN_ID')
        # these are for when hub/admin supports a display name
        c.CustomAttrSpawner.env_keep.append('OAUTH2_NAME')
        c.CustomAttrSpawner.env_keep.append('OAUTH2_SORTABLE_NAME')
        # we might not need this one
        c.CustomAttrSpawner.env_keep.append('OAUTH2_PRIMARY_EMAIL')

        c.JupyterHub.extra_log_file = '/srv/jupyterhub/jupyterhub.log'

      03-working-dir: |
        # Make sure working directory is ${HOME}
        # hubploy has a bug where it unconditionally puts workingdir to be /srv/repo
        c.KubeSpawner.working_dir = '/home/jovyan'
      05-prometheus: |
        # Allow unauthenticated prometheus requests
        # Otherwise our prometheus server can't get to these
        c.JupyterHub.authenticate_prometheus = False
      06-no-setuid: |
        c.KubeSpawner.extra_container_config = {
          'securityContext': {
            # Explicitly disallow setuid binaries from working inside the container
            'allowPrivilegeEscalation': False
          }
        }
