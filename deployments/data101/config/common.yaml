nfsPVC:
  enabled: true
  nfs:
    serverIP: 10.156.140.2

jupyterhub:
  scheduling:
    userScheduler:
      nodeSelector:
        hub.jupyter.org/pool-name: core-pool
  prePuller:
    extraImages:
      postgres:
        name: gcr.io/ucb-datahub-2018/jupyterhub-postgres
        tag: 0.0.1-n5296.hdee70868
  proxy:
    chp:
      nodeSelector:
        hub.jupyter.org/pool-name: core-pool

  hub:
    extraConfig:
      data101-new-db: |
        from jupyterhub.utils import exponential_backoff
        from kubespawner.objects import make_pvc
        from functools import partial

        def make_extra_pvc(component, name_template, storage_class, storage_capacity, spawner):
          labels = spawner._build_common_labels({})
          labels.update({
              'component': component
          })
          annotations = spawner._build_common_annotations({})
          storage_selector = spawner._expand_all(spawner.storage_selector)
          return make_pvc(
              name=spawner._expand_all(name_template),
              storage_class=storage_class,
              access_modes=['ReadWriteOnce'],
              selector={},
              storage=storage_capacity,
              labels=labels,
              annotations=annotations
          )

        make_db_pvc = partial(make_extra_pvc, 'db-storage', 'db-{username}', 'standard', '2G')
        make_mongo_pvc = partial(make_extra_pvc, 'mongo-storage', 'mongo-{username}', 'standard', '2G')

        pvc_makers = (make_db_pvc, make_mongo_pvc)
        async def ensure_db_pvc(spawner):
          """"
          Ensure a PVC is created for this user's database volume
          """
          for pvc_maker in pvc_makers:
            pvc = pvc_maker(spawner)
            # If there's a timeout, just let it propagate
            await exponential_backoff(
                partial(spawner._make_create_pvc_request, pvc, spawner.k8s_api_request_timeout),
                f'Could not create pvc {pvc.metadata.name}',
                # Each req should be given k8s_api_request_timeout seconds.
                timeout=spawner.k8s_api_request_retry_timeout
            )
        c.Spawner.pre_spawn_hook = ensure_db_pvc
    nodeSelector:
      hub.jupyter.org/pool-name: core-pool
    config:

      Authenticator:
        admin_users:
          # infrastructure
          - sknapp
          - rylo
          - yuvipanda
          - felder
          - balajialwar
          # List of other admin users
          - lukeliu

  singleuser:
    extraContainers:
      - name: mongo
        image: mongo:5.0.11
        args:
          # We run mongodb without authentication, but only listening on localhost
          # This matches what we do for postgres
          - --bind_ip
          - 127.0.0.1
        volumeMounts:
        - name: home
          mountPath: /home/jovyan
          subPath: "{username}"
        - name: mongodb
          mountPath: /data/db
        securityContext:
          runAsUser: 1000
        resources:
          limits:
            # Best effort only. No more than 1 CPU
            memory: 512Mi
            cpu: 1.0
          requests:
            # If we don't set requests, k8s sets requests == limits!
            memory: 64Mi
            cpu: 0.01
      - name: postgres
        image: gcr.io/ucb-datahub-2018/jupyterhub-postgres:0.0.1-n5296.hdee70868
        resources:
          limits:
            # Best effort only. No more than 1 CPU
            memory: 512Mi
            cpu: 1.0
          requests:
            # If we don't set requests, k8s sets requests == limits!
            memory: 64Mi
            cpu: 0.01
        env:
        - name: POSTGRES_HOST_AUTH_METHOD
          value: "trust"
        - name: POSTGRES_USER
          value: "jovyan"
        securityContext:
          runAsUser: 1000
        volumeMounts:
        - name: home
          mountPath: /home/jovyan
          subPath: "{username}"
        - name: postgres-db
          mountPath: /var/lib/postgresql/data
          # postgres recommends against mounting a volume directly here
          # So we put data in a subpath
          subPath: data
    initContainers:
      # /var/lib/postgresql should be writeable by uid 1000, so students
      # can blow out their db directories if need to. Just setting UID on the
      # postgres server should have been enough - but because we did not do it
      # early enough, many users have data dirs owned by uid 999.
      # We explicitly chown them back. We should remove this eventually.
      - name: postgres-volume-mount-hack
        image: busybox
        command: ["sh", "-c", "id && chown -R 1000:1000 /var/lib/postgresql && ls -lhd /var/lib/postgresql"]
        securityContext:
          runAsUser: 0
        volumeMounts:
        - name: postgres-db
          mountPath: /var/lib/postgresql/data
          # postgres recommends against mounting a volume directly here
          # So we put data in a subpath
          subPath: data
    nodeSelector:
      hub.jupyter.org/pool-name: data101-pool
    defaultUrl: /lab
    storage:
      type: static
      static:
        pvcName: home-nfs-v3
        subPath: "{username}"
      extraVolumes:
        - name: postgres-db
          persistentVolumeClaim:
            claimName: 'db-{username}'
        - name: mongodb
          persistentVolumeClaim:
            claimName: 'mongo-{username}'
      extraVolumeMounts:
        - name: postgres-db
          mountPath: /var/lib/postgresql/data
          # postgres recommends against mounting a volume directly here
          # So we put data in a subpath
          subPath: data
        - name: mongodb
          mountPath: /data/db
    memory:
      guarantee: 512M
      limit: 1G
