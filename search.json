[
  {
    "objectID": "admins/deployments/datahub.html",
    "href": "admins/deployments/datahub.html",
    "title": "DataHub",
    "section": "",
    "text": "datahub.berkeley.edu provides standard computing environment to many foundational courses across diverse disciplines.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Hub Deployments",
      "DataHub"
    ]
  },
  {
    "objectID": "admins/deployments/datahub.html#image",
    "href": "admins/deployments/datahub.html#image",
    "title": "DataHub",
    "section": "Image",
    "text": "Image\nThe datahub image contains both Python and R environments. A user can create jupyter notebooks utilizing either Python or R, or can run RStudio using R or Python.\nThe image is currently not based on repo2docker.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Hub Deployments",
      "DataHub"
    ]
  },
  {
    "objectID": "admins/deployments/datahub.html#resources",
    "href": "admins/deployments/datahub.html#resources",
    "title": "DataHub",
    "section": "Resources",
    "text": "Resources\nA handful of courses have been granted elevated memory limits within the hub configuration.\nCDSS staff and a small number of instructors have been given administrative privileges.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Hub Deployments",
      "DataHub"
    ]
  },
  {
    "objectID": "admins/deployments/stat159.html",
    "href": "admins/deployments/stat159.html",
    "title": "Stat 159",
    "section": "",
    "text": "stat159.datahub.berkeley.edu is a course-specific hub for Stat 159 as taught by Fernando Perez. It tends to include a lot of applications so that students can shift their local development workflows to the cloud.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Hub Deployments",
      "Stat 159"
    ]
  },
  {
    "objectID": "admins/deployments/stat159.html#image",
    "href": "admins/deployments/stat159.html#image",
    "title": "Stat 159",
    "section": "Image",
    "text": "Image\nNotably the image contains support for RTC. As of March 2023, this requires:\n- altair==4.2.2\n- boken==2.4.3\n- dask==2023.1.1\n- jupyter_server==2.2.1\n- jupyterlab==3.6.1\n- jupyterlab_server==2.19.0\n- tornado==6.2.0\n- git+https://&lt;github.com/berkeley-dsep-infra/tmpystore.git@84765e1&gt;\nSome of these are hard requirements and others were necessary to make conda happy.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Hub Deployments",
      "Stat 159"
    ]
  },
  {
    "objectID": "admins/deployments/stat159.html#configuration",
    "href": "admins/deployments/stat159.html#configuration",
    "title": "Stat 159",
    "section": "Configuration",
    "text": "Configuration\nAlong with the dependencies, the singleuser server is modified to launch as\nsingleuser:\n     cmd:\n       - jupyterhub-singleuser\n       - --LabApp.collaborative=true\n       # https://jupyterlab-realtime-collaboration.readthedocs.io/en/latest/configuration.html#configuration\n       - --YDocExtension.ystore_class=tmpystore.TmpYStore\nThis turns on collaboration and moves some sqlite storage from home directories to /tmp/.\nIn addition to RTC, the hub also has configuration to enable shared accounts with impersonation. There are a handful of fabricated user accounts, e.g. collab-shared-1, collab-shared-2, etc. not affiliated with any real person in bCourses. There are also corresponding JupyterHub groups, shared-1, shared-2, etc. The instructors add real students to the hub groups, and some roles and scopes logic in the hub configuration gives students access to launch jupyter servers for the collaborative user accounts. The logic is in config/common.yaml while the current group affiliations are kept private in secrets.\nThis configuration is to encourage use of RTC, and to prevent one student from having too much access to another student's home directory. The fabricated (essentially service) accounts have initally empty home directories and exist solely to provide workspaces for the group. There is currently no archive or restore procedure in mind for these shared accounts.\nFor now, groups are defined in either the hub configuration or in the administrative /hub/admin user interface. In order to enable group assignment in this manner, we must set Authenticator.managed_groups to False. Ordinarily groups are provided by CanvasAuthenticator where this setting is True.\nEventually instructors will be able to define groups in bCourses so that CanvasAuthenticator can remain in charge of managing groups. This will be important for the extremely large courses. It will also be beneficial in that resource allocation can be performed more easily through group affiliations and group properties.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Hub Deployments",
      "Stat 159"
    ]
  },
  {
    "objectID": "admins/deployments/stat159.html#historical-information",
    "href": "admins/deployments/stat159.html#historical-information",
    "title": "Stat 159",
    "section": "Historical Information",
    "text": "Historical Information\nThe image has been periodically shared with data100 for when Fernando has taught both. Going forward, it is probably best to keep them separate and optionally kept in sync. We don't want changes in one course to come as a surprise to the other.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Hub Deployments",
      "Stat 159"
    ]
  },
  {
    "objectID": "admins/storage.html",
    "href": "admins/storage.html",
    "title": "User home directory storage",
    "section": "",
    "text": "All users on all the hubs get a home directory with persistent storage.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "User home directory storage"
    ]
  },
  {
    "objectID": "admins/storage.html#why-nfs",
    "href": "admins/storage.html#why-nfs",
    "title": "User home directory storage",
    "section": "Why NFS?",
    "text": "Why NFS?\nNFS isn't a particularly cloud-native technology. It isn't highly available nor fault tolerant by default, and is a single point of failure. However, it is currently the best of the alternatives available for user home directories, and so we use it.\n\nHome directories need to be fully POSIX compliant file systems that work with minimal edge cases, since this is what most instructional code assumes. This rules out object-store backed filesystems such as s3fs.\nUsers don't usually need guaranteed space or IOPS, so providing them each a persistent cloud disk gets unnecessarily expensive - since we are paying for it whether it is used or not.\nWhen we did use one persistent disk per user, the storage cost dwarfed everything else by an order of magnitude for no apparent benefit.\nAttaching cloud disks to user pods also takes on average about 30s on Google Cloud, and much longer on Azure. NFS mounts pretty quickly, getting this down to a second or less.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "User home directory storage"
    ]
  },
  {
    "objectID": "admins/storage.html#nfs-server",
    "href": "admins/storage.html#nfs-server",
    "title": "User home directory storage",
    "section": "NFS Server",
    "text": "NFS Server\nWe currently have two approaches to running NFS Servers.\n\nRun a hand-maintained NFS Server with ZFS SSD disks.\nThis gives us control over performance, size and most importantly, server options. We use anonuid=1000, so all reads / writes from the cluster are treated as if they have uid 1000, which is the uid all user processes run as. This prevents us from having to muck about permissions & chowns - particularly since Kubernetes creates new directories on volumes as root with strict permissions (see issue).\nUse a hosted NFS service like Google Cloud Filestore.\nWe do not have to perform any maintenance if we use this - but we have no control over the host machine either.\n\nAfter running our own NFS server from 2020 through the end of 2022, we decided to move wholesale to Google Cloud Filestore. This was mostly due to NFS daemon stability issues, which caused many outages and impacted thousands of our users and courses.\nCurrently each hub has it's own filestore instance, except for a few small courses that share one. This has proven to be much more stable and able to handle the load.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "User home directory storage"
    ]
  },
  {
    "objectID": "admins/storage.html#home-directory-paths",
    "href": "admins/storage.html#home-directory-paths",
    "title": "User home directory storage",
    "section": "Home directory paths",
    "text": "Home directory paths\nEach user on each hub gets their own directory on the server that gets treated as their home directory. The staging & prod servers share home directory paths, so users get the same home directories on both.\nFor most hubs, the user's home directory path relative to the exported filestore share is &lt;hub-name&gt;-filestore/&lt;hub-name&gt;/&lt;prod|staging&gt;/home/&lt;user-name&gt;.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "User home directory storage"
    ]
  },
  {
    "objectID": "admins/storage.html#nfs-client",
    "href": "admins/storage.html#nfs-client",
    "title": "User home directory storage",
    "section": "NFS Client",
    "text": "NFS Client\nWe currently have two approaches for mounting the user's home directory into each user's pod.\n\nMount the NFS Share once per node to a well known location, and use hostpath volumes with a subpath on the user pod to mount the correct directory on the user pod.\nThis lets us get away with one NFS mount per node, rather than one per pod.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "User home directory storage"
    ]
  },
  {
    "objectID": "admins/cluster-config.html",
    "href": "admins/cluster-config.html",
    "title": "Kubernetes Cluster Configuration",
    "section": "",
    "text": "We use kubernetes to run our JupyterHubs. It has a healthy open source community, managed offerings from multiple vendors & a fast pace of development. We can run easily on many different cloud providers with similar config by running on top of Kubernetes, so it is also our cloud agnostic abstraction layer.\nWe prefer using a managed Kubernetes service (such as Google Kubernetes Engine). This document lays out our preferred cluster configuration on various cloud providers.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Kubernetes Cluster Configuration"
    ]
  },
  {
    "objectID": "admins/cluster-config.html#google-kubernetes-engine",
    "href": "admins/cluster-config.html#google-kubernetes-engine",
    "title": "Kubernetes Cluster Configuration",
    "section": "Google Kubernetes Engine",
    "text": "Google Kubernetes Engine\nIn our experience, Google Kubernetes Engine (GKE) has been the most stable, performant, and reliable managed kubernetes service. We prefer running on this when possible.\nA gcloud container clusters create command can succintly express the configuration of our kubernetes cluster. The following command represents the currently favored configuration.\nThis creates the GKE cluster. It may host one or more node pools:\ngcloud container clusters create \\\n     --enable-ip-alias \\\n     --enable-autoscaling \\\n     --max-nodes=20 --min-nodes=1 \\\n     --region=us-central1 --node-locations=us-central1-b \\\n     --image-type=cos_containerd \\\n     --disk-size=100 --disk-type=pd-balanced \\\n     --machine-type=n2-highmem-8 \\\n     --cluster-version latest \\\n     --no-enable-autoupgrade \\\n     --enable-network-policy \\\n     --create-subnetwork=\"\" \\\n     --tags=hub-cluster \\\n     &lt;cluster-name&gt;\nHere's how we add a node pool to the cluster, beyond the default pool:\ngcloud container node-pools create  \\\n    --machine-type n2-highmem-8 \\\n    --num-nodes 1 \\\n    --enable-autoscaling \\\n    --min-nodes 1 --max-nodes 20 \\\n    --node-labels hub.jupyter.org/pool-name=&lt;pool-name&gt;-pool \\\n    --node-taints hub.jupyter.org_dedicated=user:NoSchedule \\\n    --region=us-central1 \\\n    --image-type=cos_containerd \\\n    --disk-size=200 --disk-type=pd-balanced \\\n    --no-enable-autoupgrade \\\n    --tags=hub-cluster \\\n    --cluster=&lt;cluster-name&gt; \\\n    user-&lt;pool-name&gt;-&lt;yyyy&gt;-&lt;mm&gt;-&lt;dd&gt;\n\nIP Aliasing\n--enable-ip-alias creates VPC Native Clusters.\nThis becomes the default soon, and can be removed once it is the default.\n\n\nAutoscaling\nWe use the kubernetes cluster autoscaler to scale our node count up and down based on demand. It waits until the cluster is completely full before triggering creation of a new node - but that's ok, since new node creation time on GKE is pretty quick.\n--enable-autoscaling turns the cluster autoscaler on.\n--min-nodes sets the minimum number of nodes that will be maintained regardless of demand. This should ideally be 2, to give us some headroom for quick starts without requiring scale ups when the cluster is completely empty.\n--max-nodes sets the maximum number of nodes that the cluster autoscaler will use - this sets the maximum number of concurrent users we can support. This should be set to a reasonably high number, but not too high - to protect against runaway creation of hundreds of VMs that might drain all our credits due to accident or security breach.\n\n\nHighly available master\nThe kubernetes cluster's master nodes are managed by Google Cloud automatically. By default, it is deployed in a non-highly-available configuration - only one node. This means that upgrades and master configuration changes cause a few minutes of downtime for the kubernetes API, causing new user server starts / stops to fail.\nWe request our cluster masters to have highly available masters with --region parameter. This specifies the region where our 3 master nodes will be spread across in different zones. It costs us extra, but it is totally worth it.\nBy default, asking for highly available masters also asks for 3x the node count, spread across multiple zones. We don't want that, since all our user pods have in-memory state & can't be relocated. Specifying --node-locations explicitly lets us control how many and which zones the nodes are located in.\n\n\nRegion / Zone selection\nWe generally use the us-central1 region and a zone in it for our clusters -simply because that is where we have asked for quota.\nThere are regions closer to us, but latency hasn't really mattered so we are currently still in us-central1. There are also unsubstantiated rumors that us-central1 is their biggest data center and hence less likely to run out of quota.\n\n\nDisk Size\n--disk-size sets the size of the root disk on all the kubernetes nodes. This isn't used for any persistent storage such as user home directories. It is only used ephemerally for the operations of the cluster - primarily storing docker images and other temporary storage. We can make this larger if we use a large number of big images, or if we want our image pulls to be faster (since disk performance increases with disk size ).\n--disk-type=pd-standard gives us standard spinning disks, which are cheaper. We can also request SSDs instead with --disk-type=pd-ssd - it is much faster, but also much more expensive. We compromise with --disk-type=pd-balanced, faster than spinning disks but not as fast as ssds all the time.\n\n\nNode size\n--machine-type lets us select how much RAM and CPU each of our nodes have. For non-trivial hubs, we generally pick n2-highmem-8, with 64G of RAM and 8 cores. This is based on the following heuristics:\n\nStudents generally are memory limited than CPU limited. In fact, while we have a hard limit on memory use per-user pod, we do not have a CPU limit -it hasn't proven necessary.\nWe try overprovision clusters by about 2x - so we try to fit about 100G of total RAM use in a node with about 50G of RAM. This is accomplished by setting the memory request to be about half of the memory limit on user pods. This leads to massive cost savings, and works out ok.\nThere is a kubernetes limit on 100 pods per node.\n\nBased on these heuristics, n2-highmem-8 seems to be most bang for the buck currently. We should revisit this for every cluster creation.\n\n\nCluster version\nGKE automatically upgrades cluster masters, so there is generally no harm in being on the latest version available.\n\n\nNode autoupgrades\nWhen node autoupgrades are enabled, GKE will automatically try to upgrade our nodes whenever needed (our GKE version falling off the support window, security issues, etc). However, since we run stateful workloads, we disable this right now so we can do the upgrades manually.\n\n\nNetwork Policy\nKubernetes Network Policy lets you firewall internal access inside a kubernetes cluster, whitelisting only the flows you want. The JupyterHub chart we use supports setting up appropriate NetworkPolicy objects it needs, so we should turn it on for additional security depth. Note that any extra in-cluster services we run must have a NetworkPolicy set up for them to work reliabliy.\n\n\nSubnetwork\nWe put each cluster in its own subnetwork, since seems to be a limit on how many clusters you can create in the same network with IP aliasing on - you just run out of addresses. This also gives us some isolation - subnetworks are isolated by default and can't reach other resources. You must add firewall rules to provide access, including access to any manually run NFS servers. We add tags for this.\n\n\nTags\nTo help with firewalling, we add network tags to all our cluster nodes. This lets us add firewall rules to control traffic between subnetworks.\n\n\nCluster name\nWe try use a descriptive name as much as possible.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Kubernetes Cluster Configuration"
    ]
  },
  {
    "objectID": "admins/howto/dns.html",
    "href": "admins/howto/dns.html",
    "title": "Update DNS",
    "section": "",
    "text": "Some staff have access to make and update DNS entries in the .datahub.berkeley.edu and .data8x.berkeley.edu subdomains.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Update DNS"
    ]
  },
  {
    "objectID": "admins/howto/dns.html#authorization",
    "href": "admins/howto/dns.html#authorization",
    "title": "Update DNS",
    "section": "Authorization",
    "text": "Authorization\nRequest access to make changes by creating an issue in this repository.\nAuthorization is granted via membership in the edu:berkeley:org:nos:DDI:datahub CalGroup. @yuvipanda and @ryanlovett are group admins and can update membership.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Update DNS"
    ]
  },
  {
    "objectID": "admins/howto/dns.html#making-changes",
    "href": "admins/howto/dns.html#making-changes",
    "title": "Update DNS",
    "section": "Making Changes",
    "text": "Making Changes\n\nLog into Infoblox from a campus network or through the campus VPN. Use your CalNet credentials.\nNavigate to Data Management &gt; DNS &gt; Zones and click berkeley.edu.\nNavigate to Subzones and choose either data8x or datahub, then click Records.\n\n\nFor quicker access, click the star next to the zone name to make a bookmark in the Finder pane on the left side.\n\n\nCreate a new record\n\nClick the down arrow next to + Add in the right-side Toolbar. Then choose Record &gt; A Record.\nEnter the name and IP of the A record, and uncheck Create associated PTR record.\nConsider adding a comment with a timestamp, your ID, and the nature of the change.\nClick Save & Close.\n\n\n\nEdit an existing record\n\nClick the gear icon to the left of the record's name and choose Edit.\nMake a change.\nConsider adding a comment with a timestamp, your ID, and the nature of the change.\nClick Save & Close.\n\n\n\nDelete a record\n\nClick the gear icon to the left of the record's name and choose Delete.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Update DNS"
    ]
  },
  {
    "objectID": "admins/howto/remove-users-orm.html",
    "href": "admins/howto/remove-users-orm.html",
    "title": "Remove inactive users from hub ORM",
    "section": "",
    "text": "JupyterHub performance sometimes scales with the total number of users in its ORM database, rather than the number of running users. Reducing the user count enables the hub to restart much faster. While this issue should be addressed, we can work around it by deleting inactive users from the hub database once in a while. Note that this does not delete the user’s storage. The script scripts/delete-unused-users.py will delete anyone who hasn’t registered any activity in a given period of time, double checking to make sure they aren’t active right now. This will require users to log in again the next time they use the hub, but that is probably fine. This should be done before the start of each semester, particularly on hubs with a lot of users.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Remove inactive users from hub ORM"
    ]
  },
  {
    "objectID": "admins/howto/remove-users-orm.html#run-the-script",
    "href": "admins/howto/remove-users-orm.html#run-the-script",
    "title": "Remove inactive users from hub ORM",
    "section": "Run the script",
    "text": "Run the script\nYou can run the script on your own device. The script depends on the jhub_client python library. This can be installed with pip install jhub_client.\n\nYou will need to acquire a JupyterHub API token with administrative rights. A hub admin can go to {hub_url}/hub/token to create a new one.\nSet the environment variable JUPYTERHUB_API_TOKEN to the token.\nRun python scripts/delete-unused-users.py --hub_url {hub_url}",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Remove inactive users from hub ORM"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html",
    "href": "admins/howto/clusterswitch.html",
    "title": "Switching over a hub to a new cluster",
    "section": "",
    "text": "This document describes how to switch an existing hub to a new cluster. The example used here refers to moving all UC Berkeley Datahubs.\nYou might find it easier to switch to a new cluster if you’re running a very old k8s version, or in lieu of performing a cluster credential rotation. Sometimes starting from scratch is easier than an iterative and potentially destructive series of operations.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#create-a-new-cluster",
    "href": "admins/howto/clusterswitch.html#create-a-new-cluster",
    "title": "Switching over a hub to a new cluster",
    "section": "Create a new cluster",
    "text": "Create a new cluster\n\nCreate a new cluster using the specifications here:\nhttps://docs.datahub.berkeley.edu/en/latest/admins/cluster-config.html\nSet up helm on the cluster according to the instructions here:\nhttp://z2jh.jupyter.org/en/latest/setup-helm.html\n\nMake sure the version of helm you’re working with matches the version CircleCI is using.\nFor example: https://github.com/berkeley-dsep-infra/datahub/blob/staging/.circleci/config.yml#L169\n\nRe-create all existing node pools for hubs, support and prometheus deployments in the new cluster. If the old cluster is still up and running, you will probably run out of CPU quota, as the new node pools will immediately default to three nodes. Wait ~15m for the new pools to wind down to zero, and then continue.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#setting-the-context-for-kubectl-and-work-on-the-new-cluster.",
    "href": "admins/howto/clusterswitch.html#setting-the-context-for-kubectl-and-work-on-the-new-cluster.",
    "title": "Switching over a hub to a new cluster",
    "section": "Setting the ‘context’ for kubectl and work on the new cluster.",
    "text": "Setting the ‘context’ for kubectl and work on the new cluster.\n\nEnsure you’re logged in to GCP: gcloud auth login\nPull down the credentials from the new cluster: gcloud container clusters get-credentials &lt;CLUSTER_NAME&gt; --region us-central1\nSwitch the kubectl context to this cluster: kubectl config use-context gke_ucb-datahub-2018_us-central1_&lt;CLUSTER_NAME&gt;",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#recreate-node-pools",
    "href": "admins/howto/clusterswitch.html#recreate-node-pools",
    "title": "Switching over a hub to a new cluster",
    "section": "Recreate node pools",
    "text": "Recreate node pools\nRe-create all existing node pools for hubs, support and prometheus deployments in the new cluster.\nIf the old cluster is still up and running, you will probably run out of CPU quota, as the new node pools will immediately default to three nodes. Wait ~15m for the new pools to wind down to zero, and then continue.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#install-and-configure-the-certificate-manager",
    "href": "admins/howto/clusterswitch.html#install-and-configure-the-certificate-manager",
    "title": "Switching over a hub to a new cluster",
    "section": "Install and configure the certificate manager",
    "text": "Install and configure the certificate manager\nBefore you can deploy any of the hubs or support tooling, the certificate manager must be installed and configured on the new cluster. Until this is done, hubploy and helm will fail with the following error: ensure CRDs are installed first.\n\nCreate a new feature branch and update your helm dependencies: helm dep up\nAt this point, it’s usually wise to upgrade cert-manager to the latest version found in the chart repo. You can find this by running the following command:\ncert-manager-version=$(helm show all -n cert-manager jetstack/cert-manager | grep ^appVersion |  awk '{print $2}')\nThen, you can install the latest version of cert-manager:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/${cert-manager-version}/cert-manager.yaml\nChange the corresponding entry in support/requirements.yaml to $cert-manager-version and commit the changes (do not push).",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#create-the-node-placeholder-k8s-namespace",
    "href": "admins/howto/clusterswitch.html#create-the-node-placeholder-k8s-namespace",
    "title": "Switching over a hub to a new cluster",
    "section": "Create the node-placeholder k8s namespace",
    "text": "Create the node-placeholder k8s namespace\nThe calendar autoscaler requires the node-placeholder namespace. Run the following command to create it:\nkubectl create namespace node-placeholder",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#create-a-new-static-ip-and-switch-dns-to-point-our-new-deployment-at-it.",
    "href": "admins/howto/clusterswitch.html#create-a-new-static-ip-and-switch-dns-to-point-our-new-deployment-at-it.",
    "title": "Switching over a hub to a new cluster",
    "section": "Create a new static IP and switch DNS to point our new deployment at it.",
    "text": "Create a new static IP and switch DNS to point our new deployment at it.\n\nCreate a new static IP in the GCP console.\nOpen infoblox and change the wildcard and empty entries for datahub.berkeley.edu to point to the IP from the previous step.\nUpdate support/values.yaml, under ingress-nginx with the newly created IP from infoblox: loadBalancerIP: xx.xx.xx.xx.\nAdd and commit this change to your feature branch (still do not push).\n\nYou will re-deploy the support chart in the next step.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#manually-deploy-the-support-and-prometheus-pools",
    "href": "admins/howto/clusterswitch.html#manually-deploy-the-support-and-prometheus-pools",
    "title": "Switching over a hub to a new cluster",
    "section": "Manually deploy the support and prometheus pools",
    "text": "Manually deploy the support and prometheus pools\nFirst, update any node pools in the configs to point to the new cluster. Typically, this is just for the ingress-nginx controllers in support/values.yaml.\nNow we will manually deploy the support helm chart:\nsops -d support/secrets.yaml &gt; /tmp/secrets.yaml\nhelm install -f support/values.yaml -f /tmp/secrets.yaml \\\n    -n support support support/ \\\n    --set installCRDs=true --debug --create-namespace\nBefore continuing, confirm via the GCP console that the IP that was defined in step 1 is now bound to a forwarding rule. You can further confirm by listing the services in the support chart and making sure the ingress-controller is using the newly defined IP.\nOne special thing to note: our prometheus instance uses a persistent volume that contains historical monitoring data. This is specified in support/values.yaml, under the prometheus: block:\npersistentVolume:\n  size: 1000Gi\n  storageClass: ssd\n  existingClaim: prometheus-data-2024-05-15",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#manually-deploy-a-hub-to-staging",
    "href": "admins/howto/clusterswitch.html#manually-deploy-a-hub-to-staging",
    "title": "Switching over a hub to a new cluster",
    "section": "Manually deploy a hub to staging",
    "text": "Manually deploy a hub to staging\nFinally, we can attempt to deploy a hub to the new cluster! Any hub will do, but we should start with a low-traffic hub (eg: https://dev.datahub.berkeley.edu).\nFirst, check the hub’s configs for any node pools that need updating. Typically, this is just the core pool.\nSecond, update hubploy.yaml for this hub and point it to the new cluster you’ve created.\nAfter this is done, add the changes to your feature branch (but don’t push). After that, deploy a hub manually:\nhubploy deploy dev hub staging\nWhen the deploy is done, visit that hub and confirm that things are working.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#manually-deploy-remaining-hubs-to-staging-and-prod",
    "href": "admins/howto/clusterswitch.html#manually-deploy-remaining-hubs-to-staging-and-prod",
    "title": "Switching over a hub to a new cluster",
    "section": "Manually deploy remaining hubs to staging and prod",
    "text": "Manually deploy remaining hubs to staging and prod\nNow, update the remaining hubs’ configs to point to the new node pools and hubploy.yaml to the cluster.\nThen use hubploy to deploy them to staging as with the previous step. The easiest way to do this is to have a list of hubs in a text file, and iterate over it with a for loop:\nfor x in $(cat hubs.txt); do hubploy deploy ${x} hub staging; done\nfor x in $(cat hubs.txt); do hubploy deploy ${x} hub prod; done\nWhen done, add the modified configs to your feature branch (and again, don’t push yet).",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#update-circleci",
    "href": "admins/howto/clusterswitch.html#update-circleci",
    "title": "Switching over a hub to a new cluster",
    "section": "Update CircleCI",
    "text": "Update CircleCI\nOnce you’ve successfully deployed the clusters manually via hubploy, it’s time to update CircleCI to point to the new cluster.\nAll you need to do is grep for the old cluster name in .circleci/config.yaml and change this to the name of the new cluster. There should just be four entries: two for the gcloud get credentials &lt;cluster-name&gt;, and two in comments. Make these changes and add them to your existing feature branch, but don’t commit yet.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#create-and-merge-your-pr",
    "href": "admins/howto/clusterswitch.html#create-and-merge-your-pr",
    "title": "Switching over a hub to a new cluster",
    "section": "Create and merge your PR!",
    "text": "Create and merge your PR!\nNow you can finally push your changes to github. Create a PR, merge to staging and immediately kill off the deploy jobs for node-placeholder, support and deploy.\nCreate another PR to merge to prod and that deploy should work just fine.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#update-log-and-billing-sinks-bigquery-queries-etc.",
    "href": "admins/howto/clusterswitch.html#update-log-and-billing-sinks-bigquery-queries-etc.",
    "title": "Switching over a hub to a new cluster",
    "section": "Update log and billing sinks, BigQuery queries, etc.",
    "text": "Update log and billing sinks, BigQuery queries, etc.\nI would recommend searching GCP console for all occurrences of the old cluster name, and fixing any bits that might be left over. This should only take a few minutes, but should definitely be done.\nFIN!",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/clusterswitch.html#deleting-the-old-cluster",
    "href": "admins/howto/clusterswitch.html#deleting-the-old-cluster",
    "title": "Switching over a hub to a new cluster",
    "section": "Deleting the old cluster",
    "text": "Deleting the old cluster\nAfter waiting a reasonable period of time (a day or two just to be cautious) and after fetching the usage logs, you may delete the old cluster:\ngcloud container clusters delete ${OLDCLUSTER} --region=us-central1",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Switching over a hub to a new cluster"
    ]
  },
  {
    "objectID": "admins/howto/rebuild-hub-image.html",
    "href": "admins/howto/rebuild-hub-image.html",
    "title": "Rebuild a custom hub image",
    "section": "",
    "text": "We use a customized JupyterHub image so we can use versions of hub packages (such as authenticators) and install additional software required by custom config we might have.\nThe image is located in images/hub. It must inherit from the JupyterHub image used in the Zero to JupyterHub.\nchartpress is used to build the image and update hub/values.yaml with the new image version. chartpress may be installed locally with pip install chartpress.\n\nRun gcloud auth configure-docker us-central1-docker.pkg.dev once per machine to setup docker for authentication with the gcloud credential helper.\nModify the image in images/hub and make a git commit.\nRun chartpress --push. This will build and push the hub image, and modify hub/values.yaml appropriately.\nMake a commit with the hub/values.yaml file, so the new hub image name and tag are comitted.\nProceed to deployment as normal.\n\nSome of the following commands may be required to configure your environment to run the above chartpress workflow successfully:\n\ngcloud auth login\ngcloud auth configure-docker us-central1-docker.pkg.dev\ngcloud auth application-default login\nsometimes running gcloud auth login additional time(s) may fix issues\nsudo usermod -a -G docker ${USER}\ngcloud auth configure-docker\n\n\nRebuild the custom postgres image\nFor data100, we provide a postgresql server per user. We want the python extension installed. So we inherit from the upstream postgresql docker image, and add the appropriate package.\nThis image is in images/postgres. If you update it, you need to rebuild and push it.\n\nModify the image in images/postgres and make a git commit.\nRun chartpress --push. This will build and push the image, but not put anything in YAML. There is no place we can put thi in values.yaml, since this is only used for data100.\nNotice the image name + tag from the chartpress --push command, and put it in the appropriate place (under extraContainers) in data100/config/common.yaml.\nMake a commit with the new tag in data100/config/common.yaml.\nProceed to deploy as normal.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Rebuild a custom hub image"
    ]
  },
  {
    "objectID": "admins/howto/google-sheets.html",
    "href": "admins/howto/google-sheets.html",
    "title": "Reading Google Sheets from DataHub",
    "section": "",
    "text": "Available in: DataHub\nWe provision and make available credentials for a service account that can be used to provide readonly access to Google Sheets. This is useful in pedagogical situations where data is read from Google Sheets, particularly with the gspread library.\nThe entire contents of the JSON formatted service account key is available as an environment variable GOOGLE_SHEETS_READONLY_KEY. You can use this to read publicly available Google Sheet documents.\nThe service account has no implicit permissions, and can be found under singleuser.extraEnv.GOOGLE_SHEETS_READONLY_KEY in datahub/secrets/staging.yaml and datahub/secrets/prod.yaml.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Reading Google Sheets from DataHub"
    ]
  },
  {
    "objectID": "admins/howto/google-sheets.html#gspread-sample-code",
    "href": "admins/howto/google-sheets.html#gspread-sample-code",
    "title": "Reading Google Sheets from DataHub",
    "section": "gspread sample code",
    "text": "gspread sample code\nThe following sample code reads a sheet from a URL given to it, and prints the contents.\nimport gspread\nimport os\nimport json\nfrom oauth2client.service_account import ServiceAccountCredentials\n\n# Authenticate to Google\nscope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\ncreds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(os.environ['GOOGLE_SHEETS_READONLY_KEY']), scope)\ngc = gspread.authorize(creds)\n\n# Pick URL of Google Sheet to open\nurl = 'https://docs.google.com/spreadsheets/d/1SVRsQZWlzw9lV0MT3pWlha_VCVxWovqvu-7cb3feb4k/edit#gid=0'\n\n# Open the Google Sheet, and print contents of sheet 1\nsheet = gc.open_by_url(url)\nprint(sheet.sheet1.get_all_records())",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Reading Google Sheets from DataHub"
    ]
  },
  {
    "objectID": "admins/howto/google-sheets.html#gspread-pandas-sample-code",
    "href": "admins/howto/google-sheets.html#gspread-pandas-sample-code",
    "title": "Reading Google Sheets from DataHub",
    "section": "gspread-pandas sample code",
    "text": "gspread-pandas sample code\nThe gspread-pandas library helps get data from Google Sheets into a pandas dataframe.\nfrom gspread_pandas.client import Spread\nimport os\nimport json\nfrom oauth2client.service_account import ServiceAccountCredentials\n\n# Authenticate to Google\nscope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\ncreds = ServiceAccountCredentials.from_json_keyfile_dict(json.loads(os.environ['GOOGLE_SHEETS_READONLY_KEY']), scope)\n\n# Pick URL of Google Sheet to open\nurl = 'https://docs.google.com/spreadsheets/d/1SVRsQZWlzw9lV0MT3pWlha_VCVxWovqvu-7cb3feb4k/edit#gid=0'\n\n# Open the Google Sheet, and print contents of sheet 1 as a dataframe\nspread = Spread(url, creds=creds)\nsheet_df = spread.sheet_to_df(sheet='sheet1')\nprint(sheet_df)",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Reading Google Sheets from DataHub"
    ]
  },
  {
    "objectID": "admins/howto/new-image.html",
    "href": "admins/howto/new-image.html",
    "title": "Creating a new single user image",
    "section": "",
    "text": "When deploying a new hub, or moving from a shared single user server image, you might need to create a new image for users. We use repo2docker to do this.\nThere are two approaches to creating a repo2docker image: 1. Use a repo2docker-style image template (environment.yaml, etc) 2. Use a Dockerfile (useful for larger/more complex images)\nGenerally, we prefer to use the former approach, unless we need to install specific packages or utilities outside of python/apt as root. If that is the case, only a Dockerfile format will work.\nOf course, as always create a feature branch for your changes, and submit a PR when done.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating a new single user image"
    ]
  },
  {
    "objectID": "admins/howto/new-image.html#find-a-hub-to-use-as-a-template",
    "href": "admins/howto/new-image.html#find-a-hub-to-use-as-a-template",
    "title": "Creating a new single user image",
    "section": "Find a hub to use as a template",
    "text": "Find a hub to use as a template\nBrowse through our deployments/ directory to find a hub that is similar to the one you are trying to create. This will give you a good starting point.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating a new single user image"
    ]
  },
  {
    "objectID": "admins/howto/new-image.html#create-the-image-directory-for-your-new-hub",
    "href": "admins/howto/new-image.html#create-the-image-directory-for-your-new-hub",
    "title": "Creating a new single user image",
    "section": "Create the image/ directory for your new hub",
    "text": "Create the image/ directory for your new hub\nCreate a new directory under deployments/ with the name of your hub. This directory will contain the files that will be used to create the image.\nThen, copy the contents (and any subdirectories) of the source image/ directory in to the new directory.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating a new single user image"
    ]
  },
  {
    "objectID": "admins/howto/new-image.html#modify-hubploy.yaml-for-the-hub",
    "href": "admins/howto/new-image.html#modify-hubploy.yaml-for-the-hub",
    "title": "Creating a new single user image",
    "section": "Modify hubploy.yaml for the hub",
    "text": "Modify hubploy.yaml for the hub\nIn the deployment's hubploy.yaml file, add or modify the name, path and base_image fields to configure the image build and where it's stored in the Google Artifcat Registry.\nname should contain the path to the image in the Google Artifact Registry and the name of the image. path points to the directory containing the image configuration (typically :file::image/. base_image is the base Docker image to use for the image build.\nFor example, hubploy.yaml for the data100 image looks like this:\nimages:\n   images:\n      - name: us-central1-docker.pkg.dev/ucb-datahub-2018/user-images/data100-user-image\n         path: image/\n         repo2docker:\n            base_image: docker.io/library/buildpack-deps:jammy\n   registry:\n      provider: gcloud\n      gcloud:\n         project: ucb-datahub-2018\n         service_key: gcr-key.json\n\ncluster:\nprovider: gcloud\ngcloud:\n   project: ucb-datahub-2018\n   service_key: gke-key.json\n   cluster: spring-2024\n   zone: us-central1",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating a new single user image"
    ]
  },
  {
    "objectID": "admins/howto/new-image.html#modify-the-image-configuration-as-necessary",
    "href": "admins/howto/new-image.html#modify-the-image-configuration-as-necessary",
    "title": "Creating a new single user image",
    "section": "Modify the image configuration as necessary",
    "text": "Modify the image configuration as necessary\nThis step is straightforward: edit/modify/delete/add any files in the image/ directory to configure the image as needed.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating a new single user image"
    ]
  },
  {
    "objectID": "admins/howto/new-image.html#update-cicd-configuration",
    "href": "admins/howto/new-image.html#update-cicd-configuration",
    "title": "Creating a new single user image",
    "section": "Update CI/CD configuration",
    "text": "Update CI/CD configuration\nNext, ensure that this image will be built and deployed by updating the .circleci/config.yml file in the root of the repository. Add new steps under the jobs/deploy:, workflows/test-build-images: and workflows/deploy: stanzas.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating a new single user image"
    ]
  },
  {
    "objectID": "admins/howto/new-image.html#submitting-a-pull-request",
    "href": "admins/howto/new-image.html#submitting-a-pull-request",
    "title": "Creating a new single user image",
    "section": "Submitting a pull request",
    "text": "Submitting a pull request\nFamiliarize yourself with pull requests and repo2docker , and create a fork of the datahub staging branch.\n\nSet up your git/dev environment by following the instructions here.\nCreate a new branch for this PR.\n\nTest the changes locally using repo2docker, then submit a PR to staging.\n\n\nTo use repo2docker, you have to point it at the correct image directory. For example, to build the data100 image, you would run repo2docker deployments/data100/image from the base datahub directory.\n\n\n\nCommit and push your changes to your fork of the datahub repo, and create a new pull request at https://github.com/berkeley-dsep-infra/datahub/.\nOnce the PR is merged to staging and the new image is built and pushed to Artifact Registry, you can test it out on &lt;hub&gt;-staging.datahub.berkeley.edu.\nChanges are only deployed to prod once the relevant CI job is completed. See https://circleci.com/gh/berkeley-dsep-infra/datahub to view CircleCI job statuses.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating a new single user image"
    ]
  },
  {
    "objectID": "admins/howto/preview-local.html",
    "href": "admins/howto/preview-local.html",
    "title": "Develop Documentation",
    "section": "",
    "text": "Navigate to the docs directory and run quarto preview. You can view the documentation in a browser while you make changes.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Develop Documentation"
    ]
  },
  {
    "objectID": "admins/howto/preview-local.html#live-preview",
    "href": "admins/howto/preview-local.html#live-preview",
    "title": "Develop Documentation",
    "section": "",
    "text": "Navigate to the docs directory and run quarto preview. You can view the documentation in a browser while you make changes.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Develop Documentation"
    ]
  },
  {
    "objectID": "admins/howto/preview-local.html#render-static-html",
    "href": "admins/howto/preview-local.html#render-static-html",
    "title": "Develop Documentation",
    "section": "Render Static HTML",
    "text": "Render Static HTML\nNavigate to the docs directory and run quarto render. This will build the endire website into the *_site* directory. You can then open files in your web browser.\nYou can also render individual files, which saves time if you do not want to render the whole site. Run quarto render ./path/to/filename.qmd, and then open the corresponding HTML file in the *_site* directory.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Develop Documentation"
    ]
  },
  {
    "objectID": "admins/pre-reqs.html",
    "href": "admins/pre-reqs.html",
    "title": "Pre-requisites",
    "section": "",
    "text": "Smoothly working with the JupyterHubs maintained in this repository has a number of pre-requisite skills you must possess. The rest of the documentation assumes you have at least a basic level of these skills, and know how to get help related to these technologies when necessary.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Pre-requisites"
    ]
  },
  {
    "objectID": "admins/pre-reqs.html#basic",
    "href": "admins/pre-reqs.html#basic",
    "title": "Pre-requisites",
    "section": "Basic",
    "text": "Basic\nThese skills let you interact with the repository in a basic manner. This lets you do most 'self-service' tasks - such as adding admin users, libraries, making changes to resource allocation, etc. This doesn't give you any skills to debug things when they break, however.\n\nBasic git & GitHub skills.\nThe Git Book & GitHub Help are good resources for this.\nFamiliarity with YAML syntax.\nUnderstanding of how packages are installed in the languages we support.\nRights to merge changes into this repository on GitHub.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Pre-requisites"
    ]
  },
  {
    "objectID": "admins/pre-reqs.html#full",
    "href": "admins/pre-reqs.html#full",
    "title": "Pre-requisites",
    "section": "Full",
    "text": "Full\nIn addition to the basic skills, you'll need the following skills to 'fully' work with this repository. Primarily, you need this to debug issues when things break -since we strive to never have things break in the same way more than twice.\n\nKnowledge of our tech stack:\n\nKubernetes\nGoogle Cloud\nHelm\nDocker\nrepo2docker\nJupyter\nLanguages we support: Python & R\n\nUnderstanding of our JupyterHub distribution, Zero to JupyterHub.\nFull access to the various cloud providers we use.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Pre-requisites"
    ]
  },
  {
    "objectID": "policy/create_policy.html",
    "href": "policy/create_policy.html",
    "title": "Process to publish policy proposals",
    "section": "",
    "text": "Process to publish policy proposals\n\nThe policy subfolder inside the docs folder in the datahub repository acts as a single source of truth for all our policy-related proposals.\nEvery PR related to a specific policy will go through multiple reviews from the team before getting finalized and merged. Reviewers should be explicitly assigned in order to seek the required approval prior to merging the PR.\nEach policy issue area will get mapped to a specific policy document. Eg: The policy to create a new hub is currently mapped to the policy_create_hubs.rst file.",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Process to publish policy proposals"
    ]
  },
  {
    "objectID": "policy/storage-retention.html",
    "href": "policy/storage-retention.html",
    "title": "Storage Retention Policy",
    "section": "",
    "text": "No non-hidden files in the user's home directory that have been modified in the last 6 months.\n\n\n\n\nZip the whole home directory\nUpload it to Google drive of a SPA created for this purpose\nShare the ZIP file in the Google Drive with the user."
  },
  {
    "objectID": "policy/storage-retention.html#policy",
    "href": "policy/storage-retention.html#policy",
    "title": "Storage Retention Policy",
    "section": "",
    "text": "No non-hidden files in the user's home directory that have been modified in the last 6 months.\n\n\n\n\nZip the whole home directory\nUpload it to Google drive of a SPA created for this purpose\nShare the ZIP file in the Google Drive with the user."
  },
  {
    "objectID": "policy/storage-retention.html#rationale",
    "href": "policy/storage-retention.html#rationale",
    "title": "Storage Retention Policy",
    "section": "Rationale",
    "text": "Rationale\nToday (6 Feb 2020), we have 18,623 home directories in datahub. Most of these users used datahub in previous semesters, have not logged in for a long time, and will probably never log in again. This costs us a lot of money in disk space - we will have to forever expand disk space.\nBy cleaning it up after 6 months of non-usage, we will not affect any current users - just folks who haven't logged in for a long time. Archiving the contents would make sure people still have access to their old work, without leaving the burden of maintaining it forever on us."
  },
  {
    "objectID": "policy/storage-retention.html#why-google-drive",
    "href": "policy/storage-retention.html#why-google-drive",
    "title": "Storage Retention Policy",
    "section": "Why Google Drive?",
    "text": "Why Google Drive?\nWe can also perform access control easily with Google Drive."
  },
  {
    "objectID": "policy/storage-retention.html#alternatives",
    "href": "policy/storage-retention.html#alternatives",
    "title": "Storage Retention Policy",
    "section": "Alternatives",
    "text": "Alternatives\n\nEmail it to our users. This will most likely be rejected by most mail servers as the home directory will be too big an attachment\nPut it in Google Cloud Nearline storage, build a token based access control mechanism on top, and email this link to the users. We will need to probably clean this up every 18 months or so for cost reasons. This is the viable alternative, if we decide to not use Google Drive"
  },
  {
    "objectID": "policy/principles.html",
    "href": "policy/principles.html",
    "title": "Principles to be considered while creating new policies",
    "section": "",
    "text": "Kickstarting the iterative process of defining the core values for the product and the team to ensure that our decision-making is aligned to the values we commonly agree upon.\n\n\nWhenever we see a problem, we always think “How can we solve this problem for the next 1000/10000 users?” not just for the user we are solving the problem currently. Thus, we balance long-term orientation with a strong bias towards solving the user’s immediate needs.\n\n\n\nWe want to be mindful that the most users we serve are novice audiences who are not experts in programming/data science. It can be a challenging experience to learn the language, tools, and content together, and handling outages/product issues can be an added cognitive overload that can affect students’ morale. As a result, it is essential to focus on the user experience for the entire teaching team, and the students so that they are spending the time on what they actually want to do with Datahub. This means we will go out of our way to support the needs of the teaching team and the students to ensure that they have the best experience possible while having additional discomfort at our end in the short term. The product gained traction because of the exceptional user centricity which led to solving complex problems, reducing friction for users, and easing their user experience. Now that we have scaled to 10,000+ users, it is equally important to keep the ethos that took us from 0 to 10k users in mind and not operate with the mentality of a standardized service. We are an early-stage innovation with 10,000+ users and not a mature stage product that is poised for stability.\n\n\n\nWe want to offer an equitable experience to all students irrespective of their class size, compute requirements or affiliation to STEM-based courses etc. In addition, we want non STEM users to have the same or in some cases better user experience than their STEM counterparts. This means we may prioritize R-related use cases over Jupyter stuff as most of our audience in humanities and social science prefer using R for their data science workflow.\n\n\n\nWe want to be the best stewards of CDSS money. We value being thoughtful and frugal about spending the limited resources we have on the right things. However, when it comes to a trade-off between frugality and user-centricity, we always take the extra effort to think about how we can solve the user problems first.\n\n\n\nWe are not a big team, and we don’t have a lot of experts related to Marketing, Design, Data Science, etc. However, we want to be resourceful with the limited resources we have and engage with the open-source ecosystem, our partners across Berkeley, and the instructors/students to work towards our mission.",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Principles to be considered while creating new policies"
    ]
  },
  {
    "objectID": "policy/principles.html#balance-long-term-orientation-with-bias-towards-action-in-the-short-term",
    "href": "policy/principles.html#balance-long-term-orientation-with-bias-towards-action-in-the-short-term",
    "title": "Principles to be considered while creating new policies",
    "section": "",
    "text": "Whenever we see a problem, we always think “How can we solve this problem for the next 1000/10000 users?” not just for the user we are solving the problem currently. Thus, we balance long-term orientation with a strong bias towards solving the user’s immediate needs.",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Principles to be considered while creating new policies"
    ]
  },
  {
    "objectID": "policy/principles.html#user-centricity",
    "href": "policy/principles.html#user-centricity",
    "title": "Principles to be considered while creating new policies",
    "section": "",
    "text": "We want to be mindful that the most users we serve are novice audiences who are not experts in programming/data science. It can be a challenging experience to learn the language, tools, and content together, and handling outages/product issues can be an added cognitive overload that can affect students’ morale. As a result, it is essential to focus on the user experience for the entire teaching team, and the students so that they are spending the time on what they actually want to do with Datahub. This means we will go out of our way to support the needs of the teaching team and the students to ensure that they have the best experience possible while having additional discomfort at our end in the short term. The product gained traction because of the exceptional user centricity which led to solving complex problems, reducing friction for users, and easing their user experience. Now that we have scaled to 10,000+ users, it is equally important to keep the ethos that took us from 0 to 10k users in mind and not operate with the mentality of a standardized service. We are an early-stage innovation with 10,000+ users and not a mature stage product that is poised for stability.",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Principles to be considered while creating new policies"
    ]
  },
  {
    "objectID": "policy/principles.html#equity",
    "href": "policy/principles.html#equity",
    "title": "Principles to be considered while creating new policies",
    "section": "",
    "text": "We want to offer an equitable experience to all students irrespective of their class size, compute requirements or affiliation to STEM-based courses etc. In addition, we want non STEM users to have the same or in some cases better user experience than their STEM counterparts. This means we may prioritize R-related use cases over Jupyter stuff as most of our audience in humanities and social science prefer using R for their data science workflow.",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Principles to be considered while creating new policies"
    ]
  },
  {
    "objectID": "policy/principles.html#frugality",
    "href": "policy/principles.html#frugality",
    "title": "Principles to be considered while creating new policies",
    "section": "",
    "text": "We want to be the best stewards of CDSS money. We value being thoughtful and frugal about spending the limited resources we have on the right things. However, when it comes to a trade-off between frugality and user-centricity, we always take the extra effort to think about how we can solve the user problems first.",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Principles to be considered while creating new policies"
    ]
  },
  {
    "objectID": "policy/principles.html#resourcefulness",
    "href": "policy/principles.html#resourcefulness",
    "title": "Principles to be considered while creating new policies",
    "section": "",
    "text": "We are not a big team, and we don’t have a lot of experts related to Marketing, Design, Data Science, etc. However, we want to be resourceful with the limited resources we have and engage with the open-source ecosystem, our partners across Berkeley, and the instructors/students to work towards our mission.",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Principles to be considered while creating new policies"
    ]
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html",
    "href": "incidents/2019-02-25-k8s-api-server-down.html",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "",
    "text": "On February 25, 2019, the kubernetes API server for data100 became unreachable, causing new resource creation requests to fail. When the hub pod was stopped, a new one did not get created leading users to see a proxy error message. The hub came back online after a new cluster was created, storage was migrated to the new cluster, and then DNS was updated."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#summary",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#summary",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "",
    "text": "On February 25, 2019, the kubernetes API server for data100 became unreachable, causing new resource creation requests to fail. When the hub pod was stopped, a new one did not get created leading users to see a proxy error message. The hub came back online after a new cluster was created, storage was migrated to the new cluster, and then DNS was updated."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#timeline",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#timeline",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "Timeline",
    "text": "Timeline\n\n2019-02-25 11:21a\nThe kubernetes API server became unavailable. The time of this event was determine post mortem via the cloud provider’s monitoring metrics.\n\n\n11:34\nInfrastructure staff is notified in slack. It is determined that the hub proxy is up, but kubectl fails for all operations. The API server is unreachable.\n\n\n11:57\nA C ticket is created via the cloud provider’s portal. There are no other reports on the cloud provider’s status page. Infrastructure staff consider creating a new cluster and attaching storage to it.\n\n\n12:28p\nAn email is sent to contacts with the cloud provider asking for the ability to escalate the issue. C tickets have 8 hour response times.\n\n\n12:40\nIt is decided that rather than moving the nfs server from one cluster to another, the ZFS pool should be migrated to a new nfs server in the new cluster. The new cluster is requested.\n\n\n12:43 - 12:49\nCloud provider responds and calls infrastructure staff.\n\n\n13:00\nThe cluster is created and a new nfs server is requested in the cluster’s resource group.\n\n\n13:10\nData volumes are detached from the old server and moved from the old cluster’s resource group to the new one."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#section-6",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#section-6",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "13:20",
    "text": "13:20\nThe ZFS pool is imported into the new nfs server. helm is run to create the staging hub."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#section-7",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#section-7",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "13:34",
    "text": "13:34\nhelm completes and the staging hub is up. DNS is updated. helm is run to create the prod hub."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#section-8",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#section-8",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "13:41",
    "text": "13:41\nprod hub is up and DNS is updated.\n\n13:46\nCloud provider asks their upstream why the API server went down.\n\n\n14:48\nletsencrypt on prod can successfully retrieve an SSL certificate enabling students to connect."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#conclusion",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#conclusion",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "Conclusion",
    "text": "Conclusion\nThe managed kubernetes service went down for as yet unknown reasons. A new cluster was created and existing storage was attached to it."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#action-items",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#action-items",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "Action items",
    "text": "Action items\n\nMonitoring\n\nRemotely monitor the API server endpoint and send an alert when it is down."
  },
  {
    "objectID": "incidents/2019-02-25-k8s-api-server-down.html#update",
    "href": "incidents/2019-02-25-k8s-api-server-down.html#update",
    "title": "Azure Kubernetes API Server outage causes downtime",
    "section": "Update",
    "text": "Update\nCloud provider’s response on 3/15/2019:\n\nAfter reviewing all the logs we have, our backend advised below.\n\nWe’ve identified that there were problems with the infrastructure hosting your cluster which caused the kubelet on the master stopped responding. There were alerts regarding this issue which were addressed by our teams. We’re working to reduce the impact of these events as much as possible.\n\nPlease be advised this is not related with region stability\nFeel free to let me know if any further questions and thanks for your patience."
  },
  {
    "objectID": "incidents/2018-02-06-hub-db-dir.html",
    "href": "incidents/2018-02-06-hub-db-dir.html",
    "title": "Azure PD refuses to detach, causing downtime for data100",
    "section": "",
    "text": "On February 5, 2018, a PR was merged into the production cluster for Data 100. The CI got as far as running helm upgrade but the hub’s persistent volume would not detach from the old hub. The new hub pod had to wait on the hub-db-dir volume and so would not start. The persistent volume claim was ultimately deleted. The subsequent helm upgrade created a new volume and a new hub pod was able to start."
  },
  {
    "objectID": "incidents/2018-02-06-hub-db-dir.html#summary",
    "href": "incidents/2018-02-06-hub-db-dir.html#summary",
    "title": "Azure PD refuses to detach, causing downtime for data100",
    "section": "",
    "text": "On February 5, 2018, a PR was merged into the production cluster for Data 100. The CI got as far as running helm upgrade but the hub’s persistent volume would not detach from the old hub. The new hub pod had to wait on the hub-db-dir volume and so would not start. The persistent volume claim was ultimately deleted. The subsequent helm upgrade created a new volume and a new hub pod was able to start."
  },
  {
    "objectID": "incidents/2018-02-06-hub-db-dir.html#timeline",
    "href": "incidents/2018-02-06-hub-db-dir.html#timeline",
    "title": "Azure PD refuses to detach, causing downtime for data100",
    "section": "Timeline",
    "text": "Timeline\n\n2018-02-05 20:57\nA PR for data100 is merged.\n\n\n21:20\nTowards the end of the build, the upgrade fails because the new hub pod does not start up. The hub-db-dir volume remains bound to the old hub pod which is stuck in a Terminating state. The hub pod only completes termination when delete is passed a grace period of 0. The hub volume remains bound however.\n\n\n21:30\nCI is restarted but by the time helm is run, the hub-db-dir volume remains bound and cannot be attached to the new hub pod. Additionally, helm errors because the jupyterhub-internal ingress object cannot be found even though it does exist.\n\n\n21:45\nSince it cannot be determined what node the volume is bound to, the volume is deleted. The jupyterhub-internal ingress object is also deleted prior to restarting the CI build.\n\n\n22:05\nThe hub comes up with a new hub-db-dir volume. CI fails due to the same jupyterhub-internal object error."
  },
  {
    "objectID": "incidents/2018-02-06-hub-db-dir.html#conclusion",
    "href": "incidents/2018-02-06-hub-db-dir.html#conclusion",
    "title": "Azure PD refuses to detach, causing downtime for data100",
    "section": "Conclusion",
    "text": "Conclusion\nAzure was not able to detach the hub-db-dir azure disk from the hub pod. The pvc is deleted and the hub comes up on the next CI run."
  },
  {
    "objectID": "incidents/2018-02-06-hub-db-dir.html#action-items",
    "href": "incidents/2018-02-06-hub-db-dir.html#action-items",
    "title": "Azure PD refuses to detach, causing downtime for data100",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nStore the hub db in a cloud database to eliminate reliance on hub volume.\nDowngrade helm to 2.6.x to see if this fixes the helm upgrades."
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html",
    "href": "incidents/2024-core-node-incidents.html",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "",
    "text": "Over the past couple of years, all of our production hubs have been having persistent issues with our core nodes having major load spikes during ‘peak’ usage and the impacted node (which hosts all of our hub and proxy pods – not user pods) crashing. This would then impact every hub, causing all users to see 503 http errors until a new node finishing spinning up. We also suspect that the ‘white screen’ issue some users see after logging in is related to this.\nThese outages would usually last anywhere from 45 to 90+ minutes. The first chunk of time would be the core node getting wedged and eventually dying, and the last 15-20 minutes would be spent on the new node spinning up and services restarting.\nMany of these incidents are tracked here.\nWe have spent much time working to debug and track this, including with our friends at 2i2c. After much deep-diving and debugging, we were able to narrow this down to a memory (socket?) leak in the configurable http proxy.\nAfter some back and forth w/the upstream maintainers, we received a forked version of the proxy to test.\nDuring this testing, we triggered some user-facing downtime, as well as the proxy itself crashing and causing small outages.\nAnother (unrelated) issue that impacted users was that GKE was autoscaling our core pool (where the hub and proxy pods run) node to zero. Since it takes about 10-15m for a new node to spin up, all hubs were inaccessible until the new node was deployed.\nUser Impact:\n\n\nOn the afternoon of Feb 7th, I was testing the fork of the proxy + some revised timeouts on the Data 8 hub. This caused the proxy to crash every ~20m over the course of a few hours. I then reverted the fork and timeout changes.\nDuring the latter half of February, our core pool was being autoscaled from 1 to 0 nodes. This caused multiple short outages.\nThe proxy pods for Data 8 and Data 100 (our largest classes) crashed continually under load (~250+ simultaneous users), causing users to receive 500 HTTP errors until the pod automatically restarted."
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html#summary",
    "href": "incidents/2024-core-node-incidents.html#summary",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "",
    "text": "Over the past couple of years, all of our production hubs have been having persistent issues with our core nodes having major load spikes during ‘peak’ usage and the impacted node (which hosts all of our hub and proxy pods – not user pods) crashing. This would then impact every hub, causing all users to see 503 http errors until a new node finishing spinning up. We also suspect that the ‘white screen’ issue some users see after logging in is related to this.\nThese outages would usually last anywhere from 45 to 90+ minutes. The first chunk of time would be the core node getting wedged and eventually dying, and the last 15-20 minutes would be spent on the new node spinning up and services restarting.\nMany of these incidents are tracked here.\nWe have spent much time working to debug and track this, including with our friends at 2i2c. After much deep-diving and debugging, we were able to narrow this down to a memory (socket?) leak in the configurable http proxy.\nAfter some back and forth w/the upstream maintainers, we received a forked version of the proxy to test.\nDuring this testing, we triggered some user-facing downtime, as well as the proxy itself crashing and causing small outages.\nAnother (unrelated) issue that impacted users was that GKE was autoscaling our core pool (where the hub and proxy pods run) node to zero. Since it takes about 10-15m for a new node to spin up, all hubs were inaccessible until the new node was deployed.\nUser Impact:\n\n\nOn the afternoon of Feb 7th, I was testing the fork of the proxy + some revised timeouts on the Data 8 hub. This caused the proxy to crash every ~20m over the course of a few hours. I then reverted the fork and timeout changes.\nDuring the latter half of February, our core pool was being autoscaled from 1 to 0 nodes. This caused multiple short outages.\nThe proxy pods for Data 8 and Data 100 (our largest classes) crashed continually under load (~250+ simultaneous users), causing users to receive 500 HTTP errors until the pod automatically restarted."
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html#hub-information",
    "href": "incidents/2024-core-node-incidents.html#hub-information",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "Hub information",
    "text": "Hub information\n\nData 8\nData 100"
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html#timeline",
    "href": "incidents/2024-core-node-incidents.html#timeline",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "Timeline",
    "text": "Timeline\n\n2024-03-05 Data8 outage\nbetween ~4pm and ~5:15pm, data8’s configurable-http-proxy (chp) was oomkilled and caused many 503 errors to be issued to the users. here’s a rough timeline of what happened (culled from grafana, gcp logs and kernel logs on the core node):\n\n\n15:12:00\n~280 concurrent users\n\n\n15:12:42\nchp “uncaught exception: write EPIPE”\n\n\n16:00:00\nproxy ram 800Mi (steady)\n\n\n16:05:00\n~300 concurrent users\n\n\n~16:05:00\nspike on proxy — cpu 181%, mem 1.06Gi –&gt; 1.86Gi\n\n\n16:05:53\nchp healthz readiness probe failure\n\n\n16:05:56\nchp/javascript runs out of heap “Ineffective mark-compass near heap limit Allocation Failed”\n\n\n16:05:57-58\nchp restarts\n\n\n16:05:57\nnode dmesg: “TCP: request_sock_TCP: Possible SYN flooding on port 8000. Sending cookies. Check SNMP counters.”\n\n\n~16:06:00\n2.5K 503 errors\n\n\n~16:06:00 - 16:16:00\nmany many chp routes added and deleted causing 503 errors for some users\n\n\n16:16:00 - 16:49:00\neverything back to normal\n\n\n16:49:00\n~300 users (slowly decreasing)\n\n\n~16:49:00\nspike on proxy — cpu 107%, mem 814Mi –&gt; 2.45Gi\n\n\n16:49:40\ncore node dmesg: node invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=999\n\n\n16:50:25\nchp restarts (no heap error)\n\n\n~16:50:00\n5.7K 503 errors\n\n\n16:54:15 - 17:15:31\n300 users (slowly descreasing), 3x chp “uncaught exception: write EPIPE”, intermittent 503 errors in spikes of 30, 60, 150, hub latency 2.5sec\n\n\n18:47:19 - 18:58:10\n~120 users (constant), 3x chp “uncaught exception: write EPIPE”, intermittent 503 errors in spikes of 30, 60, hub latency 3sec\n\n\n~19:00\nthings return to normal"
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html#what-went-wrong",
    "href": "incidents/2024-core-node-incidents.html#what-went-wrong",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "What went wrong",
    "text": "What went wrong\n\nOnly being able to test things like the forked chp in prod is dangerous and has a huge potential impact on users.\nThe configurable http proxy (chp) is written in javascript (nodejs), and under load (~250+ concurrent users) it leaks ports and eventually fills up the heap (~728M) and gets oomkilled.\nThe CPU allocation on the core node was also spiking, potentially causing login latency across all hubs.\nUpon chp restart, it can take up to 10-15m for the routing table to be repopulated. During this time most, if not all, users that were already connected to the hub will get 503 errors. Any new logins during this time will not."
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html#where-we-got-lucky",
    "href": "incidents/2024-core-node-incidents.html#where-we-got-lucky",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "Where we got lucky",
    "text": "Where we got lucky\n\nSince we bumped the RAM allocated to the core node from 8G to 32G instances like are isolated to whatever hub’s chp begins failing, and does not impact all hubs."
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html#action-items",
    "href": "incidents/2024-core-node-incidents.html#action-items",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "Action items",
    "text": "Action items\n\nProcess/Policy improvements\n\nWork with instructors to identify and immediately troubleshoot directly with users impacted by the white screen issue.\nSpin up a dev hub and figure out how to use hubtraf to simulate a large number of users doing work.\n\n\n\nDocumentation improvements\nNone.\n\n\nTechnical improvements\n\nContinue to track the port leak issue upstream.\nDeploy a new core pool with the same RAM and more CPU (Jira DH-259).\nSpin up a dev hub and figure out how to use hubtraf to simulate a large number of users doing work."
  },
  {
    "objectID": "incidents/2024-core-node-incidents.html#actions",
    "href": "incidents/2024-core-node-incidents.html#actions",
    "title": "Core nodes being autoscaled, configurable HTTP proxy crashes",
    "section": "Actions",
    "text": "Actions\n\nIncident has been dealt with or is over\nSections above are filled out\nIncident title and after-action report is cleaned up\nAll actionable items above have linked GitHub Issues"
  },
  {
    "objectID": "incidents/2017-02-09-datahub-db-outage-pvc-recreate-script.html",
    "href": "incidents/2017-02-09-datahub-db-outage-pvc-recreate-script.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\n!head pvcs.csv\n\nNAME,STATUS,VOLUME,CAPACITY,ACCESSMODES,AGE\nclaim-redacted-205,Bound,pvc-5643aadf-ddb3-11e6-98ef-42010af000c3,10Gi,RWO,21d\nclaim-redacted-1088,Bound,pvc-bc68d752-ecba-11e6-98ef-42010af000c3,10Gi,RWO,2d\nclaim-redacted-389,Bound,pvc-5d67d43e-ddcb-11e6-98ef-42010af000c3,10Gi,RWO,21d\nclaim-redacted-119,Bound,pvc-73c5d644-ddb1-11e6-98ef-42010af000c3,10Gi,RWO,21d\nclaim-redacted-64,Bound,pvc-3c2345e4-dd9f-11e6-98ef-42010af000c3,10Gi,RWO,21d\nclaim-redacted-192,Bound,pvc-51b672a8-ddb3-11e6-98ef-42010af000c3,10Gi,RWO,21d\nclaim-redacted-814,Bound,pvc-2b83ae02-dee0-11e6-98ef-42010af000c3,10Gi,RWO,20d\nclaim-redacted-775,Bound,pvc-ec1d47be-deb0-11e6-98ef-42010af000c3,10Gi,RWO,20d\nclaim-redacted-954,Bound,pvc-5a0acdc5-e3ee-11e6-98ef-42010af000c3,10Gi,RWO,13d\n\n\n\ndf = pd.DataFrame.from_csv('pvcs.csv')\ndf.head()\n\n\n\n\n\n\n\nSTATUS\nVOLUME\nCAPACITY\nACCESSMODES\nAGE\n\n\nNAME\n\n\n\n\n\n\n\n\n\nclaim-redacted-205\nBound\npvc-5643aadf-ddb3-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\n\n\nclaim-redacted-1088\nBound\npvc-bc68d752-ecba-11e6-98ef-42010af000c3\n10Gi\nRWO\n2d\n\n\nclaim-redacted-389\nBound\npvc-5d67d43e-ddcb-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\n\n\nclaim-redacted-119\nBound\npvc-73c5d644-ddb1-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\n\n\nclaim-redacted-64\nBound\npvc-3c2345e4-dd9f-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\n\n\n\n\n\n\n\n\ndf['username'] = np.array(df.index.str.extract('\\w+-([\\w-]+)-\\d+$'))\ndf['id'] = np.array(df.index.str.extract('\\w+-\\w+-(\\d+)$'))\ndf.head()\n\n/Users/redacted/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n  if __name__ == '__main__':\n/Users/redacted/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n  from ipykernel import kernelapp as app\n\n\n\n\n\n\n\n\nSTATUS\nVOLUME\nCAPACITY\nACCESSMODES\nAGE\nusername\nid\n\n\n\nNAME\n\n\n\n\n\n\n\n\n\n\n\n\nclaim-redacted-205\nBound\npvc-5643aadf-ddb3-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n\n205\n\n\nclaim-redacted-1088\nBound\npvc-bc68d752-ecba-11e6-98ef-42010af000c3\n10Gi\nRWO\n2d\nredacted\n1088\n\n\n\nclaim-redacted-389\nBound\npvc-5d67d43e-ddcb-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n389\n\n\n\nclaim-redacted-119\nBound\npvc-73c5d644-ddb1-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n119\n\n\n\nclaim-redacted-64\nBound\npvc-3c2345e4-dd9f-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n64\n\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\nSTATUS\nVOLUME\nCAPACITY\nACCESSMODES\nAGE\nusername\nid\n\n\nNAME\n\n\n\n\n\n\n\n\n\n\n\nclaim-redacted-562\nBound\npvc-4418d1d6-de0a-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n562\n\n\nclaim-redacted-400\nBound\npvc-2399fe54-ddcf-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n400\n\n\nclaim-redacted-946\nBound\npvc-294cb2c7-e35e-11e6-98ef-42010af000c3\n10Gi\nRWO\n14d\nredacted\n946\n\n\nclaim-redacted-900\nBound\npvc-f6859607-e1c0-11e6-98ef-42010af000c3\n10Gi\nRWO\n16d\nredacted\n900\n\n\nhub-db-dir\nBound\npvc-ea8a0bef-dd44-11e6-98ef-42010af000c3\n10Gi\nRWO\n22d\nNaN\nNaN\n\n\n\n\n\n\n\n\nvalids = df[~df['AGE'].str.contains('m')].dropna()\nvalids = valids[valids['id'] != '14']\nvalids.head()\n\n\n\n\n\n\n\nSTATUS\nVOLUME\nCAPACITY\nACCESSMODES\nAGE\nusername\nid\n\n\nNAME\n\n\n\n\n\n\n\n\n\n\n\nclaim-redacted-205\nBound\npvc-5643aadf-ddb3-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n205\n\n\nclaim-redacted-1088\nBound\npvc-bc68d752-ecba-11e6-98ef-42010af000c3\n10Gi\nRWO\n2d\nredacted\n1088\n\n\nclaim-redacted-389\nBound\npvc-5d67d43e-ddcb-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n389\n\n\nclaim-redacted-119\nBound\npvc-73c5d644-ddb1-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n119\n\n\nclaim-redacted-64\nBound\npvc-3c2345e4-dd9f-11e6-98ef-42010af000c3\n10Gi\nRWO\n21d\nredacted\n64\n\n\n\n\n\n\n\n\nvalids['username'].head().value_counts()\n\nredacted              1\nredacted    1\nredacted              1\nredacted            1\nredacted         1\nName: username, dtype: int64\n\n\n\nvalids['id'].value_counts().head()\n\n400     1\n90      1\n756     1\n488     1\n1029    1\nName: id, dtype: int64\n\n\n\nlen(valids['id'])\n\n1105\n\n\n\nimport sqlite3\nconn = sqlite3.connect('jupyterhub.sqlite')\nc = conn.cursor()\n\n\nc.execute('PRAGMA TABLE_INFO({})'.format('users'))\n\nc.fetchall()\n\n[(0, 'id', 'INTEGER', 1, None, 1),\n (1, 'name', 'VARCHAR(1023)', 0, None, 0),\n (2, '_server_id', 'INTEGER', 0, None, 0),\n (3, 'admin', 'BOOLEAN', 0, None, 0),\n (4, 'last_activity', 'DATETIME', 0, None, 0),\n (5, 'cookie_id', 'VARCHAR(1023)', 0, None, 0),\n (6, 'state', 'TEXT', 0, None, 0),\n (7, 'auth_state', 'TEXT', 0, None, 0)]\n\n\n\nc.execute('SELECT * FROM users LIMIT 20')\ncurrent_users = c.fetchall()\ncurrent_users[:2]\n\n[(1,\n  'redacted',\n  None,\n  1,\n  '2017-02-09 09:07:03.936620',\n  '8ec09f8def774b668e838d1e70e00329',\n  None,\n  None),\n (2,\n  'redacted',\n  None,\n  1,\n  '2017-02-09 09:07:03.938610',\n  'c98f59afc1fe4a70b18b451ac1190d45',\n  None,\n  None)]\n\n\n\nadmins = {user[1] for user in current_users if user[3] == 1}\nadmins\n\n{'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted',\n 'redacted'}\n\n\n\n# I just did this manually in the sqlite3 CLI\n# c.execute('DROP FROM users where id &gt; -1')\n# c.fetchall()\n\n\n---------------------------------------------------------------------------\nOperationalError                          Traceback (most recent call last)\n&lt;ipython-input-73-a8eb9e4f2c5d&gt; in &lt;module&gt;()\n----&gt; 1 c.execute('DROP FROM users where id &gt; -1')\n      2 c.fetchall()\n\nOperationalError: near \"FROM\": syntax error\n\n\n\n\nimport datetime\nimport itertools\n\nrecords = list(zip(\n    valids['id'].astype(int),\n    valids['username'],\n    itertools.repeat('NULL'),\n    [1 if name in admins else 0 for name in valids['username']],\n#     itertools.repeat('NULL'), # Pick a random valid value\n    itertools.repeat('2017-02-09 09:07:03.936620'), # Pick a random valid value\n    itertools.repeat('thisisadummycookiehopefullyitworks'),\n    itertools.repeat('NULL'),\n    itertools.repeat('NULL')\n))\nrecords[:3]\n\n[(205,\n  'redacted',\n  'NULL',\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  'NULL',\n  'NULL'),\n (1088,\n  'redacted',\n  'NULL',\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  'NULL',\n  'NULL'),\n (389,\n  'redacted',\n  'NULL',\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  'NULL',\n  'NULL')]\n\n\n\n# I hate everything\ndef record_to_sql(record):\n    return \"INSERT INTO users VALUES({}, '{}', {}, '{}', '{}', '{}', {}, {})\".format(\n        *record\n    )\n\nsql_statements = [record_to_sql(record) for record in records]\nrecord_to_sql(records[0])\n\n\"INSERT INTO users VALUES(205, 'redacted', NULL, '0', '2017-02-09 09:07:03.936620', 'thisisadummycookiehopefullyitworks', NULL, NULL)\"\n\n\n[(0, 'id', 'INTEGER', 1, None, 1),\n (1, 'name', 'VARCHAR(1023)', 0, None, 0),\n (2, '_server_id', 'INTEGER', 0, None, 0),\n (3, 'admin', 'BOOLEAN', 0, None, 0),\n (4, 'last_activity', 'DATETIME', 0, None, 0),\n (5, 'cookie_id', 'VARCHAR(1023)', 0, None, 0),\n (6, 'state', 'TEXT', 0, None, 0),\n (7, 'auth_state', 'TEXT', 0, None, 0)]\n\nwith conn:\n    c = conn.cursor()\n    for statement in sql_statements:\n        c.execute(statement)\n\n\nc.execute('SELECT * FROM users LIMIT 20')\nc.fetchall()\n\n[(2,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (3,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (4,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (5,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (6,\n  'redacted',\n  None,\n  1,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (7,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (8,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (9,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (10,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (11,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (12,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (13,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (15,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (17,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (18,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (19,\n  'redacted',\n  None,\n  1,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (20,\n  'redacted',\n  None,\n  1,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (21,\n  'redacted',\n  None,\n  1,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (22,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None),\n (24,\n  'redacted',\n  None,\n  0,\n  '2017-02-09 09:07:03.936620',\n  'thisisadummycookiehopefullyitworks',\n  None,\n  None)]"
  },
  {
    "objectID": "incidents/2018-01-26-hub-slow-startup.html",
    "href": "incidents/2018-01-26-hub-slow-startup.html",
    "title": "Hub starts up very slow, causing outage for users",
    "section": "",
    "text": "On January 26, 2018, a new version of the helm chart was being installed on the production hub. Though the pod prepuller worked fine on the staging cluster, the prepuller never successfully finished on prod. This caused the CI to error because helm ran for too long. Additionally, the hub was taking a very long time to check user routes. After users were deleted in the hub’s orm and the hub was restarted, it came back up fairly quickly."
  },
  {
    "objectID": "incidents/2018-01-26-hub-slow-startup.html#summary",
    "href": "incidents/2018-01-26-hub-slow-startup.html#summary",
    "title": "Hub starts up very slow, causing outage for users",
    "section": "",
    "text": "On January 26, 2018, a new version of the helm chart was being installed on the production hub. Though the pod prepuller worked fine on the staging cluster, the prepuller never successfully finished on prod. This caused the CI to error because helm ran for too long. Additionally, the hub was taking a very long time to check user routes. After users were deleted in the hub’s orm and the hub was restarted, it came back up fairly quickly."
  },
  {
    "objectID": "incidents/2018-01-26-hub-slow-startup.html#timeline",
    "href": "incidents/2018-01-26-hub-slow-startup.html#timeline",
    "title": "Hub starts up very slow, causing outage for users",
    "section": "Timeline",
    "text": "Timeline\n\n2018-01-26 15:00\nThe helm chart for datahub was upgraded to a beta of v0.6 to make use of a new image puller. This was merged into the staging branch, successfully tested on the staging, and passed CI checks on prod. It was then merged to prod.\n\n\n15:15\nhelm times out because the prepuller never completes. It is determined that the master node on staging is cordoned while the master node on prod is not and has:\ntaints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n    timeAdded: null\n    value: \"true\"\nThe master is cordoned on prod and a new build is started in CI.\n\n\n15:33\nAfter CI times out again due to the prepuller, it is discovered that the master node has been uncordoned. Though the hub and proxy pods restart, the hub is taking a very long time to check user routes. It is slower than the most recent hub restart which was itself slow enough to warrant a new issue on jupyterhub, https://github.com/jupyterhub/jupyterhub/issues/1633.\n\n\n13:40\nIt is decided that the most expedient way to get the hub up is to delete users from the orm.\n\n\n13:50\nThe following command is run after the database is backed up:\ndelete from users where users.id in (select users.id from users join spawners on spawners.user_id = users.id where server_id is null);\ndeleting 4902 records. The hub pod is deleted and the hub comes up shortly after."
  },
  {
    "objectID": "incidents/2018-01-26-hub-slow-startup.html#conclusion",
    "href": "incidents/2018-01-26-hub-slow-startup.html#conclusion",
    "title": "Hub starts up very slow, causing outage for users",
    "section": "Conclusion",
    "text": "Conclusion\nAt 5000, the hub takes long enough to restart to inconvenience the number of active users at any one time."
  },
  {
    "objectID": "incidents/2018-01-26-hub-slow-startup.html#action-items",
    "href": "incidents/2018-01-26-hub-slow-startup.html#action-items",
    "title": "Hub starts up very slow, causing outage for users",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nThe prepuller should be fixed so that helm does not time out.\nThe hub route checking should be parallelized so that startup is not slow.\nThe staging hub should be seeded with users so that scaling issues can be exposed prior to reaching production."
  },
  {
    "objectID": "incidents/2017-02-24-proxy-death-incident.html",
    "href": "incidents/2017-02-24-proxy-death-incident.html",
    "title": "Proxy eviction strands user",
    "section": "",
    "text": "On the evening of Feb 23, several students started experiencing 500 errors in trying to access datahub. The proxy had died because of a known issue, and it took a while for the hub to re-add all the user routes to the proxy. Some students’ needed their servers to be manually restarted, due to a JupyterHub spawner bug that is showing up at scale. Everything was fixed in about 40 minutes."
  },
  {
    "objectID": "incidents/2017-02-24-proxy-death-incident.html#summary",
    "href": "incidents/2017-02-24-proxy-death-incident.html#summary",
    "title": "Proxy eviction strands user",
    "section": "",
    "text": "On the evening of Feb 23, several students started experiencing 500 errors in trying to access datahub. The proxy had died because of a known issue, and it took a while for the hub to re-add all the user routes to the proxy. Some students’ needed their servers to be manually restarted, due to a JupyterHub spawner bug that is showing up at scale. Everything was fixed in about 40 minutes."
  },
  {
    "objectID": "incidents/2017-02-24-proxy-death-incident.html#timeline",
    "href": "incidents/2017-02-24-proxy-death-incident.html#timeline",
    "title": "Proxy eviction strands user",
    "section": "Timeline",
    "text": "Timeline\nAll times in PST\n\n21:10:57\nThe proxy pod is evicted, due to a known issue that is currently being worked on. Users start running into issue now, with connection failures.\n\n\n21:11:04\nNew proxy pod is started by kubernetes, and starts accepting connections. However, the JupyterHub model currently has the proxy starting with no state about user routes, and so the users’ requests aren’t being routed to their notebook pods. This manifests as errors for users.\nThe hub process is supposed to poll the proxy every 300s, and repopulate the route table when it notices it is empty. The hub does this at some point in the next 300s (we do not know when), and starts repopulating the route table. As routes get added for currently users, their notebook starts working again.\n\n\n21:11:52\nThe repopulate process starts running into issues - it is making far too many http requests (to the kubernetes and proxy APIs) that it starts running into client side limits on tornado http client (which is what we use to make these requests). This causes them to time out on the request queue. We were running into https://github.com/tornadoweb/tornado/issues/1400. Not all requests fail - for those that succeed, the students are able to access their notebooks.\nThe repopulate process takes a while to process, and errors for a lot of students who are left with notebook in inconsistent state - JupyterHub thinks their notebook is running but it isn’t, or vice versa. Lots of 500s for users.\n\n\n21:14\nReports of errors start reaching the Slack channel + Piazza.\nThe repopulate process keeps being retried, and notebooks for users slowly come back. Some users are ‘stuck’ in a bad state, however - their notebook isn’t running, but JupyterHub thinks it is (or vice versa).\n\n\n21:34\nMost users are fine by now. For those still with problems, a forced delete from the admin interface + a start works, since this forces JupyterHub to really check if they’re there or not.\n\n\n22:03\nLast reported user with 500 error is fixed, and datahub is fully operational again."
  },
  {
    "objectID": "incidents/2017-02-24-proxy-death-incident.html#conclusion",
    "href": "incidents/2017-02-24-proxy-death-incident.html#conclusion",
    "title": "Proxy eviction strands user",
    "section": "Conclusion",
    "text": "Conclusion\nThis is almost a ‘perfect storm’ event. Three things colluded to make this outage happen:\n\nThe inodes issue, which causes containers to fail randomly\nThe fact that the proxy is a single point of failure with a longish recovery time in current JupyterHub architecture.\nKubeSpawner’s current design is inefficient at very high user volumes, and its request timeouts & other performance characteristics had not been tuned (because we have not needed to before).\n\nWe have both long term (~1-2 months) architectural fixes as well as short term tuning in place for all three of these issues."
  },
  {
    "objectID": "incidents/2017-02-24-proxy-death-incident.html#action-items",
    "href": "incidents/2017-02-24-proxy-death-incident.html#action-items",
    "title": "Proxy eviction strands user",
    "section": "Action items",
    "text": "Action items\n\nUpstream JupyterHub\n\nWork on abstracting the proxy interface, so the proxy is no longer a single point of failure. Issue\n\n\n\nUpstream KubeSpawner\n\nRe-architect the spawner to make a much smaller number of HTTP requests. DataHub has become big enough that this is a problem. Issue\nTune the HTTP client kubespawner uses. This would be an interim solution until (1) gets fixed. Issue\n\n\n\nDataHub configuration\n\nSet resource requests explicitly for hub and proxy, so they have less chance of getting evicted. Issue\nReduce the interval at which the hub checks to see if the proxy is running. PR\nSpeed up the fix for the inodes issue which is what triggered this whole issue."
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html",
    "title": "Hubs throwing 505 errors",
    "section": "",
    "text": "PR 1 and PR 2 were merged to prod between 2 AM and 2.30 AM PST on 1/20. Difference due to the commits can be viewed here\nDue to these changes, image rebuild happened which broke multiple hubs which used that image including Datahub, ISchool, R, Data 100 and Data 140 hubs.\nOne of the dependenices highlighted as part of the image build had an upgrade which resulted in R hub throwing 505 error and Data 100/140 hub throwing “Error starting Kernel”. [Yuvi to fill in the right technical information]\nUser Impact:\n\n\nR Hub was not accessible for about 6 hours. Issue affected 10+ Stat 20 GSIs planning for their first class of the semester (catering to the needs of 600+ students). Hub went down for few minutes during the instruction.\nProb 140 hub was not available till 12.15 AM PST\nData 100 hub was not available till 12.33 AM. Thankfully, assignments were not due till friday (1/21)\nFew users in Ischool were affected as they could not access R Studio"
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html#summary",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html#summary",
    "title": "Hubs throwing 505 errors",
    "section": "",
    "text": "PR 1 and PR 2 were merged to prod between 2 AM and 2.30 AM PST on 1/20. Difference due to the commits can be viewed here\nDue to these changes, image rebuild happened which broke multiple hubs which used that image including Datahub, ISchool, R, Data 100 and Data 140 hubs.\nOne of the dependenices highlighted as part of the image build had an upgrade which resulted in R hub throwing 505 error and Data 100/140 hub throwing “Error starting Kernel”. [Yuvi to fill in the right technical information]\nUser Impact:\n\n\nR Hub was not accessible for about 6 hours. Issue affected 10+ Stat 20 GSIs planning for their first class of the semester (catering to the needs of 600+ students). Hub went down for few minutes during the instruction.\nProb 140 hub was not available till 12.15 AM PST\nData 100 hub was not available till 12.33 AM. Thankfully, assignments were not due till friday (1/21)\nFew users in Ischool were affected as they could not access R Studio"
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html#hub-information",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html#hub-information",
    "title": "Hubs throwing 505 errors",
    "section": "Hub information",
    "text": "Hub information\n\nHub URL: https://r.datahub.berkeley.edu/ and most other hubs highlighted above"
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html#timeline-if-relevant",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html#timeline-if-relevant",
    "title": "Hubs throwing 505 errors",
    "section": "Timeline (if relevant)",
    "text": "Timeline (if relevant)\n\n{{ 2022-01-20 Between 02:00 and 02.30 PM }}\nPR 1 and PR 2 were merged to prod. Notably, PR 1 had multiple commits related to creation of Stat 20 hub, Stat 259 hub etc..\n\n\n{{ 06:10 }}\nAndrew Bray (Stat 20 instructor) raised a github issue around 5.45 AM PST.\n\n\n{{ 07:45 }}\nYuvi quickly jumped in to make a fix to get the R hub working. However this fix resulted in breaking Stat 20 hub.\n\n\n{{ 07:53 }}\nISchool folks reported issues with using RStudio in Datahub\n\n\n{{ 08:45 }}\nYuvi fixed issue with Stat 20 and other hubs\n\n\n{{ 12:10 }}\nGSIs from Data 100 and 140 reported “Unhandled error” in their hubs\n\n\n{{ 12:15 }}\nGSIs for Data 140 hub reported that the error was fixed\n\n\n{{ 12:33 }}\nGSIs Data 100 hub reported that the error was fixed"
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html#what-went-wrong",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html#what-went-wrong",
    "title": "Hubs throwing 505 errors",
    "section": "What went wrong",
    "text": "What went wrong\n\nR, Stat 20, Datahub, ISchool, Data 100 and 140 hubs went down around 2.30 AM PST. However, the team was aware of these issues only when users reported errors at different time intervals (as listed above)\nMultiple commits went through a single PR. Dependency package’s version upgrade broke the image build (Yuvi to fill in the required details)\n\nThings that could have gone better. Ideally these should result in concrete action items that have GitHub issues created for them and linked to under Action items."
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html#where-we-got-lucky",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html#where-we-got-lucky",
    "title": "Hubs throwing 505 errors",
    "section": "Where we got lucky",
    "text": "Where we got lucky\nThese are good things that happened to us but not because we had planned for them.\n\nYuvi was awake at the time when issue was reported and was able to fix the issues immediately.\nClasses using hubs were not completely affected due to this outage (Data 100 did not have assignments due till 1/21 and Stat 20 had few mins of outage during instruction)"
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html#action-items",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html#action-items",
    "title": "Hubs throwing 505 errors",
    "section": "Action items",
    "text": "Action items\nThese are only sample subheadings. Every action item should have a GitHub issue (even a small skeleton of one) attached to it, so these do not get forgotten. These issues don’t have to be in infrastructure/, they can be in other repositories.\n\nProcess/Policy improvements\n\n{{Develop manual testing process whenever a PR gets merged to staging of the major hubs (till automated test suites are written)}} link to github issue]\nDevelop a policy around when to create a new hub and what type of changes get deployed to Datahub!\n\n\n\nDocumentation improvements\n\n{{ Start writing after action reports for future outages }} [link to github issue]\n{{ summary }} [link to github issue]\n\n\n\nTechnical improvements\n\n{{ Enabling logging mechanism across all hubs to track future outages }}\n{{ Adapt 2I2C testing suite to develop automated test cases that check the sanity of the different services whenever a PR gets merged in staging}} [link to github issue]\n{{ Investigate the reason why pager duty did not throw an alert for 5xx errors when the hubs went down. Fix the alerting mechanism so that they notify all kind of errors }} [link to github issue]\n{{ Adding R Studio as part of Repo2Docker}} [link to github issue]"
  },
  {
    "objectID": "incidents/2022-01-20-package-dependency-upgrade-incident.html#actions",
    "href": "incidents/2022-01-20-package-dependency-upgrade-incident.html#actions",
    "title": "Hubs throwing 505 errors",
    "section": "Actions",
    "text": "Actions\n\nIncident has been dealt with or is over\nSections above are filled out\nIncident title and after-action report is cleaned up\nAll actionable items above have linked GitHub Issues"
  },
  {
    "objectID": "incidents/2017-10-19-course-subscription-canceled.html",
    "href": "incidents/2017-10-19-course-subscription-canceled.html",
    "title": "Billing confusion with Azure portal causes summer hub to be lost",
    "section": "",
    "text": "On October 10, 2017, the cloud vendor notified ds-instr that the data8r-17s subscription was canceled due to its end date, and we had 90 days to reactivate it using the educator portal. A support ticket was created to reverse the cancellation since the educator portal did not permit reactivation. On October 18 we were notified that the subscription’s resources were deleted.\nCoincidentally, a script was written on Oct. 9 to backup data to ds-instr’s Google Drive and it was performed for the instructor as a test. Unfortunately it wasn’t run for all users before the resources were taken offline."
  },
  {
    "objectID": "incidents/2017-10-19-course-subscription-canceled.html#summary",
    "href": "incidents/2017-10-19-course-subscription-canceled.html#summary",
    "title": "Billing confusion with Azure portal causes summer hub to be lost",
    "section": "",
    "text": "On October 10, 2017, the cloud vendor notified ds-instr that the data8r-17s subscription was canceled due to its end date, and we had 90 days to reactivate it using the educator portal. A support ticket was created to reverse the cancellation since the educator portal did not permit reactivation. On October 18 we were notified that the subscription’s resources were deleted.\nCoincidentally, a script was written on Oct. 9 to backup data to ds-instr’s Google Drive and it was performed for the instructor as a test. Unfortunately it wasn’t run for all users before the resources were taken offline."
  },
  {
    "objectID": "incidents/2017-10-19-course-subscription-canceled.html#timeline",
    "href": "incidents/2017-10-19-course-subscription-canceled.html#timeline",
    "title": "Billing confusion with Azure portal causes summer hub to be lost",
    "section": "Timeline",
    "text": "Timeline\n\n2017-10-10 9:06a\nds-instr received an email from the cloud vendor:\n\nThe following subscriptions under your [cloud vendor] sponsorships for ds-instr@berkeley.edu have recently become canceled. Because these subscription(s) are canceled, all services have been suspended but no data has been lost. You have 90 days from the date of cancellation before [the cloud vendor] will delete the subscription and all attached data. Please use the Educator Portal to reactivate the subscription(s).\n\n\n\n\nSubscription Name\nSubscription Id\nCanceled Reason\n\n\n\n\ndata8r-17s\nomitted here\nSubscription End Date\n\n\n\n\n\n9:30\nThe instructor was notified. The educator portal did not provide a way to view or alter the subscription end date of a canceled subscription so a support request was filed at the cloud vendor.\n\n\n11:14\nThe cloud vendor asks that a payment instrument be added to the ds-instr account. We respond that the account is funded by a sponsorship.\n\n\n17:22\nThe cloud vendor contacts their sponsorship team.\n\n\n2017-10-11 15:00\nThe cloud vendor calls to discuss the situation. Screenshots of the educator and cloud portal were sent to the cloud vendor.\n\n\n2017-10-12 16:19\nThe cloud vendor offers to enable the subscription for a 60 minute period from the backend so that the End Date may be extended from the portal. Though the subscription is re-enabled for an hour, the portal still does not permit the subscription parameters to be changed.\n\n\n2017-10-18 15:29\nThe cloud vendor says that the subscription was actually disabled because it had exhausted the allocated funds, and the data was deleted within 24 hours despite the stated 90 day grace period. Later the following was provided by the cloud vendor:\n\nI worked with our backend engineering team last night and I am afraid to say that we could not retrieve the storage account after all our sincere efforts. I understand how frustrating it would be for you and I do not have the words to express the same, I just wish if I could be of some help to you.\nHaving said that we did dig into the reasons behind this situation, the subscription was initially suspended by an internal engineering job occurred that auto-suspended all Academic Account Sponsorship subscriptions with an end date that was part of the previous fiscal year. Usually this suspension does not delete the subscription. There are a few [cloud vendor] accounts which are on legacy commerce platform which are affected and these accounts are in the process of modern platform. Your account was in the transition mode when the subscription got suspended and your account was partially converted to the modern platform. The billing & subscription part was converted to the modern platform but the not at the service level. Hence you got the message that your data would be retained for 90 days, at the same stated at the service level it was not converted to the modern hence the data got deleted.\nI had a detailed discussion with our product group team on this and how we can avoid this in future. First of all, your account is now completely migrated/transitioned completely to the modern platform. Also, to ensure that our other Academic Account Sponsorships customers do not face the same issue they have agreed to complete the migration manually on those accounts.\n\n\n\n2017-10-19 10:53\nThe cloud vendor compensates ds-instr with an additional $10k for the experience."
  },
  {
    "objectID": "incidents/2017-10-19-course-subscription-canceled.html#conclusion",
    "href": "incidents/2017-10-19-course-subscription-canceled.html#conclusion",
    "title": "Billing confusion with Azure portal causes summer hub to be lost",
    "section": "Conclusion",
    "text": "Conclusion\nThere were insufficient funds on the subscription to persist its resources. The resources were deleted by the cloud vendor before the grace period ran out."
  },
  {
    "objectID": "incidents/2017-10-19-course-subscription-canceled.html#action-items",
    "href": "incidents/2017-10-19-course-subscription-canceled.html#action-items",
    "title": "Billing confusion with Azure portal causes summer hub to be lost",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nUntil there is a per-user backup implemented hub-side, set a schedule for backing up user data for every course.\nAlways set a billing alert at some conservative amount less than the subscription alotment\nIf a subscription is ever canceled, backup user data within 24 hours, regardless of the stated grace period."
  },
  {
    "objectID": "incidents/2018-06-11-course-subscription-canceled.html",
    "href": "incidents/2018-06-11-course-subscription-canceled.html",
    "title": "Azure billing issue causes downtime",
    "section": "",
    "text": "On June 11, 2018, the cloud vendor notified ds-instr that the data8-17f-prod subscription was canceled due to its usage cap. The educator portal confirmed that the spend had surpassed the budget. After additional funds were allocated to the subscription, a portion of the VMs were manually started. The hub came back online after pods were forcibly deleted and nodes were cordoned."
  },
  {
    "objectID": "incidents/2018-06-11-course-subscription-canceled.html#summary",
    "href": "incidents/2018-06-11-course-subscription-canceled.html#summary",
    "title": "Azure billing issue causes downtime",
    "section": "",
    "text": "On June 11, 2018, the cloud vendor notified ds-instr that the data8-17f-prod subscription was canceled due to its usage cap. The educator portal confirmed that the spend had surpassed the budget. After additional funds were allocated to the subscription, a portion of the VMs were manually started. The hub came back online after pods were forcibly deleted and nodes were cordoned."
  },
  {
    "objectID": "incidents/2018-06-11-course-subscription-canceled.html#timeline",
    "href": "incidents/2018-06-11-course-subscription-canceled.html#timeline",
    "title": "Azure billing issue causes downtime",
    "section": "Timeline",
    "text": "Timeline\n\n2018-06-11 9:02a\nds-instr received an email from the cloud vendor:\n\nThe following subscriptions under your Microsoft Azure sponsorships for ds-instr@berkeley.edu have recently become canceled. Because these subscription(s) are canceled, all services have been suspended but no data has been lost. You have 90 days from the date of cancellation before Microsoft will delete the subscription and all attached data. Please use the Educator Portal to reactivate the subscription(s).\n\n\n\n\nSubscription Name\nSubscription Id\nCanceled Reason\n\n\n\n\ndata8-17f-prod\nomitted here\nSubscription Cap\n\n\n\n\n\n9:29\nThe subscription status was confirmed at https://www.microsoftazuresponsorships.com/Manage. In order to allocate additional budget to data8-17f-prod, budget for other subscriptions had to be reduced.\n\n\n9:40\nVMs were turned on at https://portal.azure.com: 3 nodes in each node pool, the nfs server, the kubernetes master, and the database server.\n\n\n9:45\nThe hub was unreachable even though the VMs were online. The hub and proxy pods were shown as Running and all nodes were shown as online even though some nodes had not been started. The offline cluster nodes were manually cordoned. All pods had to be forcibly deleted before they would start.\n\n\n10:14\nThe Billing Alert Service was checked at https://account.azure.com/Subscriptions/alert?subscriptionId=06f94ac5-b029-411f-8896-411f3c6778b4 and it was discovered that alerts were no longer registered."
  },
  {
    "objectID": "incidents/2018-06-11-course-subscription-canceled.html#conclusion",
    "href": "incidents/2018-06-11-course-subscription-canceled.html#conclusion",
    "title": "Azure billing issue causes downtime",
    "section": "Conclusion",
    "text": "Conclusion\nThere were insufficient funds on the subscription to persist its resources. The subscription budget was increased and the hub was brought back online. The billing alert service that was configured to prevent such incidents did not function properly."
  },
  {
    "objectID": "incidents/2018-06-11-course-subscription-canceled.html#action-items",
    "href": "incidents/2018-06-11-course-subscription-canceled.html#action-items",
    "title": "Azure billing issue causes downtime",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nDo not use subscription portal billing alerts.\nManually check subscription usage via an unattended process."
  },
  {
    "objectID": "incidents/2018-01-25-helm-chart-upgrade.html",
    "href": "incidents/2018-01-25-helm-chart-upgrade.html",
    "title": "Accidental merge to prod brings things down",
    "section": "",
    "text": "On January 25, 2018, a new version of the helm chart was installed on the staging hub. It was not immediately merged to production because there were active labs throughout the day. While preparing another course’s hub via Travis CI, the Data 8 change was accidentally merged from staging to production. This production hub went down because the new helm chart’s jupyterhub image was broken."
  },
  {
    "objectID": "incidents/2018-01-25-helm-chart-upgrade.html#summary",
    "href": "incidents/2018-01-25-helm-chart-upgrade.html#summary",
    "title": "Accidental merge to prod brings things down",
    "section": "",
    "text": "On January 25, 2018, a new version of the helm chart was installed on the staging hub. It was not immediately merged to production because there were active labs throughout the day. While preparing another course’s hub via Travis CI, the Data 8 change was accidentally merged from staging to production. This production hub went down because the new helm chart’s jupyterhub image was broken."
  },
  {
    "objectID": "incidents/2018-01-25-helm-chart-upgrade.html#timeline",
    "href": "incidents/2018-01-25-helm-chart-upgrade.html#timeline",
    "title": "Accidental merge to prod brings things down",
    "section": "Timeline",
    "text": "Timeline\n\n2018-01-25 14:30\nThe helm chart for datahub was upgraded to a beta of v0.6 to make use of a new image puller. This was merged into the staging branch. After some initial debugging, the helm chart was installed successfully and the image puller worked correctly. However, the staging hub was not tested.\nSince labs were scheduled throughout the day until 7p, it was decided to delay the upgrade of the production hub until after 7p.\n\n\n15:30\nWhile a different hub was being managed in Travis CI, the production hub for Data 8 was accidentally upgraded. This upgrade brought with it the faulty hub image from staging which wasn’t working.\n\n\n16:11\nGSIs report in slack that the hub is down for lab users. It is confirmed that the hub process has crashed due to a shared C library included from a python library. It is decided that the quickest way to bring the hub back up is to downgrade the helm-chart back to v0.5.0.\n\n\n16:35\nThe chart is installed into the staging repo, merged to staging, and checked on the staging hub. It is then merged into production and brought online there."
  },
  {
    "objectID": "incidents/2018-01-25-helm-chart-upgrade.html#conclusion",
    "href": "incidents/2018-01-25-helm-chart-upgrade.html#conclusion",
    "title": "Accidental merge to prod brings things down",
    "section": "Conclusion",
    "text": "Conclusion\nA relatively large change was made to the hub configuration with insufficient testing on the staging server. This was compounded when the change was accidentally merged to production."
  },
  {
    "objectID": "incidents/2018-01-25-helm-chart-upgrade.html#action-items",
    "href": "incidents/2018-01-25-helm-chart-upgrade.html#action-items",
    "title": "Accidental merge to prod brings things down",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nAdmins should refamiliarize themselves with the deployment policy to check the staging hub before changes are merged to production.\nDetermine if there is a way to block merges to production if the staging hub is not online.\nDetermine if there is a way to contextualize the Travis CI interface so that it is obvious which deployment is being managed."
  },
  {
    "objectID": "incidents/2017-02-24-autoscaler-incident.html",
    "href": "incidents/2017-02-24-autoscaler-incident.html",
    "title": "Custom Autoscaler gonee haywire",
    "section": "",
    "text": "On the evening of February 24, 2017, a premature version of the Autoscaler script for the Datahub deployment was mistakenly run on the prod cluster, resulting in a large amount of nodes (roughly 30-40) being set as unschedulable for about 20 minutes. Though no information was lost nor service critically disturbed, it was necessary to manually re-enable these nodes to be scheduled."
  },
  {
    "objectID": "incidents/2017-02-24-autoscaler-incident.html#summary",
    "href": "incidents/2017-02-24-autoscaler-incident.html#summary",
    "title": "Custom Autoscaler gonee haywire",
    "section": "",
    "text": "On the evening of February 24, 2017, a premature version of the Autoscaler script for the Datahub deployment was mistakenly run on the prod cluster, resulting in a large amount of nodes (roughly 30-40) being set as unschedulable for about 20 minutes. Though no information was lost nor service critically disturbed, it was necessary to manually re-enable these nodes to be scheduled."
  },
  {
    "objectID": "incidents/2017-02-24-autoscaler-incident.html#timeline",
    "href": "incidents/2017-02-24-autoscaler-incident.html#timeline",
    "title": "Custom Autoscaler gonee haywire",
    "section": "Timeline",
    "text": "Timeline\nAs of this commit in the Autoscaler branch history, there exists a scale.py file that would based on the utilization of the cluster, mark a certain number of nodes unschedulable before attempting to shut down nodes with no pods in them. Unfortunately, this script was executed prematurely, and without configuration, looked to execute in whatever context currently specified in .kube/config, which ended up being the production cluster rather than the dev cluster.\n\n2017-02-24 11:14 PM\nScript is mistakenly executed. A bug in the calculations for the utilization of the cluster leads to about 40 nodes being marked as unschedulable. The mistake is noted immediately.\n\n\n2017-02-24 11:26 PM\nThe unschedulability of these nodes is reverted. All nodes in the cluster were first all set to be schedulable to ensure that no students current and future would be disturbed. Immediately after, 10 of the most idle nodes on the cluster were manually set to be unschedulable (to facilitate them later being manually descaled - to deal with https://github.com/data-8/infrastructure/issues/6) using kubectl cordon &lt;node_name&gt;."
  },
  {
    "objectID": "incidents/2017-02-24-autoscaler-incident.html#conclusion",
    "href": "incidents/2017-02-24-autoscaler-incident.html#conclusion",
    "title": "Custom Autoscaler gonee haywire",
    "section": "Conclusion",
    "text": "Conclusion\nA cluster autoscaler script was accidentally run against the production cluster instead of the dev cluster, reducing capacity for new user logins for about 12 minutes. There was still enough capacity so we had no adverse effects."
  },
  {
    "objectID": "incidents/2017-02-24-autoscaler-incident.html#action-items",
    "href": "incidents/2017-02-24-autoscaler-incident.html#action-items",
    "title": "Custom Autoscaler gonee haywire",
    "section": "Action Items",
    "text": "Action Items\n\nDatahub Deployment Changes\n\nThe Autoscaler should not be run unless the context is explicitly set via environment variables or command line arguments. This is noted in the comments of the pull request for the Autoscaler.\nThe idea of the ‘current context’ should be abolished in all the tools we build / read.\n\n\n\nFuture organizational change\n\nUse a separate billing account for production vs development clusters. This makes it harder to accidentally run things on the wrong cluster"
  },
  {
    "objectID": "users/private-repo.html",
    "href": "users/private-repo.html",
    "title": "Accessing private GitHub repos",
    "section": "",
    "text": "GitHub is used to store class materials (lab notebooks, lecture notebooks, etc), and nbgitpuller is used to distribute it to students. By default, nbgitpuller only supports public GitHub repositories. However, Berkeley’s JupyterHubs are set up to allow pulling from private repositories as well.\nPublic repositories are still preferred, but if you want to distribute a private repository to your students, you can do so.\n\nGo to the GitHub app for the hub you are interested in.\n\nR Hub\nDataHub\nPublicHealth Hub\nBiology Hub\nEECS Hub\nOpen an issue if you want more hubs supported.\n\nClick the ‘Install’ button.\nSelect the organization / user containing the private repository you want to distribute on the JupyterHub. If you are not the owner or administrator of this organization, you might need extra permissions to do this action.\nSelect ‘Only select repositories’, and below that select the private repositories you want to distribute to this JupyterHub.\nClick the ‘Install’ button. The JupyterHub you picked now has access to this private repository. You can revoke this anytime by coming back to this page, and removing the repo from the list of allowed repos. You can also totally uninstall the GitHub app.\nYou can now make a link for your repo at nbgitpuller.link. If you had just created your repo, you might have to specify main instead of master for the branch name, since GitHub changed the name of the default branch recently.\n\nThat’s it! You’re all set. You can distribute these links to your students, and they’ll be able to access your materials! You can also use more traditional methods (like the git commandline tool, or RStudio’s git interface) to access this repo as well.\nNote: Everyone on the selected JupyterHub can clone your private repo if you do this. They won’t be able to see that this repo exists, but if they get their hands on your nbgitpuller link they can fetch that too. More fine-grained permissions coming soon.",
    "crumbs": [
      "Using DataHub",
      "Accessing private GitHub repos"
    ]
  },
  {
    "objectID": "users/hubs.html",
    "href": "users/hubs.html",
    "title": "JupyterHubs in this repository",
    "section": "",
    "text": "datahub.berkeley.edu is the 'main' JupyterHub for use on UC Berkeley campus. It's the largest and most active hub. It has many Python & R packages installed.\nIt runs on Google Cloud Platform in the ucb-datahub-2018 project. You can see all config for it under deployments/datahub.\n\n\n\nThe big data8 class.\nActive connector courses\nData Science Modules\nAstro 128/256\n\nThis hub is also the 'default' when folks wanna use a hub for a short period of time for any reason without super specific requirements.",
    "crumbs": [
      "Using DataHub",
      "JupyterHubs in this repository"
    ]
  },
  {
    "objectID": "users/hubs.html#datahub",
    "href": "users/hubs.html#datahub",
    "title": "JupyterHubs in this repository",
    "section": "",
    "text": "datahub.berkeley.edu is the 'main' JupyterHub for use on UC Berkeley campus. It's the largest and most active hub. It has many Python & R packages installed.\nIt runs on Google Cloud Platform in the ucb-datahub-2018 project. You can see all config for it under deployments/datahub.\n\n\n\nThe big data8 class.\nActive connector courses\nData Science Modules\nAstro 128/256\n\nThis hub is also the 'default' when folks wanna use a hub for a short period of time for any reason without super specific requirements.",
    "crumbs": [
      "Using DataHub",
      "JupyterHubs in this repository"
    ]
  },
  {
    "objectID": "users/hubs.html#prob140-hub",
    "href": "users/hubs.html#prob140-hub",
    "title": "JupyterHubs in this repository",
    "section": "Prob140 Hub",
    "text": "Prob140 Hub\nA hub specifically for prob140. Some of the admin users on hubs/datahub are students in prob140 - this would allow them to see the work of other prob140 students. Hence, this hub is separate until JupyterHub gains features around restricting admin use.\nIt runs on Google Cloud Platform in the ucb-datahub-2018 project. You can see all config for it under deployments/prob140.",
    "crumbs": [
      "Using DataHub",
      "JupyterHubs in this repository"
    ]
  },
  {
    "objectID": "users/hubs.html#data-100",
    "href": "users/hubs.html#data-100",
    "title": "JupyterHubs in this repository",
    "section": "Data 100",
    "text": "Data 100\nThis hub is for Data 100 which has a unique user and grading environment. It runs on Google Cloud Platform in the ucb-datahub-2018 account. You can see all config for it under deployments/data100.\nData100 also has shared folders between staff (professors and GSIs) and students. Staff, assuming they have been added as admins in config/common.yaml, can see a shared and a shared-readwrite folder. Students can only see the shared folder, which is read-only. Anything that gets put in shared-readwrite is automatically viewable in shared, but as read-only files. The purpose of this is to be able to share large data files instead of having one per student.",
    "crumbs": [
      "Using DataHub",
      "JupyterHubs in this repository"
    ]
  },
  {
    "objectID": "users/hubs.html#data-102",
    "href": "users/hubs.html#data-102",
    "title": "JupyterHubs in this repository",
    "section": "Data 102",
    "text": "Data 102\nData 102 runs on Google Cloud Platform in the ucb-datahub-2018 project. You can see all config for it under deployments/data102.",
    "crumbs": [
      "Using DataHub",
      "JupyterHubs in this repository"
    ]
  },
  {
    "objectID": "users/hubs.html#data8x-hub",
    "href": "users/hubs.html#data8x-hub",
    "title": "JupyterHubs in this repository",
    "section": "Data8X Hub",
    "text": "Data8X Hub\nA hub for the data8x course on EdX. This hub is open to use by anyone in the world, using LTI Authentication to provide login capability from inside EdX.\nIt runs on Google Cloud Platform in the data8x-scratch project. You can see all config for it under deployments/data8x.",
    "crumbs": [
      "Using DataHub",
      "JupyterHubs in this repository"
    ]
  },
  {
    "objectID": "users/features.html",
    "href": "users/features.html",
    "title": "Features",
    "section": "",
    "text": "This page lists the various environments, applications, and tools we offer on DataHub. Not all those listed here are available on all hubs, but we can easily enable them."
  },
  {
    "objectID": "users/features.html#programming-languages",
    "href": "users/features.html#programming-languages",
    "title": "Features",
    "section": "Programming Languages",
    "text": "Programming Languages\nWe support the usual suspects - Python, R, and Julia. However, Jupyter and other applications can support many more. If you would like to use a different, open-source programming language, contact us."
  },
  {
    "objectID": "users/features.html#applications",
    "href": "users/features.html#applications",
    "title": "Features",
    "section": "Applications",
    "text": "Applications\nOur diverse user population has diverse needs, so we offer many different user interfaces for instructors to choose from.\n\nJupyterLab\n\n\n\nDo complex layouts with JupyterLab\n\n\nJupyterLab is a more modern and customizable version of the classic Jupyter notebook from the Jupyter project. Most of our classes use JupyterLab.\n\n\nJupyter Notebook (Classic)\nThis familiar interface is used for most of our introductory classes. It is document oriented, no-frills, and well known by a lot of people.\n\n\nRStudio\n\n\n\nRStudio Screenshot\n\n\nWe want to provide first class support for teaching with R, which means providing strong support for RStudio. This also includes support for running Shiny applications.\nTry RStudio on DataHub with your berkeley.edu account, or on Binder without a berkeley.edu account.\n\n\nRemote Desktop\n\n\n\nDo image processing with qt\n\n\nSometimes, you just need to use something that requires a full desktop environment to run. Instead of trying to get students to install things locally, we offer a full fledged Linux Desktop environment they can access from inside their browser! This is just a different ‘UI’ on the same infrastructure as the notebook environment, so they all use the same libraries and home directories.\nTry remote desktop on EECS DataHub with your berkeley.edu account, or on Binder without a berkeley.edu account.\n\n\nVisual Studio Code\n\n\n\nCompile C with vscode\n\n\nSometimes you just want an IDE, not a notebook environment. We are experimenting with a hosted, web version of the popular Visual Studio Code editor, to see if it would be useful for teaching more traditional CS classes.\nTry VS Code on EECS DataHub with your berkeley.edu account, or on Binder without a berkeley.edu account.\n\n\nOther Web Applications\nWe can make many web based applications work on a hub. Contact us and we’ll see what we can do!\n\n\nPostgresql\nSome of our classes require using real databases to teach. We now experimentally offer a postgresql server for each user on the data100 hub.\nThe data does not persist right now, but we can turn that on whenever needed."
  },
  {
    "objectID": "users/features.html#more",
    "href": "users/features.html#more",
    "title": "Features",
    "section": "More?",
    "text": "More?\nWe want to find solution to your interesting problems, so please bring us your interesting problems. 😁"
  },
  {
    "objectID": "users/authentication.html",
    "href": "users/authentication.html",
    "title": "User Authentication",
    "section": "",
    "text": "UC Berkeley uses a Canvas instance, called bcourses.berkeley.edu. Almost all our hubs use this for authentication, although not all yet.",
    "crumbs": [
      "Using DataHub",
      "User Authentication"
    ]
  },
  {
    "objectID": "users/authentication.html#who-has-access",
    "href": "users/authentication.html#who-has-access",
    "title": "User Authentication",
    "section": "Who has access?",
    "text": "Who has access?\nAnyone who can log in to bcourses can log into our JupyterHubs. This includes all berkeley affiliates. If you have a working berkeley.edu email account, you can most likely log in to bcourses, and hence to our JupyterHubs.\nStudents have access for 9 months after they graduate. If they have an incomplete, they have 13 months of access instead.",
    "crumbs": [
      "Using DataHub",
      "User Authentication"
    ]
  },
  {
    "objectID": "users/authentication.html#non-berkeley-affiliates",
    "href": "users/authentication.html#non-berkeley-affiliates",
    "title": "User Authentication",
    "section": "Non-berkeley affiliates",
    "text": "Non-berkeley affiliates\nIf someone who doesn’t have a berkeley.edu account wants to use the JupyterHubs, they need to get a CalNet Sponsored Guest account This gives people access to bcourses, and hence to all the JupyterHubs.",
    "crumbs": [
      "Using DataHub",
      "User Authentication"
    ]
  },
  {
    "objectID": "users/authentication.html#troubleshooting",
    "href": "users/authentication.html#troubleshooting",
    "title": "User Authentication",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you can log in to bcourses but not to any of the JupyterHubs, please contact us.\nIf you can not log in to bcourses, please contact bcourses support",
    "crumbs": [
      "Using DataHub",
      "User Authentication"
    ]
  },
  {
    "objectID": "incidents/2019-05-01-service-account-leak.html",
    "href": "incidents/2019-05-01-service-account-leak.html",
    "title": "Service Account key leak incident",
    "section": "",
    "text": "Service account keys that granted restricted access to some of our cloud services were inadvertently leaked on GitHub. Google immediately notified us in seconds, and the credentials were revoked within the next few minutes."
  },
  {
    "objectID": "incidents/2019-05-01-service-account-leak.html#summary",
    "href": "incidents/2019-05-01-service-account-leak.html#summary",
    "title": "Service Account key leak incident",
    "section": "",
    "text": "Service account keys that granted restricted access to some of our cloud services were inadvertently leaked on GitHub. Google immediately notified us in seconds, and the credentials were revoked within the next few minutes."
  },
  {
    "objectID": "incidents/2019-05-01-service-account-leak.html#impact",
    "href": "incidents/2019-05-01-service-account-leak.html#impact",
    "title": "Service Account key leak incident",
    "section": "Impact",
    "text": "Impact\nDeployments are paused until this was fixed."
  },
  {
    "objectID": "incidents/2019-05-01-service-account-leak.html#timeline",
    "href": "incidents/2019-05-01-service-account-leak.html#timeline",
    "title": "Service Account key leak incident",
    "section": "Timeline",
    "text": "Timeline\n\nMay 1 2019, 3:18 PM\nA template + documentation for creating new hubs easily is pushed to GitHub as a pull request. This inadvertantly contained live credentials for pushing & pulling our (already public) docker images, and for access to our kubernetes clusters.\nGoogle immediately notified us via email within seconds that this might be a breach.\n\n\n3:19 PM\nDiscussion and notification starts in slack about dealing with the issue.\n\n\n3:27 PM\nBoth keys are revoked so they are no longer valid credentials.\n\n\n3:36 PM\nAll in-use resources are checked, and verified to not be compromised by automated bots looking for leaked accounts.\n\n\n3:40 PM\nAn email is sent out to all owners of the compromised project (ucb-datahub-2018) giving an all-clear."
  },
  {
    "objectID": "incidents/2019-05-01-service-account-leak.html#action-items",
    "href": "incidents/2019-05-01-service-account-leak.html#action-items",
    "title": "Service Account key leak incident",
    "section": "Action items",
    "text": "Action items\n\nDon’t duplicate service key credentials across multiple hubs. Issue\nSwitch to a different secret management strategy than what we have now. Issue"
  },
  {
    "objectID": "incidents/2018-02-28-hung-node.html",
    "href": "incidents/2018-02-28-hung-node.html",
    "title": "A node hangs, causing a subset of users to report issues",
    "section": "",
    "text": "On February 28, 2018, a handful of users reported on piazza that there servers wouldn’t start. It was determined that all problematic servers were running on the same node. After the node was cordoned and rebooted, the student servers were able to start properly."
  },
  {
    "objectID": "incidents/2018-02-28-hung-node.html#summary",
    "href": "incidents/2018-02-28-hung-node.html#summary",
    "title": "A node hangs, causing a subset of users to report issues",
    "section": "",
    "text": "On February 28, 2018, a handful of users reported on piazza that there servers wouldn’t start. It was determined that all problematic servers were running on the same node. After the node was cordoned and rebooted, the student servers were able to start properly."
  },
  {
    "objectID": "incidents/2018-02-28-hung-node.html#timeline",
    "href": "incidents/2018-02-28-hung-node.html#timeline",
    "title": "A node hangs, causing a subset of users to report issues",
    "section": "Timeline",
    "text": "Timeline\n\n2018-02-28 21:21\nThree students report problems starting their server on piazza and a GSI links to the reports on slack. More reports come in by 21:27.\n\n\n21:30\nThe infrastructure team is alerted to the problem. The command kubectl --namespace=prod get pod -o wide | egrep -v -e prepull -e Running shows that all non-running pods were scheduled on the same node. Most of the pods have an “Unknown” status while the rest are in “Terminating”. The oldest problematic pod is 29m.\n\n\n21:34\nThe node k8s-pool1-19522833-9 is cordoned. It has a load of about 90 with no processes consuming much CPU. The node is rebooted via sysrq trigger. The hung pods remain stuck.\n\n\n21:39\nWhen the node comes back online, kubectl reports no more hung pods. Students are able to start their servers."
  },
  {
    "objectID": "incidents/2018-02-28-hung-node.html#conclusion",
    "href": "incidents/2018-02-28-hung-node.html#conclusion",
    "title": "A node hangs, causing a subset of users to report issues",
    "section": "Conclusion",
    "text": "Conclusion\nA problematic VM prevented nodes from launching pods. Once the VM was cordoned and rebooted, pods launch without trouble."
  },
  {
    "objectID": "incidents/2018-02-28-hung-node.html#action-items",
    "href": "incidents/2018-02-28-hung-node.html#action-items",
    "title": "A node hangs, causing a subset of users to report issues",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nMonitor the cluster for non-running pods and send an alert if the count exceeds a threshold or if the non-running pods are clustered on the same node(s)."
  },
  {
    "objectID": "incidents/2017-04-03-cluster-full-incident.html",
    "href": "incidents/2017-04-03-cluster-full-incident.html",
    "title": "Custom autoscaler does not scale up when it should",
    "section": "",
    "text": "On April 3, 2017, as students were returning from spring break, the cluster wasn’t big enough in time and several students had errors spawning. This was because the simple-autoscaler was ‘stuck’ on a populate call. More capacity was manually added, the pending pods were deleted & this seemed to fix the outage."
  },
  {
    "objectID": "incidents/2017-04-03-cluster-full-incident.html#summary",
    "href": "incidents/2017-04-03-cluster-full-incident.html#summary",
    "title": "Custom autoscaler does not scale up when it should",
    "section": "",
    "text": "On April 3, 2017, as students were returning from spring break, the cluster wasn’t big enough in time and several students had errors spawning. This was because the simple-autoscaler was ‘stuck’ on a populate call. More capacity was manually added, the pending pods were deleted & this seemed to fix the outage."
  },
  {
    "objectID": "incidents/2017-04-03-cluster-full-incident.html#timeline",
    "href": "incidents/2017-04-03-cluster-full-incident.html#timeline",
    "title": "Custom autoscaler does not scale up when it should",
    "section": "Timeline",
    "text": "Timeline\n\nOver spring break week\nThe cluster is scaled down to a much smaller size (7 machines), and the simple scaler is left running.\n\n\n2017-04-03 11:32\nStudents report datahub isn’t working on Piazza, and lots of Pods in PENDING state.\nDoing a kubectl --namespace=datahub describe pod &lt;pod-name&gt; said the pod was unschedulable because there wasn’t enough RAM in the cluster. This clearly implied the cluster wasn’t big enough.\nLooking at the simple scaler shows it was ‘stuck’ at a populate.bash call, and wasn’t scaling up fast enough.\n\n\n11:35\nThe cluster is manually scaled up to 30 nodes:\ngcloud compute instance-groups managed resize gke-prod-highmem-pool-0df1a536-grp --size=30\nAt the same time, pods stuck in Pending state are deleted so they don’t become ghost pods, with:\nkubectl --namespace=datahub get pod | grep -v Running | grep -P 'm$' | awk '{print $1;}' | xargs -L1 kubectl --namespace=datahub delete pod\n\n\n11:40\nThe nodes have come up, so a populate.bash call is performed to pre-populate all user container images on the new nodes.\nUsers in Pending state are deleted again.\n\n\n11:46\nThe populate.bash call is complete, and everything is back online!"
  },
  {
    "objectID": "incidents/2017-04-03-cluster-full-incident.html#conclusion",
    "href": "incidents/2017-04-03-cluster-full-incident.html#conclusion",
    "title": "Custom autoscaler does not scale up when it should",
    "section": "Conclusion",
    "text": "Conclusion\nOur simple scaler didn’t scale up fast enough when a large number of students came back online quickly after a time of quiet (spring break). Took a while for this to get noticed, and manual scaling fixed everything."
  },
  {
    "objectID": "incidents/2017-04-03-cluster-full-incident.html#action-items",
    "href": "incidents/2017-04-03-cluster-full-incident.html#action-items",
    "title": "Custom autoscaler does not scale up when it should",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nWhen coming back from breaks, pre-scale the cluster back up.\nConsider cancelling spring break.\n\n\n\nMonitoring\n\nHave monitoring for pods stuck in non-Running states"
  },
  {
    "objectID": "incidents/2017-03-20-too-many-volumes.html",
    "href": "incidents/2017-03-20-too-many-volumes.html",
    "title": "Too many volumes per disk leave students stuck",
    "section": "",
    "text": "From sometime early March 20 2017 till about 1300, some new student servers were stuck in Pending forever, giving them 500 errors. This was an unintended side-effect of reducing student memory limit to 1G while keeping the size of our nodes constant, causing us to hit a Google Cloud limit on number of disks per node. This was fixed by spawning more nodes that were smaller."
  },
  {
    "objectID": "incidents/2017-03-20-too-many-volumes.html#summary",
    "href": "incidents/2017-03-20-too-many-volumes.html#summary",
    "title": "Too many volumes per disk leave students stuck",
    "section": "",
    "text": "From sometime early March 20 2017 till about 1300, some new student servers were stuck in Pending forever, giving them 500 errors. This was an unintended side-effect of reducing student memory limit to 1G while keeping the size of our nodes constant, causing us to hit a Google Cloud limit on number of disks per node. This was fixed by spawning more nodes that were smaller."
  },
  {
    "objectID": "incidents/2017-03-20-too-many-volumes.html#timeline",
    "href": "incidents/2017-03-20-too-many-volumes.html#timeline",
    "title": "Too many volumes per disk leave students stuck",
    "section": "Timeline",
    "text": "Timeline\n\nMarch 18, 16:30\nRAM per student is reduced from 2G to 1G, as a resource optimization measure. The size of our nodes remains the same (26G RAM), and many are cordonned off and slowly decomissioned over the coming few days.\nLife seems fine, given the circumstances.\n\n\nMarch 20, 12:44\nNew student servers report a 500 error preventing them from logging on. This is deemed widespread & not an isolated incident.\n\n\n12:53\nA kubectl describe pod on an affected student’s pod shows it’s stuck in Pending state, with an error message:\npod failed to fit in any node fit failure on node (XX): MaxVolumeCount\nThis seems to be common problem for all the new student servers, which are all stuck in Pending state.\nGoogling leads to https://github.com/kubernetes/kubernetes/issues/24317 - even though Google Compute Engine can handle more than 16 disks per node (we had checked this before deploying), Kubernetes itself still can not. This wasn’t foreseen, and seemed to be the direct cause of the incident.\n\n\n13:03\nA copy of the instance template that is used by Google Container Engine is made and then modified to spawn smaller nodes (n1-highmem-2 rather than n1-highmem-4). The managed instance group used by Google Container Engine is then modified to use the new template. This was the easiest way to not distrupt students for whom things are working, while also allowing new students to be able to log in.\nThis new instance group was then set to expand for 30 new nodes, which will provide capacity for about 12 students each. populate.bash was also run to make sure that students pods start up on time in the newnodes.\n\n\n13:04\nThe simple autoscaler is stopped, on fear that it’ll be confused by the unusal mixed state of the nodes and do something wonky.\n\n\n13:11\nAll the new nodes are online, and populate.bash has completed. Pods start leaving the Pending state.\nHowever, since it’s been more than the specified timeout that JupyterHub will wait before giving up on Pod (5 minutes), JupyterHub doesn’t know the pods exist. This causes state of cluster + state in JupyterHub to go out of sync, causing the dreaded ‘redirected too many times’ error. Admins need to manually stop and start user pods in the control panel as users report this to fix this issue.\n\n\n14:23\nThe hub and proxy pods are restarted since there were plenty of ‘redirected too many times’ errors. This seems to catch most users state, although some requests still failed with a 599 timeout (similar to an earlier incident, but much less frequent). A long tail of manual user restarts are performed by admins over the next few days."
  },
  {
    "objectID": "incidents/2017-03-20-too-many-volumes.html#action-items",
    "href": "incidents/2017-03-20-too-many-volumes.html#action-items",
    "title": "Too many volumes per disk leave students stuck",
    "section": "Action Items",
    "text": "Action Items\n\nUpstream: Kubernetes\n\nKeep an eye on the status of the bug we ran into\n\n\n\nUpstream: JupyterHub\n\nTrack down and fix the ‘too many redirects’ issue at source. Issue\n\n\n\nCleanup\n\nDelete all the older larger nodes that are no longer in use. (Done!)\n\n\n\nMonitoring\n\nHave alerting for when there are any number of pods in Pending state for a non-negligible amount of time. There is always something wrong when this happens."
  },
  {
    "objectID": "incidents/2017-02-09-datahub-db-outage.html",
    "href": "incidents/2017-02-09-datahub-db-outage.html",
    "title": "JupyterHub db manual overwrite",
    "section": "",
    "text": "Datahub was reportedly down at 1am. Users attempting to log in to datahub were greeted with a proxy error. The hub pod was up but the log was full of sqlite errors. After the hub pod was deleted and a new one came up, students logging in to datahub found their notebooks were missing and their home directories were empty. Once this was fixed, some students still were being logged in as a different particular user. Finally, students with a ‘.’ in their username were still having issues after everyone else was fine. This was all fixed and an all-clear signalled at about 2017-02-09 11:35 AM."
  },
  {
    "objectID": "incidents/2017-02-09-datahub-db-outage.html#summary",
    "href": "incidents/2017-02-09-datahub-db-outage.html#summary",
    "title": "JupyterHub db manual overwrite",
    "section": "",
    "text": "Datahub was reportedly down at 1am. Users attempting to log in to datahub were greeted with a proxy error. The hub pod was up but the log was full of sqlite errors. After the hub pod was deleted and a new one came up, students logging in to datahub found their notebooks were missing and their home directories were empty. Once this was fixed, some students still were being logged in as a different particular user. Finally, students with a ‘.’ in their username were still having issues after everyone else was fine. This was all fixed and an all-clear signalled at about 2017-02-09 11:35 AM."
  },
  {
    "objectID": "incidents/2017-02-09-datahub-db-outage.html#timeline",
    "href": "incidents/2017-02-09-datahub-db-outage.html#timeline",
    "title": "JupyterHub db manual overwrite",
    "section": "Timeline",
    "text": "Timeline\n\n2017-02-09 00:25 - 00:29 AM\nAttempting to debug some earlier 400 errors, Trying to set base_url and ip to something incorrect to see if it will cause a problem.\nkubectl exec hub-deployment-something --namespace=datahub -it bash\napt-get install sqlite3\nsqlite3\nATTACH 'jupyterhub.sqlite AS my_db;\nSELECT name FROM my_db.sqlite_master WHERE type='table';\nSELECT * FROM servers;\nSELECT * FROM servers WHERE base_url LIKE '%&lt;USER&gt;%';\nUPDATE servers SET ip='' WHERE base_url LIKE '%&lt;USER&gt;%';\nUPDATE servers SET base_url='/&lt;something-wrong&gt; WHERE base_url LIKE '%&lt;USER&gt;%';\nCtrl+D (exit back into bash shell)\nchecked datahub.berkeley.edu, and nothing happened to the account saw that the sql db was not updated, attempt to run .save\n```bash\nsqlite3\n.save jupyterhub.sqlite\nThis replaced the db with an empty one, since ATTACH was not run beforehand.\n\n\n0:25:59 AM\nFollowing exception shows up in hub logs:\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: proxies [SQL: 'SELECT proxies.id AS proxies_id, proxies._public_server_id AS proxies__public_server_id, proxies._api_server_id AS proxies__api_server_id \\nFROM proxies \\nWHERE proxies.id = ?'] [parameters: (1,)]\nThis continues for hub table as well, since those two seem to be most frequently used.\n\n\n1:12 AM\nSam’s roommate notices that he can log in to datahub but all his notebooks are gone. We notice that there are only ~50 users on the JHub admin panel when there used to be ~1000, so we believe that this is because the JHub sqlite user database got wiped/corrupted, then created an account for his roommate when he logged in, then created a new persistent disk since it lost track of his old one.\nThis is confirmed soon after:\n$ kubectl --namespace=datahub get pvc | grep  &lt;username&gt;\nclaim-&lt;username&gt;-257     Bound     pvc-3b405e13-ddb4-11e6-98ef-42010af000c3   10Gi       RWO           21d\nclaim-&lt;username&gt;-51      Bound     pvc-643dd900-eea7-11e6-a291-42010af000c3   10Gi       RWO           5m\n\n\n1:28 AM\nWe shut down the hub pod by scaling the replicas to 0.\nWe then begin recreating the JHub sqlite database by taking the Kubernetes PVCs and matching them back with the user ids. We could do this because the name of the PVC contains a sanitized form of the username and the userid.\nHere’s the notebook that was used to recreate the db from PVCs: pvc-sqlite.ipynb 2017-02-09-datahub-db-outage-pvc-recreate-script.ipynb\n\n\n2:34 AM\nWe recreate the sqlite3 database. Initially each user’s cookie_id was set to a dummy cookie value.\n\n\n2:42 AM\nUser cookie_id values are changed to null rather than dummy value. The sqlite file is then attached back to datahub. The number of users shown on admin page is back to ~1000. The hub was up, and a spot check of starting other user’s servers seem to work. Some users get redirected to one particular user, but deleting and recreating the affected user seems to fix this.\n\n\n10:11 AM\nAttempt to log everyone out by changing cookie secret in hub pod at /srv/jupyterhub/jupyterhub_cookie_secret. Just one character near the end was changed, and pod restarted. No effect. One character at the beginning of secret was changed next, and restarted - this caused actual change, and logged all users out.\nPeople are still being redirected to one particular user’s account when they log in. More looking around required.\n\n\n10:17 AM\nJohn Denero advises students to use ds8.berkeley.edu right now. ds8.berkeley.edu promptly starts crashing because it does not have resources for a data8 level class.\n\n\n10:29 AM\nAll user pods are deleted, which finally properly logs everyone out. However, people logging in are still all getting the same user’s pods.\n\n\n10:36 AM\nNotice that cookie_id column in the user database table is empty for many users, and the user that everyone is being logged in as has an empty cookie_id too and is the ‘first’ on the table when sorted in ascending by id. Looking at the JupyterHub code, cookie_id is always supposed to be set to a uuid, and never supposed to be empty. Setting cookie_id for users fixes their issues, and seems to spawn them into their own notebook.\n\n\n10:45 AM\nA script is run that populates cookie_id for all users, and restarts the hub to make sure there’s no stale cache in RAM. All user pods are deleted again. Most users are back online now! More users start testing and confirming things are working for them.\n\n\n10:53 AM\nUser with a ‘.’ in their name reports that they’re getting an empty home directory. More investigation shows two users - one with a ‘.’ in their name that is newer, and one with a ‘-’ in their name instead of ‘.’ that is older. Hypothesis is that one of them is the ‘original’, but they’re all attaching to a new one that is empty. Looking at pvcs confirms this - there are two PVCs for users with a . in their name who have tried to log in, and they differ only by ids.\nThere is some confusion about users ending up on prob140, because the data8.org homework link is changed to use that temporarily.\n\n\n11:05 AM\nDirectly modifying the user table to rename the user with the ‘-’ in the name to have a ‘.’ seems to work for people.\n\n\n11:15 AM\nA script is run that modifies the database user table for all users with a ‘-’ in their name, and the ‘-’ is replaced with a ‘.’. The new users created with the ‘.’ in their name are dropped before this.\n\n\n11:17 AM\nAll clear given for datahub.berkeley.edu\n\n\n11:19 AM\nLocally verified that running .save  on sqlite3 will overwrite the db file without any confirmation, and is most likely cause of the issue. Conclusion Accidental overwriting of the sqlite file during routine debugging operation led all tables being deleted. Users were getting new user ids when they were logging in now, causing them to get new disks provisioned - and these disks were empty. During reconstruction of the db, cookie_id was missing for several users, causing them all to log in to one particular user’s notebook. Users with ‘.’ in their name were also set up slightly incorrectly - their pods have ‘-’ in them but the user name should have a ‘.’."
  },
  {
    "objectID": "incidents/2017-02-09-datahub-db-outage.html#action-items",
    "href": "incidents/2017-02-09-datahub-db-outage.html#action-items",
    "title": "JupyterHub db manual overwrite",
    "section": "Action items",
    "text": "Action items\n\nUpstream bug reports for JupyterHub\n\nJupyterHub only uses a certain length of the cookie secret, and discards the rest. This causes confusion when trying to change it to log people out. Issue\nThe cookie_id column in the users table should have UNIQUE and NOT NULL constraints. Issue\n\n\n\nUpstream bug reports for KubeSpawner\n\nSupport using username hashes in PVC and Pod Names rather than user ids, so that pod and PVC names remain constant even when DB is deleted. Issue\n\n\n\nUpstream bug reports for OAuthenticator\n\nSupport setting id of user in user table to be same as ‘id’ provided by Google authenticator, thus providing a stable userid regardless of when the user first logged in. Issue\n\n\n\nDataHub deployment changes\n\nSwitch to using Google Cloud SQL, which provides hosted and managed MySQL database\nPerform regular and tested backups of the database\nStart writing an operational FAQ for things to do and not do\nSetup better monitoring and paging systems\nDocument escalation procedures explicitly"
  },
  {
    "objectID": "incidents/2017-10-10-hung-nodes.html",
    "href": "incidents/2017-10-10-hung-nodes.html",
    "title": "Docker dies on a few Azure nodes",
    "section": "",
    "text": "On Oct 10, 2017, some user pods were not starting or terminating correctly. After checking node status, it was found that all affected pods were running on two specific nodes. The docker daemon wasn’t responsive on these nodes so they were cordoned off. User pods were then able to start correctly."
  },
  {
    "objectID": "incidents/2017-10-10-hung-nodes.html#summary",
    "href": "incidents/2017-10-10-hung-nodes.html#summary",
    "title": "Docker dies on a few Azure nodes",
    "section": "",
    "text": "On Oct 10, 2017, some user pods were not starting or terminating correctly. After checking node status, it was found that all affected pods were running on two specific nodes. The docker daemon wasn’t responsive on these nodes so they were cordoned off. User pods were then able to start correctly."
  },
  {
    "objectID": "incidents/2017-10-10-hung-nodes.html#timeline",
    "href": "incidents/2017-10-10-hung-nodes.html#timeline",
    "title": "Docker dies on a few Azure nodes",
    "section": "Timeline",
    "text": "Timeline\n\n2017-05-09 10:45a\nA report in the course Piazza said that two students couldn’t start their servers. The /hub/admin interface was not able to start them either. It was reported that the students may have run out of memory.\n\n\n12:29p\nThe user pods were stuck in Terminating state and would not respond to explicit delete. The pods were forcefully deleted with kubectl --namespace=prod delete pod jupyter-&lt;name&gt; --grace-period=0 --force. The user pods started correctly via /hub/admin.\n\n\n13:27\nIt was reported in the course slack that another student’s server wasn’t starting correctly. After checking one of the pod logs, it was observed that the node hosting the pods, k8s-pool1-19522833-13, was also hosting many more pods stuck in a Terminating state. docker ps was hanging on that node. The node was cordoned.\n\n\n13:42\nIt was reported in slack that the student’s server was able to start.\nBy this time, the cluster was checked for all pods to see if any other nodes were hosting an unusual number of pods in Terminating. It was found that k8s-pool2-19522833-9 was in a similar state. All stuck pods on that node were forcefully deleted and the node was also cordoned. docker ps was hung on that node too. pool2-…-9 had a load of 530 while pool1-…-13 had a load of 476. On the latter, hypercube was at 766% cpu utilization while it was nominal on the former. Node pool1-…-13 was rebooted from the shell however it did not come back online. The node was manually restarted from the Azure portal but it still didn’t come back.\nA node previously cordoned on another day, pool1-…-14, was rebooted. It came back online and was uncordoned.\n\n\n13:51\nSome relevant systemctl status docker logs were captured from pool2-…-9:\nOct 10 20:55:30 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:30.790401257Z\" level=error msg=\"containerd: start container\" error=\"containerd: container did not start before the specified timeout\" id=abd267ef08b4a4184e19307be784d62470f9a713b59e406249c6cdf0bb333260\nOct 10 20:55:30 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:30.790923460Z\" level=error msg=\"Create container failed with error: containerd: container did not start before the specified timeout\"\nOct 10 20:55:30 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:30.810309575Z\" level=error msg=\"Handler for POST /v1.24/containers/abd267ef08b4a4184e19307be784d62470f9a713b59e406249c6cdf0bb333260/start returned error: containerd: container did not start before the specified timeout\"\nOct 10 20:55:36 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:36.146453953Z\" level=error msg=\"containerd: start container\" error=\"containerd: container did not start before the specified timeout\" id=2ba6787503ab6123b509811fa44c7e42986de0b800cc4226e2ab9484f54e8741\nOct 10 20:55:36 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:36.147565759Z\" level=error msg=\"Create container failed with error: containerd: container did not start before the specified timeout\"\nOct 10 20:55:36 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:36.166295370Z\" level=error msg=\"Handler for POST /v1.24/containers/2ba6787503ab6123b509811fa44c7e42986de0b800cc4226e2ab9484f54e8741/start returned error: containerd: container did not start before the specified timeout\"\nOct 10 20:55:36 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:36.169360588Z\" level=error msg=\"Handler for GET /v1.24/containers/json returned error: write unix /var/run/docker.sock-&gt;@: write: broken pipe\"\nOct 10 20:55:36 k8s-pool2-19522833-9 dockerd[1237]: http: multiple response.WriteHeader calls\nOct 10 20:55:36 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:36.280209444Z\" level=error msg=\"Handler for GET /v1.24/containers/610451d9d86a58117830ea7c0189f6157ba9a9602739ee23723e923de8c7e23e/json returned error: No such container: 610451d9d86a58117830ea7c0189f6157ba9a9602739ee23723e923de8c7e23e\"\nOct 10 20:55:39 k8s-pool2-19522833-9 docker[1237]: time=\"2017-10-10T20:55:39.095888009Z\" level=error msg=\"Handler for GET /v1.24/containers/54b64ca3c1e7ef4a04192ccdaf1cb9309d73acebd7a08e13301f3263de3d376a/json returned error: No such container: 54b64ca3c1e7ef4a04192ccdaf1cb9309d73acebd7a08e13301f3263de3d376a\"\n\n\n14:00\ndatahub@k8s-pool2-19522833-9:~$ ps aux | grep exe | wc -l\n520\ndatahub@k8s-pool2-19522833-9:~$ ps aux | grep exe | head -5\nroot        329  0.0  0.0 126772  9812 ?        Dsl  00:36   0:00 /proc/self/exe init\nroot        405  0.0  0.0  61492  8036 ?        Dsl  00:36   0:00 /proc/self/exe init\nroot        530  0.0  0.0 127028  8120 ?        Dsl  00:36   0:00 /proc/self/exe init\nroot        647  0.0  0.0 127028  8124 ?        Dsl  13:07   0:00 /proc/self/exe init\nroot        973  0.0  0.0  77884  8036 ?        Dsl  13:10   0:00 /proc/self/exe init\n\n\n14:30\npool1-…-13 was manually stopped in the Azure portal, then manually started. It came back online afterwards and docker was responsive. It was uncordoned.\npool2-…-9 was manually stopped in the Azure portal.\n\n\n14:45\npool2-…-9 completed stopping and was manually started in the Azure portal.\n\n\n17:25\nIt was observed that /var/lib/docker on pool1-19522833-13/10.240.0.7 was on / (sda) and not on /mnt (sdb)."
  },
  {
    "objectID": "incidents/2017-10-10-hung-nodes.html#conclusion",
    "href": "incidents/2017-10-10-hung-nodes.html#conclusion",
    "title": "Docker dies on a few Azure nodes",
    "section": "Conclusion",
    "text": "Conclusion\nDocker was hung on two nodes, preventing pods from starting or stopping correctly."
  },
  {
    "objectID": "incidents/2017-10-10-hung-nodes.html#action-items",
    "href": "incidents/2017-10-10-hung-nodes.html#action-items",
    "title": "Docker dies on a few Azure nodes",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nWhen there are multiple reports of student servers not starting or stopping correctly, check to see if the user pods were run on the same node(s).\nDetermine how many nodes are not mounting /var/lib/docker on sdb1.\n\n\n\nMonitoring\n\nLook for elevated counts of pods stuck in Terminating state. For example, kubectl --namespace=prod get pod -o wide| grep Terminating"
  },
  {
    "objectID": "incidents/2017-05-09-gce-billing.html",
    "href": "incidents/2017-05-09-gce-billing.html",
    "title": "Oops we forgot to pay the bill",
    "section": "",
    "text": "On May 9, 2017, the compute resources associated with the data-8 project at GCE were suspended. All hubs including datahub, stat28, and prob140 were not reachable. This happened because the grant that backed the project’s billing account ran out of funds. The project was moved to a different funding source and the resources gradually came back online."
  },
  {
    "objectID": "incidents/2017-05-09-gce-billing.html#summary",
    "href": "incidents/2017-05-09-gce-billing.html#summary",
    "title": "Oops we forgot to pay the bill",
    "section": "",
    "text": "On May 9, 2017, the compute resources associated with the data-8 project at GCE were suspended. All hubs including datahub, stat28, and prob140 were not reachable. This happened because the grant that backed the project’s billing account ran out of funds. The project was moved to a different funding source and the resources gradually came back online."
  },
  {
    "objectID": "incidents/2017-05-09-gce-billing.html#timeline",
    "href": "incidents/2017-05-09-gce-billing.html#timeline",
    "title": "Oops we forgot to pay the bill",
    "section": "Timeline",
    "text": "Timeline\n\n2017-05-09 16:51\nA report in the Data 8 Spring 2017 Staff slack, #jupyter channel, says that datahub is down. This is confirmed. Attempting to access the provisioner via gcloud compute ssh provisioner-01 fails with:\nERROR: (gcloud.compute.ssh) Instance [provisioner-01] in zone [us-central1-a] has not been allocated an external IP address yet. Try rerunning this command later.\n\n\n17:01\nThe Google Cloud console shows that the billing account has run out of the grant that supported the data-8 project. The project account is moved to another billing account which has resources left.\nThe billing state is confirmed by gcloud messages:\nGoogle Compute Engine: Project data-8 cannot accept requests to setMetadata while in an inactive billing state.  Billing state may take several minutes to update.\n\n\n17:09\nprovisioner-01 is manually started. All pods in the datahub namespace are deleted.\n\n\n17:15\ndatahub is back online. stat28 and prob140 hub pods are manually killed. After a few moments the hubs are back online. The autoscaler is started.\n\n\n17:19\nThe slack duplicator is started.\n\n\n2017-05-10 10:48\nA report in uc-jupyter #jupyterhub says that try.datahub is down. This is confirmed and the hub in the tmp namespace is killed. The hub comes online a couple of minutes later."
  },
  {
    "objectID": "incidents/2017-05-09-gce-billing.html#conclusion",
    "href": "incidents/2017-05-09-gce-billing.html#conclusion",
    "title": "Oops we forgot to pay the bill",
    "section": "Conclusion",
    "text": "Conclusion\nThere was insufficient monitoring of the billing status."
  },
  {
    "objectID": "incidents/2017-05-09-gce-billing.html#action-items",
    "href": "incidents/2017-05-09-gce-billing.html#action-items",
    "title": "Oops we forgot to pay the bill",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nIdentify channels for billing alerts.\nIdentify billing threshold functions that predict when funds will run out.\nEstablish off-cloud backups. The plan is to do this via nbgdrive.\nStart autoscaler automatically. It is manually started at the moment.\n\n\n\nMonitoring\n\nSetup scheduled billing reports and threshold alarms.\nSetup hub monitoring!\nThe slack duplicator runs in one of the GCP clusters. When the clusters go down, slack messages aren’t forwarded from the data8-sp17-staff slack to uc-jupyter."
  },
  {
    "objectID": "incidents/2017-03-06-helm-config-image-mismatch.html",
    "href": "incidents/2017-03-06-helm-config-image-mismatch.html",
    "title": "Non-matching hub image tags cause downtime",
    "section": "",
    "text": "On the evening of Mar 6, the hub on prod would not come up after an upgrade. The upgrade was to accommodate a new disk for cogneuro that had been tested on dev. After some investigation it was determined that the helm’s config did not match the hub’s image. After the hub image was rebuilt and pushed out, then tested on dev, it was pushed out to prod. The problem was fixed in about 40 minutes.\nA few days later (March 12), similar almost outage is avoided when -dev breaks and deployment is put on hold. More debugging shows the underlying cause is that git submodules are hard to use. More documentation is provided, and downtime is averted!"
  },
  {
    "objectID": "incidents/2017-03-06-helm-config-image-mismatch.html#summary",
    "href": "incidents/2017-03-06-helm-config-image-mismatch.html#summary",
    "title": "Non-matching hub image tags cause downtime",
    "section": "",
    "text": "On the evening of Mar 6, the hub on prod would not come up after an upgrade. The upgrade was to accommodate a new disk for cogneuro that had been tested on dev. After some investigation it was determined that the helm’s config did not match the hub’s image. After the hub image was rebuilt and pushed out, then tested on dev, it was pushed out to prod. The problem was fixed in about 40 minutes.\nA few days later (March 12), similar almost outage is avoided when -dev breaks and deployment is put on hold. More debugging shows the underlying cause is that git submodules are hard to use. More documentation is provided, and downtime is averted!"
  },
  {
    "objectID": "incidents/2017-03-06-helm-config-image-mismatch.html#timeline",
    "href": "incidents/2017-03-06-helm-config-image-mismatch.html#timeline",
    "title": "Non-matching hub image tags cause downtime",
    "section": "Timeline",
    "text": "Timeline\nAll times in PST\n\nMarch 6 2017 22:59\ndev changes are deployed but hub does not start correctly. The describe output for the hub shows repeated instances of:\nError syncing pod, skipping: failed to “StartContainer” for “hub-container” with CrashLoopBackOff: “Back-off 10s restarting failed container=hub-container pod=hub-deployment-3498421336-91gp3_datahub-dev(bfe7d8bd-0303-11e7-ade6-42010a80001a)\nhelm chart for -dev is deleted and reinstalled.\n\n\n23:11\ndev changes are deployed successfully and tested. cogneuro’s latest data is available.\n\n\n23:21\nChanges are deployed to prod. The hub does not start properly. get pod -o=yaml on the hub pod shows that the hub container has terminated. The hub log shows that it failed due to a bad configuration parameter.\n\n\n21:31\nWhile the helm chart had been updated from git recently, the latest tag for the hub did not correspond with the one in either prod.yaml or dev.yaml.\n\n\n21:41\nThe hub image is rebuilt and pushed out.\n\n\n21:45\nThe hub is deployed on -dev.\n\n\n21:46\nThe hub is tested on -dev then deployed on -prod.\n\n\n21:50\nThe hub is tested on -prod. Students are reporting that the hub had been down.\n\n\nMarch 12 19:57\nA new deploy is attempted on -dev, but runs into same error. Deployments are halted for more debugging this time, and more people are called on.\n\n\n23:21\nMore debugging reveals that the commit update looked like this:\ndiff --git a/chart b/chart\nindex e38aba2..c590340 160000\n--- a/chart\n+++ b/chart\n@@ -1 +1 @@\n-Subproject commit e38aba2c5601de30c01c6f3c5cad61a4bf0a1778\n+Subproject commit c59034032f8870d16daba7599407db7e6eb53e04\ndiff --git a/data8/dev.yaml b/data8/dev.yaml\nindex 2bda156..ee5987b 100644\n--- a/data8/dev.yaml\n+++ b/data8/dev.yaml\n@@ -13,7 +13,7 @@ publicIP: \"104.197.166.226\"\n\n singleuser:\n   image:\n-    tag: \"e4af695\"\n+    tag: \"1a6c6d8\"\n   mounts:\n     shared:\n       cogneuro88: \"cogneuro88-20170307-063643\"\nOnly the tag should’ve been the only thing updated. The chart submodule is updated to c59034032f8870d16daba7599407db7e6eb53e04, which is from February 25 (almost two weeks old). This is the cause of the hub failing, since it is using a really old chart commit with a new hub image."
  },
  {
    "objectID": "incidents/2017-03-06-helm-config-image-mismatch.html#section-8",
    "href": "incidents/2017-03-06-helm-config-image-mismatch.html#section-8",
    "title": "Non-matching hub image tags cause downtime",
    "section": "23:27",
    "text": "23:27\nIt is determined that incomplete documentation about deployment processes caused git submodule update to be not run after a git pull, and so the chart was being accidentally moved back to older commits. Looking at the commit that caused the outage on March 6 showed the exact same root cause."
  },
  {
    "objectID": "incidents/2017-03-06-helm-config-image-mismatch.html#conclusion",
    "href": "incidents/2017-03-06-helm-config-image-mismatch.html#conclusion",
    "title": "Non-matching hub image tags cause downtime",
    "section": "Conclusion",
    "text": "Conclusion\nGit submodules are hard to use, and break most people’s mental model of how git works. Since our deployment requires that the submodule by in sync with the images used, this caused an outage."
  },
  {
    "objectID": "incidents/2017-03-06-helm-config-image-mismatch.html#action-items",
    "href": "incidents/2017-03-06-helm-config-image-mismatch.html#action-items",
    "title": "Non-matching hub image tags cause downtime",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nMake sure we treat any errors in -dev exactly like we would in prod. Any deployment error in prod should immediately halt future deployments & require a rollback or resolution before proceeding.\nWrite down actual deployment documentation & a checklist.\nMove away from git submodules to a separate versioned chart repository."
  },
  {
    "objectID": "incidents/index.html",
    "href": "incidents/index.html",
    "title": "Incident Reports",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nFeb 9, 2017\n\n\nJupyterHub db manual overwrite\n\n\n\n\nFeb 24, 2017\n\n\nCustom Autoscaler gonee haywire\n\n\n\n\nFeb 24, 2017\n\n\nProxy eviction strands user\n\n\n\n\nMar 6, 2017\n\n\nNon-matching hub image tags cause downtime\n\n\n\n\nMar 20, 2017\n\n\nToo many volumes per disk leave students stuck\n\n\n\n\nMar 23, 2017\n\n\nWeird upstream ipython bug kills kernels\n\n\n\n\nApr 3, 2017\n\n\nCustom autoscaler does not scale up when it should\n\n\n\n\nMay 9, 2017\n\n\nOops we forgot to pay the bill\n\n\n\n\nOct 10, 2017\n\n\nDocker dies on a few Azure nodes\n\n\n\n\nOct 19, 2017\n\n\nBilling confusion with Azure portal causes summer hub to be lost\n\n\n\n\nJan 25, 2018\n\n\nAccidental merge to prod brings things down\n\n\n\n\nJan 26, 2018\n\n\nHub starts up very slow, causing outage for users\n\n\n\n\nFeb 6, 2018\n\n\nAzure PD refuses to detach, causing downtime for data100\n\n\n\n\nFeb 28, 2018\n\n\nA node hangs, causing a subset of users to report issues\n\n\n\n\nJun 11, 2018\n\n\nAzure billing issue causes downtime\n\n\n\n\nFeb 25, 2019\n\n\nAzure Kubernetes API Server outage causes downtime\n\n\n\n\nMay 1, 2019\n\n\nService Account key leak incident\n\n\n\n\nJan 20, 2022\n\n\nHubs throwing 505 errors\n\n\n\n\nFeb 1, 2024\n\n\nCore nodes being autoscaled, configurable HTTP proxy crashes\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Using DataHub",
      "Incident Reports"
    ]
  },
  {
    "objectID": "incidents/2017-03-23-kernel-deaths-incident.html",
    "href": "incidents/2017-03-23-kernel-deaths-incident.html",
    "title": "Weird upstream ipython bug kills kernels",
    "section": "",
    "text": "A seemingly unrelated change caused user kernels to die on start (making notebook execution impossible) for newly started user servers from about Mar 22 19:30 to Mar 23 09:45. Most users didn’t see any errors until start of class at about 9AM, since they were running servers that were previously started."
  },
  {
    "objectID": "incidents/2017-03-23-kernel-deaths-incident.html#summary",
    "href": "incidents/2017-03-23-kernel-deaths-incident.html#summary",
    "title": "Weird upstream ipython bug kills kernels",
    "section": "",
    "text": "A seemingly unrelated change caused user kernels to die on start (making notebook execution impossible) for newly started user servers from about Mar 22 19:30 to Mar 23 09:45. Most users didn’t see any errors until start of class at about 9AM, since they were running servers that were previously started."
  },
  {
    "objectID": "incidents/2017-03-23-kernel-deaths-incident.html#timeline",
    "href": "incidents/2017-03-23-kernel-deaths-incident.html#timeline",
    "title": "Weird upstream ipython bug kills kernels",
    "section": "Timeline",
    "text": "Timeline\n\nMarch 22, around 19:30\nA deployment is performed, finally deploying https://github.com/data-8/jupyterhub-k8s/pull/146 to production. It seemed to work fine on -dev, and on prod as well. However, the testing regimen was only to see if a notebook server would show up - not if a kernel would spawn.\n\n\nMar 23, 09:08\nStudents report that their kernels keep dying. This is confirmed to be a problem for all newly launched notebooks, in both prod and dev.\n\n\n09:16\nThe last change to the repo (an update of the single-user image) is reverted, to check if that was causing the problem. This does not improve the situation. Debugging continues, but with no obvious angles of attack.\n\n\n09:41\nAfter debugging produces no obvious culprits, the state of the entire infrastructure for prod is reverted to a known good state from a few days ago. This was done with:\n./deploy.py prod data8 25abea764121953538713134e8a08e0291813834\n25abea764121953538713134e8a08e0291813834 is the commit hash of a known good commit from March 19. Our disciplined adherence to immutable & reproducible deployment paid off, and we were able to restore new servers to working order with this!\nStudents are now able to resume working after a server restart. A mass restart is also performed to aid this.\nDev is left in a broken state in an attempt to debug.\n\n\n09:48\nA core Jupyter Notebook dev at BIDS attempts to debug the problem, since it seems to be with the notebook itself and not with JupyterHub.\n\n\n11:08\nCore Jupyter Notebook dev confirms that this makes no sense.\n\n\n14:55\nAttempts to isolate the bug start again, mostly by using git bisect to deploy different versions of our infrastructure to dev until we find what broke.\n\n\n15:30\nhttps://github.com/data-8/jupyterhub-k8s/pull/146 is identified as the culprit. It continues to not make sense.\n\n\n17:25\nA very involved and laborious revert of the offending part of the patch is done in https://github.com/jupyterhub/kubespawner/pull/37. Core Jupyter Notebook dev continues to confirm this makes no sense.\nhttps://github.com/data-8/jupyterhub-k8s/pull/152 is also merged, and deployed shortly after verifiying that everything (including starting kernels & executing code) works fine on dev. Deployed to prod and everything is fine."
  },
  {
    "objectID": "incidents/2017-03-23-kernel-deaths-incident.html#conclusion",
    "href": "incidents/2017-03-23-kernel-deaths-incident.html#conclusion",
    "title": "Weird upstream ipython bug kills kernels",
    "section": "Conclusion",
    "text": "Conclusion\nInsufficient testing procedures caused a new kind of outage (kernel dying) that we had not seen before. However, since our infrastructure was immutable & reproducible, our outage really only lasted about 40 minutes (from start of lab when students were starting containers until the revert). Deeper debugging produced a fix, but attempts to understand why the fix works are ongoing.\nUpdate: We have found and fixed the underlying issue"
  },
  {
    "objectID": "incidents/2017-03-23-kernel-deaths-incident.html#action-items",
    "href": "incidents/2017-03-23-kernel-deaths-incident.html#action-items",
    "title": "Weird upstream ipython bug kills kernels",
    "section": "Action items",
    "text": "Action items\n\nProcess\n\nDocument and formalize the testing process for post-deployment checks.\nSet a short timeout (maybe ten minutes?) after which investigation temporarily stops and we revert our deployment to a known good state.\n\n\n\nUpstream KubeSpawner\n\nContinue investigating https://github.com/jupyterhub/kubespawner/issues/31, which was the core issue that prompted the changes that eventually led to the outage."
  },
  {
    "objectID": "policy/policy_create_hubs.html",
    "href": "policy/policy_create_hubs.html",
    "title": "Policy considerations for creating a new hub",
    "section": "",
    "text": "We have lots of prior experience creating 10+ new hubs catering to the diverse instructional needs of the campus audience. Our decisions to create a new hub were made with a lot of intuition about solving instructors’ immediate needs effectively. The objective of this policy document is to codify these heuristics used while creating a new hub. Our policy should guide our decisions with regard to creating new hubs in the future.\nBelow are 5 key criteria (listed in the order of importance) to be considered while making a decision to create a new hub. Satisfying any one of the below-mentioned criteria creates the opportunity for a new hub.\n\nImage Customization: Course needs image customization beyond what we currently have in the main datahub image. (Eg: Astro hub, Stat 159 hub)\nTestbed: Testbed for deploying new features which post maturity can be enabled across other major hubs (Eg: Stat 159 hub)\nLarge Computation: Course is computationally intensive requiring a large amount of CPU/Memory because of the nature of the use case (Eg: biology hub) or larger user base with 300+ students (Eg: Data 8 and Data 100 hubs). These courses may require additional compute through calendar-based scheduling.\nAdmin Access: Course has undergrad students acting as GSIs while simultaneously requiring admin access (Eg: Data8 hub)\nOrganizational Reasons: Hub is created for organizational/strategic reasons to build institutional buy-in from specific departments (Eg: Public Health and ISchool Hubs) and/or evangelize a specific service (Eg: Julia or R hub)",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Policy considerations for creating a new hub"
    ]
  },
  {
    "objectID": "policy/policy_deploy_mainhubs.html",
    "href": "policy/policy_deploy_mainhubs.html",
    "title": "Policy considerations for deploying to the main Datahub",
    "section": "",
    "text": "Policy considerations for deploying to the main Datahub\nOur goal is to provide a reliable infrastructure that instructors can completely trust while facilitating their coursework. Developing a robust protocol around deploying changes to the main datahub is important to achieve this goal. The objective of this policy document is to outline the criteria to deploy a change to an image in the main Datahub.\n\nRegular requests during the semester like package addition/change, RAM increase, CPU allocation, and providing admin access to users should be done with a robust testing protocol (either automated or manual) in place across staging and production.\nIntroduce new features in the main Datahub only after it gets successfully tested with one or many instructors across other course specific hub (eg: Data8 Hub) or use-case specific hub (Eg: Stat159 Hub).",
    "crumbs": [
      "Using DataHub",
      "Policy",
      "Policy considerations for deploying to the main Datahub"
    ]
  },
  {
    "objectID": "policy/index.html",
    "href": "policy/index.html",
    "title": "Datahub Policy Documents",
    "section": "",
    "text": "The primary objective of this documentation is to codify varied policies of the infrastructure team to operate the many Jupyterhubs deployed in UC Berkeley."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UC Berkeley DataHub Documentation",
    "section": "",
    "text": "This repository contains configuration and documentation (including policies) for the many JupyterHubs used by various organizations in UC Berkeley.",
    "crumbs": [
      "Using DataHub",
      "Home"
    ]
  },
  {
    "objectID": "admins/credentials.html",
    "href": "admins/credentials.html",
    "title": "Cloud Credentials",
    "section": "",
    "text": "Service accounts are identified by a service key, and help us grant specific access to an automated process. Our CI process needs two service accounts to operate:\n\nA gcr-readwrite key. This is used to build and push the user images. Based on the docs, this is assigned the role roles/storage.admin.\nA gke key. This is used to interact with the Google Kubernetes cluster. Roles roles/container.clusterViewer and roles/container.developer are granted to it.\n\nThese are currently copied into the secrets/ dir of every deployment, and explicitly referenced from hubploy.yaml in each deployment. They should be rotated every few months.\nYou can create service accounts through the web console or the commandline. Remember to not leave around copies of the private key elsewhere on your local computer!",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Cloud Credentials"
    ]
  },
  {
    "objectID": "admins/credentials.html#google-cloud",
    "href": "admins/credentials.html#google-cloud",
    "title": "Cloud Credentials",
    "section": "",
    "text": "Service accounts are identified by a service key, and help us grant specific access to an automated process. Our CI process needs two service accounts to operate:\n\nA gcr-readwrite key. This is used to build and push the user images. Based on the docs, this is assigned the role roles/storage.admin.\nA gke key. This is used to interact with the Google Kubernetes cluster. Roles roles/container.clusterViewer and roles/container.developer are granted to it.\n\nThese are currently copied into the secrets/ dir of every deployment, and explicitly referenced from hubploy.yaml in each deployment. They should be rotated every few months.\nYou can create service accounts through the web console or the commandline. Remember to not leave around copies of the private key elsewhere on your local computer!",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Cloud Credentials"
    ]
  },
  {
    "objectID": "admins/howto/delete-hub.html",
    "href": "admins/howto/delete-hub.html",
    "title": "Delete or spin down a Hub",
    "section": "",
    "text": "Sometimes we want to spin down or delete a hub:\n\nA course or department won’t be needing their hub for a while\nThe hub will be re-deployed in to a new or shared node pool.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Delete or spin down a Hub"
    ]
  },
  {
    "objectID": "admins/howto/delete-hub.html#why-delete-or-spin-down-a-hub",
    "href": "admins/howto/delete-hub.html#why-delete-or-spin-down-a-hub",
    "title": "Delete or spin down a Hub",
    "section": "",
    "text": "Sometimes we want to spin down or delete a hub:\n\nA course or department won’t be needing their hub for a while\nThe hub will be re-deployed in to a new or shared node pool.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Delete or spin down a Hub"
    ]
  },
  {
    "objectID": "admins/howto/delete-hub.html#steps-to-spin-down-a-hub",
    "href": "admins/howto/delete-hub.html#steps-to-spin-down-a-hub",
    "title": "Delete or spin down a Hub",
    "section": "Steps to spin down a hub",
    "text": "Steps to spin down a hub\nIf the hub is using a shared filestore, skip all filestore steps.\nIf the hub is using a shared node pool, skip all namespace and node pool steps.\n\nScale the node pool to zero: kubectl -n &lt;hubname-prod|staging&gt; scale --replicas=0 deployment/hub\nKill any remaining users’ servers. Find any running servers with kubectl -n &lt;hubname-prod|staging&gt; get pods | grep jupyter and then kubectl -n &lt;hubname-prod|staging&gt; delete pod &lt;pod name&gt; to stop them.\nCreate filestore backup:\n\ngcloud filestore backups create &lt;hubname&gt;-backup-YYYY-MM-DD --file-share=shares --instance=&lt;hubname-YYYY-MM-DD&gt; --region \"us-central1\" --labels=filestore-backup=&lt;hub name&gt;,hub=&lt;hub name&gt;\n\nLog in to nfsserver-01 and unmount filestore from nfsserver: sudo umount /export/&lt;hubname&gt;-filestore\nComment out the hub build steps out in .circleci/config.yaml (deploy and build steps)\nComment out GitHub label action for this hub in .github/labeler.yml\nComment hub entries out of datahub/node-placeholder/values.yaml\nDelete k8s namespace:\n\nkubectl delete namespace &lt;hubname&gt;-staging &lt;hubname&gt;-prod\n\nDelete k8s node pool:\n\ngcloud container node-pools delete &lt;hubname&gt; --project \"ucb-datahub-2018\" --cluster \"spring-2024\" --region \"us-central1\"\n\nDelete filestore\n\ngcloud filestore instances delete &lt;hubname&gt;-filestore --zone \"us-central1-b\"\n\nDelete PV: kubectl get pv --all-namespaces|grep &lt;hubname&gt; to get the PV names, and then kubectl delete pv &lt;pv names&gt;\nAll done.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Delete or spin down a Hub"
    ]
  },
  {
    "objectID": "admins/howto/course-config.html",
    "href": "admins/howto/course-config.html",
    "title": "Course Configuration",
    "section": "",
    "text": "It is possible to alter administrative priviliges or resources allocations (such as memory or extra volumes) of user servers from within the deployment configuration. This is mostly useful for when resources need to be increased based on users' class enrollments. The hub must be configured to use the CanvasOAuthenticator which is our default. Hubs that use dummy, Google, Generic OAuth, or other authenticators are not configured to allocate additional resources in this way.\nAdditionally, it is also possible to allocate resources based on the students membership of Canvas groups. This is useful if the instructor wants to dynamically grant additional resources without CI round-trips. Group management can be performed by the course staff directly from bCourses.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Course Configuration"
    ]
  },
  {
    "objectID": "admins/howto/course-config.html#allocating-resources",
    "href": "admins/howto/course-config.html#allocating-resources",
    "title": "Course Configuration",
    "section": "",
    "text": "It is possible to alter administrative priviliges or resources allocations (such as memory or extra volumes) of user servers from within the deployment configuration. This is mostly useful for when resources need to be increased based on users' class enrollments. The hub must be configured to use the CanvasOAuthenticator which is our default. Hubs that use dummy, Google, Generic OAuth, or other authenticators are not configured to allocate additional resources in this way.\nAdditionally, it is also possible to allocate resources based on the students membership of Canvas groups. This is useful if the instructor wants to dynamically grant additional resources without CI round-trips. Group management can be performed by the course staff directly from bCourses.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Course Configuration"
    ]
  },
  {
    "objectID": "admins/howto/course-config.html#implementation",
    "href": "admins/howto/course-config.html#implementation",
    "title": "Course Configuration",
    "section": "Implementation",
    "text": "Implementation\nThe authenticator reads users Canvas enrollments when they login, and then assigns them to JupyterHub groups based on those affiliations. Groups are named with the format \"course::{canvas_id}::enrollment_type::{canvas_role}\", e.g. \"course::123456::enrollment_type::teacher\" or \"course::234567::enrollment_type::student\". Our custom kubespawner, which we define in hub/values.yaml, reads users' group memberships prior to spawning. It then overrides various KubeSpawner paramters based on configuration we define, using the canvas ID as the key. (see below)\nNote that if a user is assigned to a new Canvas group (e.g. by the instructor manually, or by an automated Canvas/SIS system) while their server is already running, they will need to logout and then log back in in order for the authenticator to see the new affiliations. Restarting the user server is not sufficient.\nThe canvas ID is somewhat opaque to infrastructure staff -- we cannot look it up ourselves nor predict what it would be based on the name of the course. This is why we must request it from the instructor.\nThere are a number of other Canvas course attributes we could have substituted for the ID, but all had various drawbacks. An SIS ID attribute uses a consistent format that is relatively easy to predict, however it is only exposed to instructor accounts on hub login. In testing, when the Canvas admin configured student accounts to be able to read the SIS ID, we discovered that other protected SIS attributes would have been visible to all members of the course in the Canvas UI. Various friendly name attributes (e.g. \"Statistics 123, Spring '24\") were inconsistent in structure or were modifiable by the instructor. So while the Canvas ID is not predictable or easily discoverable by hub staff, it is immutable and the instructor can find it in the URL for their course.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Course Configuration"
    ]
  },
  {
    "objectID": "admins/howto/course-config.html#assigning-scopes-to-roles",
    "href": "admins/howto/course-config.html#assigning-scopes-to-roles",
    "title": "Course Configuration",
    "section": "Assigning Scopes to Roles",
    "text": "Assigning Scopes to Roles\nWhen JupyterHub only had two roles, admin and user, we would grant admin rights to course staff. This enabled course staff to start, access, and stop user servers, but it wasn't scoped to just the students in their own course. It would give them access to the accounts of everyone on the hub. They even had access to stop the hub process itself. JupyterHub now lets us create our own roles and assign scopes to them. As a result, we can grant course staff the ability to do what they need for members of their own course, and nothing more.\nAdd the following configuration for course staff who need elevated access:\njupyterhub:\n  hub:\n    loadRoles:\n      # Data 123, Summer 2024, #9876\n      course-staff-1234567:\n        description: Enable course staff to view and access servers.\n        # this role provides permissions to...\n        scopes:\n          - admin-ui\n          - list:users!group=course::1234567\n          - admin:servers!group=course::1234567\n          - access:servers!group=course::1234567\n        # this role will be assigned to...\n        groups:\n          - course::1234567::enrollment_type::teacher\n          - course::1234567::enrollment_type::ta\nThis configuration is headed by a comment which describes the course and term and links to the github issue where the staff made the request. It defines a new role, course-staff-1234567, for a course with bCourse ID 1234567. It assigns scopes for accessing and administering the servers for users in group course::1234567. Members of that group include all students and course staff. It also assigns scopes for viewing lists of users at /hub/admin. It assignes these scopes to members of the affiliated course staff groups.\nThis stanza is more verbose than inserting lists of users under admin_users, but it the privileges are more granular. We don't need to know who the individual course staff and they won't have more permissions than they need.\nThe configuration causes JupyterHub to update information in its jupyterhub.sqlite database file. When this configuraition is removed, the hub does not automatically flush out the roles and scopes from the database. So after the semester is over, it is advisable to remove this configuration and also to flush out the information in the database. There is no formal process for this, although we should develop one. We can delete the database, or we can manually remove entries from the sqlite file.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Course Configuration"
    ]
  },
  {
    "objectID": "admins/howto/course-config.html#defining-group-profiles",
    "href": "admins/howto/course-config.html#defining-group-profiles",
    "title": "Course Configuration",
    "section": "Defining group profiles",
    "text": "Defining group profiles\n\nRequire course staff to request additional resources through a github issue.\nObtain the bCourses course ID from the github issue. This ID is found in the course’s URL, e.g. https://bcourses.berkeley.edu/courses/123456. It should be a large integer. If the instructor requested resources for a specific group within the course, obtain the group name.\nEdit deployments/{deployment}/config/common.yaml.\nDuplicate an existing stanza, or create a new one under jupyterhub.custom.group_profiles by inserting yaml of the form:\njupyterhub:\n  custom:\n    group_profiles:\n\n      # Example: increase memory for everyone affiliated with a course.\n      # Name of Class 100, Fall '22; requested in #98765\n\n      course::123456:\n        mem_limit: 4096M\n        mem_guarantee: 2048M\n\n\n      # Example: increase memory just for course staff.\n      # Enrollment types returned by the Canvas API are `teacher`,\n      # `student`, `ta`, `observer`, and `designer`. (non-plural)\n      # https://canvas.instructure.com/doc/api/enrollments.html\n\n      # Some other class 200, Spring '23; requested in #98776\n      course::234567::enrollment_type::teacher:\n        mem_limit: 2096M\n        mem_guarantee: 2048M\n      course::234567::enrollment_type::ta:\n        mem_limit: 2096M\n        mem_guarantee: 2048M\n\n\n      # Example: a fully specified CanvasOAuthenticator group name where\n      # the resource request happens to be an additional mount path.\n      # Creating groups for temporary resource bumps could be useful\n      # where the instructor could add people to groups in the bCourses\n      # UI. This would benefit from the ability to read resource bumps\n      # from jupyterhub's properties. (attributes in the ORM)\n\n      # Name of Class 100, Fall '22; requested in #98770\n      course::123456::group::lab4-bigdata:\n        - mountPath: /home/rstudio/.ssh\n          name: home\n          subPath: _some_directory/_ssh\n          readOnly: true\nOur custom KubeSpawner knows to look for these values under jupyterhub.custom.\n123456 and 234567 are bCourse course identifiers from the first step. Memory limits and extra volume mounts are specified as in the examples above.\nAdd a comment associating the profile identifier with a friendly name of the course. Also link to the github issue where the instructor requested the resources. This helps us to cull old configuration during maintenance windows.\nCommit the change, then ask course staff to verify the increased allocation on staging. It is recommended that they simulate completing a notebook or run through the assignment which requires extra resources.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Course Configuration"
    ]
  },
  {
    "objectID": "admins/howto/course-config.html#defining-user-profiles",
    "href": "admins/howto/course-config.html#defining-user-profiles",
    "title": "Course Configuration",
    "section": "Defining user profiles",
    "text": "Defining user profiles\nIt may be necessary to assign additional resources to specific users, if it is too difficult to assign them to a bCourses group.\n\nEdit deployments/{deployment}/config/common.yaml.\nDuplicate an existing stanza, or create a new one under jupyterhub.custom.profiles by inserting yaml of the form:\njupyterhub:\n  custom:\n    profiles:\n\n      # Example: increase memory for these specific users.\n      special_people:\n        # Requested in #87654. Remove after YYYY-MM-DD.\n        mem_limit: 2048M\n        mem_guarantee: 2048M\n        users:\n          - user1\n          - user2\nAdd a comment which links to the github issue where the resources were requested. This helps us to cull old configuration during maintenance windows.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Course Configuration"
    ]
  },
  {
    "objectID": "admins/howto/course-config.html#housekeeping",
    "href": "admins/howto/course-config.html#housekeeping",
    "title": "Course Configuration",
    "section": "Housekeeping",
    "text": "Housekeeping\nGroup profiles should be removed at the end of every term because course affiliations are not necessarily removed from each person's Canvas account. So even if a user's class ended, the hub will grant additional resources for as long as the config persisted in both Canvas and the hub.\nUser profiles should also be evaluated at the end of every term.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Course Configuration"
    ]
  },
  {
    "objectID": "admins/howto/calendar-scaler.html",
    "href": "admins/howto/calendar-scaler.html",
    "title": "Calendar Node Pool Autoscaler",
    "section": "",
    "text": "The scheduler isn’t perfect for us, especially when large classes have assignments due and a hub is flooded with students. This “hack” was introduced to improve cluster scaling prior to known events.\nThese ‘placeholder’ nodes are used to minimize the delay that occurs when GCP creates new node pools during mass user logins. This common, especially for larger classes.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Calendar Node Pool Autoscaler"
    ]
  },
  {
    "objectID": "admins/howto/calendar-scaler.html#why-scale-node-pools-with-google-calendar",
    "href": "admins/howto/calendar-scaler.html#why-scale-node-pools-with-google-calendar",
    "title": "Calendar Node Pool Autoscaler",
    "section": "",
    "text": "The scheduler isn’t perfect for us, especially when large classes have assignments due and a hub is flooded with students. This “hack” was introduced to improve cluster scaling prior to known events.\nThese ‘placeholder’ nodes are used to minimize the delay that occurs when GCP creates new node pools during mass user logins. This common, especially for larger classes.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Calendar Node Pool Autoscaler"
    ]
  },
  {
    "objectID": "admins/howto/calendar-scaler.html#structure",
    "href": "admins/howto/calendar-scaler.html#structure",
    "title": "Calendar Node Pool Autoscaler",
    "section": "Structure",
    "text": "Structure\nThere is a Google Calendar calendar, DataHub Scaling Events shared with all infrastructure staff. The event descriptions should contain a YAML fragment, and are of the form pool_name: count, where the name is the corresponding hub name (data100, stat20) and the count is the number of extra nodes you want. There can be several pools defined, one per line.\nBy default, we usually have one spare node ready to go, so if the count in the calendar event is set to 0 or 1, there will be no change to the cluster. If the value is set to &gt;=2, additional hot spares will be created. If a value is set more than once, the entry with the greater value will be used.\nYou can determine how many placeholder nodes to have up based on how many people you expect to log in at once. Some of the bigger courses may require 2 or more placeholder nodes, but during “regular” hours, 1 is usually sufficient.\nThe scaling mechanism is implemented as the node-placeholder-node-placeholder-scaler deployment within the node-placeholder namespace. The source code is within https://github.com/berkeley-dsep-infra/datahub/tree/staging/images/node-placeholder-scaler.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Calendar Node Pool Autoscaler"
    ]
  },
  {
    "objectID": "admins/howto/calendar-scaler.html#calendar-autoscaler",
    "href": "admins/howto/calendar-scaler.html#calendar-autoscaler",
    "title": "Calendar Node Pool Autoscaler",
    "section": "Calendar Autoscaler",
    "text": "Calendar Autoscaler\nThe code for the calendar autoscaler is a python 3.11 script, located here: https://github.com/berkeley-dsep-infra/datahub/tree/staging/images/node-placeholder-scaler/scaler\n\nHow the scaler works\nThere is a k8s pod running in the node-placeholder namespace, which simply runs python3 -m scaler. This script runs in an infinite loop, and every 60 seconds checks the scaler config and calendar for entries. It then uses the highest value provided as the number of placeholder replicas for any given hub. This means that if there’s a daily evening event to ‘cool down’ the number of replicas for all hubs to 0, and a simultaneous event to set one or more hubs to a higher number, the scaler will see this and keep however many node placeholders specified up and ready to go.\nAfter determining the number of replicas needed for each hub, the scaler will create a k8s template and run kubectl in the pod.\n\n\nUpdating the scaler config\nThe scaler config sets the default number of node-placeholders that are running at any given time. These values can be overridden by creating events in the DataHub Scaling Events calendar.\nWhen classes are in session, these defaults are all typically set to 1, and during breaks (or when a hub is not expected to be in use) they can be set to 0.\nAfter making changes to values.yaml, create a PR normally and our CI will push the new config out to the node-placeholder pod. There is no need to manually restart the node-placeholder pod as the changes will be picked up automatically.\n\n\nWorking on, testing and deploying the calendar scaler\nAll file locations in this section will assume that you are in the datahub/images/node-placeholder-scaler/ directory.\nIt is strongly recommended that you create a new python 3.11 environment before doing any dev work on the scaler. With conda, you can run the following commands to create one:\nconda create -ny scalertest python=3.11\npip install -r requirements.txt\nAny changes to the scaler code will require you to run chartpress to redeploy the scaler to GCP.\nHere is an example of how you can test any changes to scaler/calendar.py locally in the python interpreter:\n# these tests will use somes dates culled from the calendar with varying numbers of events.\nimport scaler.calendar\nimport datetime\nimport zoneinfo\n\ntz = zoneinfo.ZoneInfo(key='America/Los_Angeles')\nzero_events_noon_june = datetime.datetime(2023, 6, 14, 12, 0, 0, tzinfo=tz)\none_event_five_pm_april = datetime.datetime(2023, 4, 27, 17, 0, 0, tzinfo=tz)\nthree_events_eight_thirty_pm_march = datetime.datetime(2023, 3, 6, 20, 30, 0, tzinfo=tz)\ncalendar = scaler.calendar.get_calendar('https://calendar.google.com/calendar/ical/c_s47m3m1nuj3s81187k3b2b5s5o%40group.calendar.google.com/public/basic.ics')\nzero_events = scaler.calendar.get_events(calendar, time=zero_events_noon_june)\none_event = scaler.calendar.get_events(calendar, time=one_event_five_pm_april)\nthree_events = scaler.calendar.get_events(calendar, time=three_events_eight_thirty_pm_march)\n\nassert len(zero_events) == 0\nassert len(one_event) == 1\nassert len(three_events) == 3\nget_events returns a list of ical ical.event.Event class objects.\nThe method for testing scaler/scaler.py is similar to above, but the only things you’ll be able test locally are the make_deployment() and get_replica_counts() functions.\nWhen you’re ready, create a PR. The deployment workflow is as follows:\n\nGet all authed-up for chartpress by performing the steps listed here.\nRun chartpress --push from the root datahub/ directory. If this succeeds, check your git     status and add datahub/node-placeholder/Chart.yaml and datahub/node-placeholder/values.yml to your PR.\nMerge to staging and then prod.\n\n\n\nChanging python imports\nThe python requirements file is generated using requirements.in and pip-compile. If you need to change/add/update any packages, you’ll need to do the following:\n\nEnsure you have the correct python environment activated (see above).\nPip install pip-tools\nEdit requirements.in and save your changes.\nExecute pip-compile requirements.in, which will update the requirements.txt.\nCheck your git status and diffs, and create a pull request if necessary.\nGet all authed-up for chartpress by performing the steps listed here.\nRun chartpress --push from the root datahub/ directory. If this succeeds, check your git     status and add datahub/node-placeholder/Chart.yaml and datahub/node-placeholder/values.yml to your PR.\nMerge to staging and then prod.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Calendar Node Pool Autoscaler"
    ]
  },
  {
    "objectID": "admins/howto/calendar-scaler.html#monitoring",
    "href": "admins/howto/calendar-scaler.html#monitoring",
    "title": "Calendar Node Pool Autoscaler",
    "section": "Monitoring",
    "text": "Monitoring\nYou can monitor the scaling by watching for events:\nkubectl -n node-placeholder get events -w\nAnd by tailing the logs of the pod with the scalar process:\nkubectl -n node-placeholder logs -l app.kubernetes.io/name=node-placeholder-scaler -f\nFor example if you set epsilon: 2, you might see in the pod logs:\n2022-10-17 21:36:45,440 Found event Stat20/Epsilon test 2 2022-10-17 14:21 PDT to 15:00 PDT\n2022-10-17 21:36:45,441 Overrides: {'epsilon': 2}\n2022-10-17 21:36:46,475 Setting epsilon to have 2 replicas",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Calendar Node Pool Autoscaler"
    ]
  },
  {
    "objectID": "admins/howto/new-hub.html",
    "href": "admins/howto/new-hub.html",
    "title": "Create a new Hub",
    "section": "",
    "text": "The major reasons for making a new hub are:\n\nA new course wants to join the Berkeley Datahub community!\nSome of your students are admins on another hub, so they can see other students' work there.\nYou want to use a different kind of authenticator.\nYou are running in a different cloud, or using a different billing account.\nYour environment is different enough and specialized enough that a different hub is a good idea. By default, everyone uses the same image as datahub.berkeley.edu.\nYou want a different URL (X.datahub.berkeley.edu vs just datahub.berkeley.edu)\n\nIf your reason is something else, it probably needs some justification :)",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Create a new Hub"
    ]
  },
  {
    "objectID": "admins/howto/new-hub.html#why-create-a-new-hub",
    "href": "admins/howto/new-hub.html#why-create-a-new-hub",
    "title": "Create a new Hub",
    "section": "",
    "text": "The major reasons for making a new hub are:\n\nA new course wants to join the Berkeley Datahub community!\nSome of your students are admins on another hub, so they can see other students' work there.\nYou want to use a different kind of authenticator.\nYou are running in a different cloud, or using a different billing account.\nYour environment is different enough and specialized enough that a different hub is a good idea. By default, everyone uses the same image as datahub.berkeley.edu.\nYou want a different URL (X.datahub.berkeley.edu vs just datahub.berkeley.edu)\n\nIf your reason is something else, it probably needs some justification :)",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Create a new Hub"
    ]
  },
  {
    "objectID": "admins/howto/new-hub.html#prereqs",
    "href": "admins/howto/new-hub.html#prereqs",
    "title": "Create a new Hub",
    "section": "Prereqs",
    "text": "Prereqs\nWorking installs of the following utilities:\n\nsops\nhubploy\n\nhubploy docs\npip install hubploy\n\ngcloud\nkubectl\ncookiecutter\n\nProper access to the following systems:\n\nGoogle Cloud IAM: owner\nWrite access to the datahub repo\nCircleCI account linked to our org",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Create a new Hub"
    ]
  },
  {
    "objectID": "admins/howto/new-hub.html#setting-up-a-new-hub",
    "href": "admins/howto/new-hub.html#setting-up-a-new-hub",
    "title": "Create a new Hub",
    "section": "Setting up a new hub",
    "text": "Setting up a new hub\n\nName the hub\nChoose the &lt;hubname&gt; (typically the course or department). This is permanent.\n\n\nDetermine deployment needs\nBefore creating a new hub, have a discussion with the instructor about the system requirements, frequency of assignments and how much storage will be required for the course. Typically, there are three general \"types\" of hub: Heavy usage, general and small courses.\nSmall courses will usually have one or two assignments per semester, and may only have 20 or fewer users.\nGeneral courses have up to ~500 users, but don't have large amount of data or require upgraded compute resources.\nHeavy usage courses can potentially have thousands of users, require upgraded node specs and/or have Terabytes of data each semester.\nBoth general and heavy usage courses typically have weekly assignments.\nSmall courses (and some general usage courses) can use either or both of a shared node pool and filestore to save money (Basic HDD filestore instances start at 1T).\nThis is also a good time to determine if there are any specific software packages/libraries that need to be installed, as well as what language(s) the course will be using. This will determine which image to use, and if we will need to add additional packages to the image build.\nIf you're going to use an existing node pool and/or filestore instance, you can skip either or both of the following steps and pick back up at the cookiecutter.\nWhen creating a new hub, we also make sure to label the filestore and GKE/node pool resouces with both hub and &lt;nodepool|filestore&gt;-deployment. 99.999% of the time, the values for all three of these labels will be &lt;hubname&gt;.\n\n\nCreating a new node pool\nCreate the node pool:\ngcloud container node-pools create \"user-&lt;hubname&gt;-&lt;YYYY-MM-DD&gt;\"  \\\n  --labels=hub=&lt;hubname&gt;,nodepool-deployment=&lt;hubname&gt; \\\n  --node-labels hub.jupyter.org/pool-name=&lt;hubname&gt;-pool \\\n  --machine-type \"n2-highmem-8\"  \\\n  --enable-autoscaling --min-nodes \"0\" --max-nodes \"20\" \\\n  --project \"ucb-datahub-2018\" --cluster \"spring-2024\" \\\n  --region \"us-central1\" --node-locations \"us-central1-b\" \\\n  --node-taints hub.jupyter.org_dedicated=user:NoSchedule --tags hub-cluster \\\n  --image-type \"COS_CONTAINERD\" --disk-type \"pd-balanced\" --disk-size \"200\"  \\\n  --metadata disable-legacy-endpoints=true \\\n  --scopes \"https://www.googleapis.com/auth/devstorage.read_only\",\"https://www.googleapis.com/auth/logging.write\",\"https://www.googleapis.com/auth/monitoring\",\"https://www.googleapis.com/auth/servicecontrol\",\"https://www.googleapis.com/auth/service.management.readonly\",\"https://www.googleapis.com/auth/trace.append\" \\\n  --no-enable-autoupgrade --enable-autorepair \\\n  --max-surge-upgrade 1 --max-unavailable-upgrade 0 --max-pods-per-node \"110\"\n\n\nCreating a new filestore instance\nBefore you create a new filestore instance, be sure you know the capacity required. The smallest amount you can allocate is 1T, but larger hubs may require more. Confer with the admins and people instructing the course and determine how much they think they will need.\nWe can easily scale capacity up, but not down.\nFrom the command line, first fill in the instance name (&lt;hubname&gt;-&lt;YYYY-MM-DD&gt;) and &lt;capacity&gt;, and then execute the following command:\ngcloud filestore instances create &lt;hubname&gt;-&lt;YYYY-MM-DD&gt; \\\n  --zone \"us-central1-b\" --tier=\"BASIC_HDD\" \\\n  --file-share=capacity=1TiB,name=shares \\\n  --network=name=default,connect-mode=DIRECT_PEERING\nOr, from the web console, click on the horizontal bar icon at the top left corner.\n\nAccess \"Filestore\" -&gt; \"Instances\" and click on \"Create Instance\".\nName the instance &lt;hubname&gt;-&lt;YYYY-MM-DD&gt;\nInstance Type is Basic, Storage Type is HDD.\nAllocate capacity.\nSet the region to us-central1 and Zone to us-central1-b.\nSet the VPC network to default.\nSet the File share name to shares.\nClick \"Create\" and wait for it to be deployed.\nOnce it's deployed, select the instance and copy the \"NFS mount point\".\n\nYour new (but empty) NFS filestore must be seeded with a pair of directories. We run a utility VM for NFS filestore management; follow the steps below to connect to this utility VM, mount your new filestore, and create & configure the required directories.\nYou can run the following command in gcloud terminal to log in to the NFS utility VM:\ngcloud compute ssh nfsserver-01 --zone=us-central1-b\nAlternatively, launch console.cloud.google.com -&gt; Select \"ucb-datahub-2018\" as the project name.\n\nClick on the three horizontal bar icon at the top left corner.\nAccess \"Compute Engine\" -&gt; \"VM instances\" -&gt; and search for \"nfs-server-01\".\nSelect \"Open in browser window\" option to access NFS server via GUI.\n\nBack in the NFS utility VM shell, mount the new share:\nmkdir /export/&lt;hubname&gt;-filestore\nmount &lt;filestore share IP&gt;:/shares /export/&lt;hubname&gt;-filestore\nCreate staging and prod directories owned by 1000:1000 under /export/&lt;hubname&gt;-filestore/&lt;hubname&gt;. The path might differ if your hub has special home directory storage needs. Consult admins if that's the case. Here is the command to create the directory with appropriate permissions:\ninstall -d -o 1000 -g 1000 \\\n  /export/&lt;hubname&gt;-filestore/&lt;hubname&gt;/staging \\\n  /export/&lt;hubname&gt;-filestore/&lt;hubname&gt;/prod\nCheck whether the directories have permissions similar to the below directories:\ndrwxr-xr-x 4 ubuntu ubuntu     45 Nov  3 20:33 a11y-filestore\ndrwxr-xr-x 4 ubuntu ubuntu     33 Jan  4  2022 astro-filestore\ndrwxr-xr-x 4 ubuntu ubuntu  16384 Aug 16 18:45 biology-filestore\n\n\nCreate the hub deployment locally\nIn the datahub/deployments directory, run cookiecutter. This sets up the hub's configuration directory:\ncookiecutter template/\n\nThe cookiecutter template will prompt you to provide the following information:\n\n\n&lt;hub_name&gt;: Enter the chosen name of the hub.\n&lt;project_name&gt;: Default is ucb-datahub-2018, do not change.\n&lt;cluster_name&gt;: Default is spring-2024, do not change.\n&lt;pool_name&gt;: Name of the node pool (shared or individual) to deploy on.\nhub_filestore_share: Default is shares, do not change.\nhub_filestore_ip: Enter the IP address of the filestore instance. This is available from the web console.\nhub_filestore_capacity: Enter the allocated storage capacity. This is available from the web console.\n\n\n\nThis will generate a directory with the name of the hub you provided with a skeleton configuration and all the necessary secrets.\n\n\nConfigure filestore security settings and GCP billing labels\nIf you have created a new filestore instance, you will now need to apply the ROOT_SQUASH settings. Please ensure that you've already created the hub's root directory and both staging and prod directories, otherwise you will lose write access to the share. We also attach labels to a new filestore instance for tracking individual and full hub costs.\nSkip this step if you are using an existing/shared filestore.\ngcloud filestore instances update &lt;filestore-instance-name&gt; --zone=us-central1-b  \\\n       --update-labels=hub=&lt;hubname&gt;,filestore-deployment=&lt;hubname&gt; \\\n       --flags-file=&lt;hubname&gt;/config/filestore/squash-flags.json\n\n\nAuthentication\nSet up authentication via bcourses. We have two canvas OAuth2 clients setup in bcourses for us - one for all production hubs and one for all staging hubs. The configuration and secrets for these are provided by the cookiecutter template, however the new hubs need to be added to the authorized callback list maintained in bcourses.\n\nUse sops to edit secrets/dev.yaml and secrets/prod.yaml, replacing the cookiecutter hub_name. cookiecutter can’t do this for you since the values are encrypted.\nAdd &lt;hub_name&gt;-staging.datahub.berkeley.edu/hub/oauth_callback to the staging hub client (id 10720000000000594)\nAdd &lt;hub_name&gt;.datahub.berkeley.edu/hub/oauth_callback to the production hub client (id 10720000000000472)\n\nPlease reach out to Jonathan Felder to set this up, or bcourseshelp@berkeley.edu if he is not available.\n\n\nCircleCI\nThe CircleCI configuration file .circleci/config.yml will need to include directives for building and deploying your new hub at several phases of the CircleCI process. Generally speaking, an adequate manual strategy for this is to pick the name of an existing hub, find each occurrence of that name, and add analogous entries for your new hub alongside your example existing hub. Please order new entries for your new hub in alphabetical order amongst the entries for existing hubs.\nHere is a partial (but incomplete) sampling of some of the relevant sections of the CircleCI configuration file:\n- run:\n    name: Deploy &lt;hubname&gt;\n      command: |\n        hubploy deploy &lt;hubname&gt; hub ${CIRCLE_BRANCH}\n\n- hubploy/build-image:\n    deployment: &lt;hubname&gt;\n    name: &lt;hubname&gt; image build\n    filters:\n      branches:\n        ignore:\n          - staging\n          - prod  \n\n\n  - hubploy/build-image:\n      deployment:  &lt;hubname&gt;\n      name:  &lt;hubname&gt; image build\n      push: true\n      filters:\n        branches:\n          only:\n            - staging\n\n\n    -  &lt;hubname&gt; image build\nReview hubploy.yaml file inside your project directory and update the image name to the latest image. Something like this,\nimage_name: us-central1-docker.pkg.dev/ucb-datahub-2018/user-images/a11y-user-image\n\n\nAdd hub to the github labeler workflow\nThe new hub will now need to be added to the github labeler workflow.\nEdit the file .github/labeler.yml and add an entry for this hub (alphabetically) in the # add hub-specific labels for deployment changes block:\n\"hub: &lt;hubname&gt;\":\n  - \"deployments/&lt;hubname&gt;/**\"\n\n\nCreate placeholder node pool\nNode pools have a configured minimum size, but our cluster has the ability to set aside additional placeholder nodes. These are nodes that get spun up in anticipation of the pool needing to suddenly grow in size, for example when large classes begin.\nIf you are deploying to a shared node pool, there is no need to perform this step.\nOtherwise, you'll need to add the placeholder settings in node-placeholder/values.yaml.\nThe node placeholder pod should have enough RAM allocated to it that it needs to be kicked out to get even a single user pod on the node - but not so big that it can't run on a node where other system pods are running! To do this, we'll find out how much memory is allocatable to pods on that node, then subtract the sum of all non-user pod memory requests and an additional 256Mi of \"wiggle room\". This final number will be used to allocate RAM for the node placeholder.\n\nLaunch a server on https://&lt;hubname&gt;.datahub.berkeley.edu\nGet the node name (it will look something like gke-spring-2024-user-datahub-2023-01-04-fc70ea5b-67zs): kubectl get nodes | grep &lt;hubname&gt; | awk '{print$1}'\nGet the total amount of memory allocatable to pods on this node and convert to bytes: kubectl get node &lt;nodename&gt; -o jsonpath='{.status.allocatable.memory}'\nGet the total memory used by non-user pods/containers on this node. We explicitly ignore notebook and pause. Convert to bytes and get the sum:\n\nkubectl get -A pod -l 'component!=user-placeholder' \\\n       --field-selector spec.nodeName=&lt;nodename&gt; \\\n       -o jsonpath='{range .items[*].spec.containers[*]}{.name}{\"\\t\"}{.resources.requests.memory}{\"\\n\"}{end}' \\\n       | egrep -v 'pause|notebook'\n\nSubract the second number from the first, and then subtract another 277872640 bytes (256Mi) for \"wiggle room\".\nAdd an entry for the new placeholder node config in values.yaml:\n\ndata102:\n  nodeSelector:\n    hub.jupyter.org/pool-name: data102-pool\n  resources:\n    requests:\n      # Some value slightly lower than allocatable RAM on the node pool\n      memory: 60929654784\n  replicas: 1\nFor reference, here's example output from collecting and calculating the values for data102:\n(gcpdev) ➜  ~ kubectl get nodes | grep data102 | awk '{print$1}'\ngke-spring-2024-user-data102-2023-01-05-e02d4850-t478\n(gcpdev) ➜  ~ kubectl get node gke-spring-2024-user-data102-2023-01-05-e02d4850-t478 -o jsonpath='{.status.allocatable.memory}' # convert to bytes\n60055600Ki%\n(gcpdev) ➜  ~ kubectl get -A pod -l 'component!=user-placeholder' \\\n--field-selector spec.nodeName=gke-spring-2024-user-data102-2023-01-05-e02d4850-t478 \\\n-o jsonpath='{range .items[*].spec.containers[*]}{.name}{\"\\t\"}{.resources.requests.memory}{\"\\n\"}{end}' \\\n| egrep -v 'pause|notebook' # convert all values to bytes, sum them\ncalico-node\nfluentbit       100Mi\nfluentbit-gke   100Mi\ngke-metrics-agent       60Mi\nip-masq-agent   16Mi\nkube-proxy\nprometheus-node-exporter\n(gcpdev) ➜  ~ # subtract the sum of the second command's values from the first value, then subtract another 277872640 bytes for wiggle room\n(gcpdev) ➜  ~ # in this case:  (60055600Ki - (100Mi + 100Mi + 60Mi + 16Mi)) - 256Mi\n(gcpdev) ➜  ~ # (61496934400 - (104857600 + 104857600 + 16777216 + 62914560)) - 277872640 == 60929654784\nBesides setting defaults, we can dynamically change the placeholder counts by either adding new, or editing existing, calendar events. This is useful for large courses which can have placeholder nodes set aside for predicatable periods of heavy ramp up.\n\n\nCommit and deploy staging\nCommit the hub directory, and make a PR to the the staging branch in the GitHub repo. Once tests pass, merge the PR to get a working staging hub! It might take a few minutes for HTTPS to work, but after that you can log into it at https://&lt;hub_name&gt;-staging.datahub.berkeley.edu. Test it out and make sure things work as you think they should.\n\nMake a PR from the staging branch to the prod branch. When this PR is merged, it'll deploy the production hub. It might take a few minutes for HTTPS to work, but after that you can log into it at https://&lt;hub_name&gt;.datahub.berkeley.edu. Test it out and make sure things work as you think they should.\nYou may want to customize the docker image for the hub based on your unique requirements. Navigate to deployments/'Project Name'/image and review environment.yml file and identify packages that you want to add from the conda repository &lt;https://anaconda.org/&gt;. You can copy the image manifest files from another deployment. It is recommended to use a repo2docker-style image build, without a Dockerfile, if possible. That format will probably serve as the ' basis for self-service user-created images in the future.\nAll done.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Create a new Hub"
    ]
  },
  {
    "objectID": "admins/howto/prometheus-grafana.html",
    "href": "admins/howto/prometheus-grafana.html",
    "title": "Prometheus and Grafana",
    "section": "",
    "text": "It can be useful to interact with the cluster’s prometheus server while developing dashboards in grafana. You will need to forward a local port to the prometheus server’s pod.\n\n\nListen on port 9090 locally, forwarding to the prometheus server’s port 9090.\nkubectl -n support port-forward deployment/support-prometheus-server 9090\nthen visit http://localhost:9090.\n\n\n\nListen on port 8000 locally, forwarding to the prometheus server’s port 9090.\nkubectl -n support port-forward deployment/support-prometheus-server 8000:9090\nthen visit http://localhost:8000.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Prometheus and Grafana"
    ]
  },
  {
    "objectID": "admins/howto/prometheus-grafana.html#using-the-standard-port",
    "href": "admins/howto/prometheus-grafana.html#using-the-standard-port",
    "title": "Prometheus and Grafana",
    "section": "",
    "text": "Listen on port 9090 locally, forwarding to the prometheus server’s port 9090.\nkubectl -n support port-forward deployment/support-prometheus-server 9090\nthen visit http://localhost:9090.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Prometheus and Grafana"
    ]
  },
  {
    "objectID": "admins/howto/prometheus-grafana.html#using-an-alternative-port",
    "href": "admins/howto/prometheus-grafana.html#using-an-alternative-port",
    "title": "Prometheus and Grafana",
    "section": "",
    "text": "Listen on port 8000 locally, forwarding to the prometheus server’s port 9090.\nkubectl -n support port-forward deployment/support-prometheus-server 8000:9090\nthen visit http://localhost:8000.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Prometheus and Grafana"
    ]
  },
  {
    "objectID": "admins/howto/new-packages.html",
    "href": "admins/howto/new-packages.html",
    "title": "Testing and Upgrading New Packages",
    "section": "",
    "text": "It is helpful to test package additions and upgrades for yourself before they are installed for all users. You can make sure the change behaves as you think it should, and does not break anything else. Once tested, request that the change by installed for all users by by creating a new issue in github,contacting cirriculum support staff, or creating a new pull request. Ultimately, thouroughly testing changes locally and submitting a pull request will result in the software being rolled out to everyone much faster.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Testing and Upgrading New Packages"
    ]
  },
  {
    "objectID": "admins/howto/new-packages.html#submitting-a-pull-request",
    "href": "admins/howto/new-packages.html#submitting-a-pull-request",
    "title": "Testing and Upgrading New Packages",
    "section": "Submitting a pull request",
    "text": "Submitting a pull request\nFamiliarize yourself with pull requests and repo2docker , and create a fork of the datahub staging branch.\n\nSet up your git/dev environment by following the instructions here.\nCreate a new branch for this PR.\nFind the correct environment.yml file for your class. This should be under datahub/deployments/&lt;class or hub name&gt;/image\nIn environment.yml, packages listed under dependencies are installed using conda, while packages under pip are installed using pip. Any packages that need to be installed via apt must be added to either datahub/deployments/&lt;class or hub name&gt;/image/apt.txt or datahub/deployments/&lt;class or hub name&gt;/image/Dockerfile.\nAdd any packages necessary. We typically prefer using conda packages, and pip only if necessary. Please pin to a specific version (no wildards, etc).\n\nNote that package versions for conda are specified using =, while in pip they are specified using ==\n\nTest the changes locally using repo2docker, then submit a PR to staging.\n\nTo use repo2docker, you have to point it at the right Dockerfile for your class. For example, to test the data100 datahub, you would run repo2docker deployments/data100/image from the base datahub directory.\n\nCommit and push your changes to your fork of the datahub repo, and create a new pull request at https://github.com/berkeley-dsep-infra/datahub/.\nOnce the PR is merged to staging, you can test it out on class-staging.datahub.berkeley.edu.\nChanges are only deployed to datahub once the relevant Travis CI job is completed. See https://circleci.com/gh/berkeley-dsep-infra/datahub to view Travis CI job statuses.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Testing and Upgrading New Packages"
    ]
  },
  {
    "objectID": "admins/howto/new-packages.html#tips-for-upgrading-package",
    "href": "admins/howto/new-packages.html#tips-for-upgrading-package",
    "title": "Testing and Upgrading New Packages",
    "section": "Tips for Upgrading Package",
    "text": "Tips for Upgrading Package\n\nConda can take an extremely long time to resolve version dependency conflicts, if they are resolvable at all. When upgrading Python versions or a core package that is used by many other packages, such as requests, clean out or upgrade old packages to minimize the number of dependency conflicts.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Testing and Upgrading New Packages"
    ]
  },
  {
    "objectID": "admins/howto/core-pool.html",
    "href": "admins/howto/core-pool.html",
    "title": "Creating and managing the core node pool",
    "section": "",
    "text": "What is the core node pool?\nThe core node pool is the primary entrypoint for all hubs we host. It manages all incoming traffic, and redirects said traffic (via the nginx ingress controller) to the proper hub.\nIt also does other stuff.\n\n\nDeploying a new core node pool\nRun the following command from the root directory of your local datahub repo to create the node pool:\ngcloud container node-pools create \"core-&lt;YYYY-MM-DD&gt;\"  \\\n  --labels=hub=core,nodepool-deployment=core \\\n  --node-labels hub.jupyter.org/pool-name=core-pool-&lt;YYYY-MM-DD&gt; \\\n  --machine-type \"n2-standard-8\"  \\\n  --num-nodes \"1\" \\\n  --enable-autoscaling --min-nodes \"1\" --max-nodes \"3\" \\\n  --project \"ucb-datahub-2018\" --cluster \"spring-2024\" --region \"us-central1\" --node-locations \"us-central1-b\" \\\n  --tags hub-cluster \\\n  --image-type \"COS_CONTAINERD\" --disk-type \"pd-balanced\" --disk-size \"100\"  \\\n  --metadata disable-legacy-endpoints=true \\\n  --scopes \"https://www.googleapis.com/auth/devstorage.read_only\",\"https://www.googleapis.com/auth/logging.write\",\"https://www.googleapis.com/auth/monitoring\",\"https://www.googleapis.com/auth/servicecontrol\",\"https://www.googleapis.com/auth/service.management.readonly\",\"https://www.googleapis.com/auth/trace.append\" \\\n  --no-enable-autoupgrade --enable-autorepair --max-surge-upgrade 1 --max-unavailable-upgrade 0 --max-pods-per-node \"110\" \\\n  --system-config-from-file=vendor/google/gke/node-pool/config/core-pool-sysctl.yaml\nThe system-config-from-file argument is important, as we need to tune the kernel TCP settings to handle large numbers of concurrent users and keep nginx from using up all of the TCP ram.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Creating and managing the core node pool"
    ]
  },
  {
    "objectID": "admins/howto/github-token.html",
    "href": "admins/howto/github-token.html",
    "title": "Create Finely Grained Access Token",
    "section": "",
    "text": "At https://github.com/settings/personal-access-tokens/new:\n\nToken name: set something descriptive.\nExpiration: set the token to expire no earlier or later than necessary.\nDescription: elaborate on the function of the token.\nResource owner: berkeley-dsep-infra\nRepository access: Only selected repositories &gt; datahub\nPermissions: Contents &gt; Access: Read and write",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Common Administrator Tasks",
      "Create Finely Grained Access Token"
    ]
  },
  {
    "objectID": "admins/structure.html",
    "href": "admins/structure.html",
    "title": "Repository Structure",
    "section": "",
    "text": "Each hub has a directory under deployments/ where all configuration for that particular hub is stored in a standard format. For example, all the configuration for the primary hub used on campus (datahub) is stored under deployments/datahub/.\n\n\nThe contents of the image/ directory determine the environment provided to the user. For example, it controls:\n\nVersions of Python / R / Julia available\nLibraries installed, and which versions of those are installed\nSpecific config for Jupyter Notebook or IPython\n\nrepo2docker is used to build the actual user image, so you can use any of the supported config files to customize the image as you wish.\n\n\n\nAll our JupyterHubs are based on Zero to JupyterHub (z2jh). z2jh uses configuration files in YAML format to specify exactly how the hub is configured. For example, it controls:\n\nRAM available per user\nAdmin user lists\nUser storage information\nPer-class & Per-user RAM overrides (when classes or individuals need more RAM)\nAuthentication secret keys\n\nThese files are split between files that are visible to everyone (config/) and files that are visible only to a select few illuminati (secrets/). To get access to the secret files, please consult the illuminati.\nFiles are further split into:\n\ncommon.yaml - Configuration common to staging and production instances of this hub. Most config should be here.\nstaging.yaml - Configuration specific to the staging instance of the hub.\nprod.yaml - Configuration specific to the production instance of the hub.\n\n\n\n\nWe use hubploy to deploy our hubs in a repeatable fashion. hubploy.yaml contains information required for hubploy to work - such as cluster name, region, provider, etc.\nVarious secret keys used to authenticate to cloud providers are kept under secrets/ and referred to from hubploy.yaml.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Repository Structure"
    ]
  },
  {
    "objectID": "admins/structure.html#hub-configuration",
    "href": "admins/structure.html#hub-configuration",
    "title": "Repository Structure",
    "section": "",
    "text": "Each hub has a directory under deployments/ where all configuration for that particular hub is stored in a standard format. For example, all the configuration for the primary hub used on campus (datahub) is stored under deployments/datahub/.\n\n\nThe contents of the image/ directory determine the environment provided to the user. For example, it controls:\n\nVersions of Python / R / Julia available\nLibraries installed, and which versions of those are installed\nSpecific config for Jupyter Notebook or IPython\n\nrepo2docker is used to build the actual user image, so you can use any of the supported config files to customize the image as you wish.\n\n\n\nAll our JupyterHubs are based on Zero to JupyterHub (z2jh). z2jh uses configuration files in YAML format to specify exactly how the hub is configured. For example, it controls:\n\nRAM available per user\nAdmin user lists\nUser storage information\nPer-class & Per-user RAM overrides (when classes or individuals need more RAM)\nAuthentication secret keys\n\nThese files are split between files that are visible to everyone (config/) and files that are visible only to a select few illuminati (secrets/). To get access to the secret files, please consult the illuminati.\nFiles are further split into:\n\ncommon.yaml - Configuration common to staging and production instances of this hub. Most config should be here.\nstaging.yaml - Configuration specific to the staging instance of the hub.\nprod.yaml - Configuration specific to the production instance of the hub.\n\n\n\n\nWe use hubploy to deploy our hubs in a repeatable fashion. hubploy.yaml contains information required for hubploy to work - such as cluster name, region, provider, etc.\nVarious secret keys used to authenticate to cloud providers are kept under secrets/ and referred to from hubploy.yaml.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Repository Structure"
    ]
  },
  {
    "objectID": "admins/structure.html#documentation",
    "href": "admins/structure.html#documentation",
    "title": "Repository Structure",
    "section": "Documentation",
    "text": "Documentation\nDocumentation is under the docs/ folder, and is generated with the sphinx project. It is written with the reStructuredText (rst) format. Documentation is automatically published to https://uc-berkeley-jupyterhubs.readthedocs.io/ and https://docs.datahub.berkeley.edu/. This is performed via a webhook in the github repo.",
    "crumbs": [
      "Using DataHub",
      "Contributing to DataHub",
      "Repository Structure"
    ]
  },
  {
    "objectID": "admins/index.html",
    "href": "admins/index.html",
    "title": "",
    "section": "",
    "text": "======================= Contributing to DataHub =======================\n.. toctree:: :titlesonly: :maxdepth: 2\npre-reqs structure storage cluster-config credentials incidents/index\n.. toctree:: :titlesonly: :maxdepth: 2\nhowto/index\ndeployments/index"
  }
]