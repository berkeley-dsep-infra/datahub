cert-manager:
  installCRDs: true
  # Our cluster-internal DNS seems to cache aggressively in
  # some cases. The cache is also more likely to be warm,
  # causing issues when cert-manager tries to verify
  # that the ACME http-01 challenge will succeed. Since
  # Let's encrypt itself isn't going to use our cluster-local
  # DNS, this causes false negatives - cert-manager thinks the
  # challenge will fail, but in reality it will most likely not.
  podDnsPolicy: None
  podDnsConfig:
    nameservers:
      - 1.1.1.1
      - 8.8.8.8
ingress-nginx:
  controller:
    config:
      # Let's preserve ability to have HTTP pages served under *.datahub.berkeley.edu
      # when necessary. Ideally would not be, but I want to preserve that option.
      hsts-include-subdomains: "false"
      # Increasing these per https://kubernetes.github.io/ingress-nginx/user-guide/miscellaneous/#websockets
      # Possibly related to temporary kernel connectivity issues from websocket failures
      proxy-read-timeout: "7200"
      proxy-send-timeout: "7200"
    # Best effort guess on resource needs
    # We overprovision a little - issues here cause a full cluster outage
    replicaCount: 2
    resources:
      limits:
        cpu: 2
        memory: 2Gi
      requests:
        cpu: 0.2
        memory: 384Mi
    service:
      loadBalancerIP: 34.69.164.86
    podLabels:
      hub.jupyter.org/network-access-proxy-http: 'true'

prometheus:
  # We were using network tags to restrict NFS access to just nodes from hub-cluster
  # However, it turns out packets coming from *pods* are *not* tagged with that, just from the nodes!
  # I'm guessing this is an intersection of how GKE works? I'm not sure.
  # Either way, I allowed TCP to port 9100 from 10.0.0.0/8 and it works fine.
  extraScrapeConfigs: |
    - job_name: prometheus-nfsd-server
      static_configs:
        - targets:
          - nfsserver-01:9100
  networkPolicy:
    enabled: true
  alertmanager:
    enabled: false
  nodeExporter:
    tolerations:
      - effect: NoSchedule
        # Deploy onto user nodes
        key: hub.jupyter.org_dedicated
        value: user
    updateStrategy:
      type: RollingUpdate
  pushgateway:
    enabled: false
  rbac:
    create: true
  server:
    resources:
      # Without this, prometheus can easily starve users
      requests:
        cpu: 2
        memory: 8Gi
      limits:
        cpu: 3
        memory: 12Gi
    labels:
      # For HTTP access to the hub, to scrape metrics
      hub.jupyter.org/network-access-hub: 'true'
    persistentVolume:
      size: 1000Gi
      storageClass: ssd
    retention: 180d
    strategy:
      # Can't really do rolling update, we have a persistent disk attached
      type: Recreate
    service:
      type: ClusterIP

grafana:
  deploymentStrategy:
    type: Recreate

  persistence:
    enabled: true
    storageClassName: standard

  service:
    type: ClusterIP

  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - grafana.datahub.berkeley.edu
    tls:
    - hosts:
      - grafana.datahub.berkeley.edu
      secretName: grafana-https-auto-tls

  grafana.ini:
    server:
      root_url: https://grafana.datahub.berkeley.edu/

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: prometheus
          orgId: 1
          type: prometheus
          url: http://support-prometheus-server
          access: proxy
          isDefault: true
          editable: false

prometheus-statsd-exporter:
  service:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9102"

  statsd:
    mappingConfig: |-
      mappings:
      - match: "python_popcon.hub.*.imported_package.*"
        name: "python_popcon_imported_package"
        labels:
          namespace: "$1"
          package: "$2"