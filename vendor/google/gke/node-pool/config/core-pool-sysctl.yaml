kubeletConfig:
  cpuManagerPolicy: static
# kubeletExtraConfig:
#   allowedUnsafeSysctls: 'net.core.*,net.ipv4.*'
linuxConfig:
  sysctl:
   # tune the ipv4 settings to not cause nginx to use all of the tcp memory
   # addresses: https://jira-secure.berkeley.edu/browse/DH-3
   #
   # following this process:
   # https://cloud.google.com/kubernetes-engine/docs/how-to/node-system-config
   #
   # man page:
   # https://man7.org/linux/man-pages/man7/tcp.7.html
   #
   # figures below are measured in units of system page size (4096B),
   # and gleaned from the following articles:
   # https://cromwell-intl.com/open-source/performance-tuning/tcp.html
   # https://www.ibm.com/docs/en/linux-on-systems?topic=tuning-tcpip-ipv4-settings
   # https://www.ibm.com/docs/en/linux-on-systems?topic=tuning-network-stack-settings
   #
   # net.ipv4.tcp_mem seems to be automagically generated from the supplied tcp_rmem
   # and tcp_wmem settings.  i believe?
   #
   # here be dragons.
   #
   # original values (as of 2023-04-19):
   # net.core.netdev_max_backlog=1000
   # net.core.rmem_max=212992
   # net.core.wmem_max=212992
   # net.ipv4.tcp_rmem=4096 87380 6291456
   # net.ipv4.tcp_wmem=4096 16384 4194304
   #
   # changes and additional tweaks (2024-04-11):
   # net.ipv4.tcp_max_syn_backlog=4096
   # net.core.rmem_max=3276800
   # net.core.wmem_max=3276800
   # net.ipv4.tcp_rmem=4096 87380 16777216
   # net.ipv4.tcp_wmem=4096 87380 16777216
   # net.core.somaxconn=1024
   #
   # https://fasterdata.es.net/host-tuning/linux/#toc-anchor-2
   net.core.netdev_max_backlog: '30000'
   net.core.somaxconn: '4096'
   # net.ipv4.tcp_max_syn_backlog: '8192'

   # these values are in bytes
   net.core.rmem_max: '67108864'
   net.core.wmem_max: '67108864'
   net.ipv4.tcp_rmem: '4096 87380 33554432'
   net.ipv4.tcp_wmem: '4096 87380 33554432'
